# üìö Scripts Usage Guide - DiveTeacher RAG System

> **Last Updated:** October 31, 2025  
> **Architecture:** ARIA v2.0.0 (Sequential + ARIA Chunking)

---

## ‚ö†Ô∏è CRITICAL: Two Different Workflows

### üß™ E2E TESTING (Clean Slate)

**Purpose:** Test system with fresh database  
**Script:** `./scripts/init-e2e-test.sh`  
**Default Behavior:** ‚ö†Ô∏è **CLEANS DATABASE**

```bash
# Full E2E test preparation (DEFAULT)
./scripts/init-e2e-test.sh

What it does:
‚úÖ Check containers running
‚ö†Ô∏è  CLEAN Neo4j database (DELETE ALL DATA!)
‚úÖ Warm up Docling models
‚úÖ Warm up ARIA Chunker
‚úÖ Verify all services
‚úÖ Show monitoring commands

Result: 0 nodes, 0 relationships (fresh start)
Use for: E2E testing with clean environment
```

---

### üè≠ PRODUCTION INGESTION (Preserve Data)

**Purpose:** Add documents to existing knowledge graph  
**Method:** Just start containers (warmup runs automatically)  
**Behavior:** ‚úÖ **PRESERVES DATABASE**

```bash
# Production ingestion workflow
docker-compose -f docker/docker-compose.dev.yml up -d

What happens:
‚úÖ Containers start
‚úÖ Warmup runs automatically (backend entrypoint)
   ‚Ä¢ Docling models warmed up
   ‚Ä¢ ARIA Chunker initialized
   ‚Ä¢ ‚ö†Ô∏è  Database NOT touched!
‚úÖ System ready for uploads
‚úÖ DocumentQueue ready (FIFO)

Result: Database unchanged, models ready
Use for: Adding 100 PDFs to existing knowledge graph
```

**Alternative (if you want explicit warmup check):**
```bash
# Start containers
docker-compose up -d

# Verify warmup with --skip-cleanup flag
./scripts/init-e2e-test.sh --skip-cleanup

What it does:
‚úÖ Check containers
‚úÖ SKIP database cleanup (--skip-cleanup)
‚úÖ Warm up models (if not already)
‚úÖ Verify services
‚úÖ Show current DB state (X nodes, Y relationships)

Result: Database preserved, verification complete
```

---

## üìä Monitoring Tools

### 1. DocumentQueue Monitor (Real-Time)

**Purpose:** Monitor multi-document queue status  
**Script:** `./scripts/monitor-queue.sh [interval]`

```bash
# Monitor queue every 2 seconds (default)
./scripts/monitor-queue.sh

# Monitor queue every 5 seconds
./scripts/monitor-queue.sh 5

What it shows:
‚Ä¢ Queue size (documents waiting)
‚Ä¢ Current document processing (filename, progress)
‚Ä¢ Completed/Failed counts
‚Ä¢ Success rate
‚Ä¢ Queued documents list (FIFO order)
‚Ä¢ Ingestion progress (chunks completed/total)
‚Ä¢ ARIA architecture info
```

**When to use:**
- Multi-document ingestion sessions
- Production batch uploads (100 PDFs)
- Monitoring queue health

---

### 2. Ingestion Monitor (Backend Logs)

**Purpose:** Monitor Graphiti ingestion logs in real-time  
**Script:** `./scripts/monitor_ingestion.sh [upload_id]`

```bash
# Monitor all ingestion activity
./scripts/monitor_ingestion.sh

# Monitor specific upload
./scripts/monitor_ingestion.sh c664bc97-87a4-4fc7-a846-8573de0c5a02

What it shows:
‚Ä¢ ARIA Chunker logs (RecursiveCharacterTextSplitter)
‚Ä¢ SafeIngestionQueue logs (token tracking)
‚Ä¢ DocumentQueue logs (sequential processing)
‚Ä¢ Graphiti logs (entity extraction)
‚Ä¢ Claude API calls
‚Ä¢ Chunk-by-chunk progress
‚Ä¢ Color-coded output (green=success, red=error)
```

**When to use:**
- Debugging ingestion issues
- Watching real-time processing
- Verifying ARIA chunking (3 chunks vs 204)

---

### 3. Upload Monitor (Status Polling)

**Purpose:** Monitor specific upload to completion  
**Script:** `./scripts/monitor-upload.sh <upload_id>`

```bash
# Monitor upload until completion
./scripts/monitor-upload.sh c664bc97-87a4-4fc7-a846-8573de0c5a02

What it shows:
‚Ä¢ Status updates every 5s
‚Ä¢ Stage progression
‚Ä¢ Progress percentage
‚Ä¢ Final metrics (entities, relations, chunks)
‚Ä¢ Total time
‚Ä¢ Avg time per chunk
```

**When to use:**
- Tracking single document progress
- Getting final metrics
- Timing validation

---

### 4. Backend Queue Test (Integration Test)

**Purpose:** Full integration test with upload + monitoring  
**Script:** `./scripts/test-backend-queue.sh <file.pdf>`

```bash
# Test with test.pdf
./scripts/test-backend-queue.sh TestPDF/test.pdf

# Test with Niveau 1.pdf
./scripts/test-backend-queue.sh "TestPDF/Niveau 1.pdf"

What it does:
‚úÖ Check backend health
‚úÖ Check DocumentQueue status
‚úÖ Upload file
‚úÖ Monitor processing (auto-poll)
‚úÖ Analyze backend logs
‚úÖ Validate ARIA chunking
‚úÖ Verify SafeIngestionQueue
‚úÖ Report final verdict
```

**When to use:**
- Automated testing
- CI/CD pipelines
- Validating system changes

---

## üéØ Common Scenarios

### Scenario 1: Daily E2E Test (Clean Slate)

```bash
# 1. Initialize for E2E test (cleans DB)
./scripts/init-e2e-test.sh

# 2. Upload test document
curl -X POST http://localhost:8000/api/upload \
  -F "file=@TestPDF/test.pdf"

# 3. Monitor
./scripts/monitor-upload.sh <upload_id>

# 4. Verify
curl -s http://localhost:8000/api/neo4j/stats | jq

Result: Clean test with 0‚ÜíX nodes
```

---

### Scenario 2: Week 1 Ingestion (100 PDFs)

```bash
# 1. Start system (warmup auto, DB preserved)
docker-compose up -d

# Wait 60s for warmup
sleep 60

# 2. Verify system ready (no cleanup!)
./scripts/init-e2e-test.sh --skip-cleanup --quiet

# 3. Monitor queue in Terminal 1
./scripts/monitor-queue.sh

# 4. Monitor backend in Terminal 2
./scripts/monitor_ingestion.sh

# 5. Upload all PDFs
for pdf in PDFs/*.pdf; do
  curl -X POST http://localhost:8000/api/upload -F "file=@$pdf"
  echo "Queued: $pdf"
done

# 6. Watch queue process all documents
# ‚Üí Sequential FIFO processing
# ‚Üí 60s delay between documents
# ‚Üí 100 PDFs in ~6.5 hours
# ‚Üí Knowledge graph grows progressively

Result: DB grows from X to X+100 documents
```

---

### Scenario 3: Debug Single Document

```bash
# 1. Keep existing data
./scripts/init-e2e-test.sh --skip-cleanup

# 2. Upload and get ID
RESPONSE=$(curl -s -X POST http://localhost:8000/api/upload \
  -F "file=@problem.pdf")
UPLOAD_ID=$(echo "$RESPONSE" | jq -r '.upload_id')

# 3. Monitor with all tools
# Terminal 1: Queue
./scripts/monitor-queue.sh

# Terminal 2: Backend logs
./scripts/monitor_ingestion.sh $UPLOAD_ID

# Terminal 3: Upload status
./scripts/monitor-upload.sh $UPLOAD_ID

# 4. Analyze results
curl -s http://localhost:8000/api/upload/$UPLOAD_ID/status | jq

Result: Detailed debugging info, DB preserved
```

---

## üîß System Architecture (ARIA v2.0.0)

### Processing Pipeline

```
User Upload
    ‚Üì
DocumentQueue (FIFO)
    ‚Üì
Sequential Processing (one at a time)
    ‚Üì
‚îú‚îÄ Docling Conversion (~40s)
‚îú‚îÄ ARIA Chunking (~0.1s) ‚Üí 3 chunks (3000 tokens each)
‚îî‚îÄ Graphiti Ingestion (~3 min)
   ‚îú‚îÄ SafeIngestionQueue (token-aware)
   ‚îú‚îÄ Claude Haiku (entity extraction)
   ‚îú‚îÄ OpenAI embeddings
   ‚îî‚îÄ Neo4j storage
    ‚Üì
Knowledge Graph (grows progressively)
```

### Performance

```
Single Document (Niveau 1.pdf):
‚îú‚îÄ Chunks: 3 (ARIA pattern)
‚îú‚îÄ Time: ~3.9 min
‚îú‚îÄ Cost: ~$0.02
‚îî‚îÄ Quality: 325 entities, 617 relations

100 Documents Batch:
‚îú‚îÄ Time: ~6.5 hours (overnight)
‚îú‚îÄ Cost: ~$2 total
‚îî‚îÄ Mode: Sequential FIFO + 60s inter-document delays
```

---

## üö® Common Mistakes to Avoid

### ‚ùå MISTAKE #1: Using init-e2e-test.sh for Production

```bash
# ‚ùå WRONG (for production ingestion):
./scripts/init-e2e-test.sh
# ‚Üí DELETES ALL YOUR DATA!

# ‚úÖ CORRECT (for production ingestion):
docker-compose up -d
# ‚Üí Warmup runs auto, DB preserved
```

---

### ‚ùå MISTAKE #2: Skipping Warmup for E2E Tests

```bash
# ‚ùå WRONG (cold start, slow first upload):
./scripts/init-e2e-test.sh --skip-warmup

# ‚úÖ CORRECT (warmed up, fast uploads):
./scripts/init-e2e-test.sh
# ‚Üí Full warmup, ready for immediate testing
```

---

### ‚ùå MISTAKE #3: Not Monitoring Queue During Batch

```bash
# ‚ùå WRONG (upload 100 PDFs blindly):
for pdf in *.pdf; do curl ...; done
# ‚Üí No visibility, can't debug issues

# ‚úÖ CORRECT (monitor while uploading):
# Terminal 1: Monitor queue
./scripts/monitor-queue.sh

# Terminal 2: Upload PDFs
for pdf in *.pdf; do curl ...; done
# ‚Üí See queue grow, processing status, success rate
```

---

## üìñ Quick Reference

| Task | Command | Cleans DB? |
|------|---------|------------|
| **E2E Test** | `./scripts/init-e2e-test.sh` | ‚úÖ YES |
| **Production Warmup** | `docker-compose up -d` | ‚ùå NO |
| **Verify System (Production)** | `./scripts/init-e2e-test.sh --skip-cleanup` | ‚ùå NO |
| **Monitor Queue** | `./scripts/monitor-queue.sh` | ‚ùå NO |
| **Monitor Upload** | `./scripts/monitor-upload.sh <id>` | ‚ùå NO |
| **Monitor Ingestion** | `./scripts/monitor_ingestion.sh` | ‚ùå NO |
| **Test Backend** | `./scripts/test-backend-queue.sh file.pdf` | ‚ùå NO |

---

## üéØ Summary

**For E2E Testing (Clean Start):**
```bash
./scripts/init-e2e-test.sh  # ‚ö†Ô∏è  CLEANS DB!
```

**For Production Ingestion (Preserve DB):**
```bash
docker-compose up -d  # ‚úÖ DB preserved, warmup automatic
```

**Remember:**
- ‚úÖ `init-e2e-test.sh` = Testing tool (cleans by default)
- ‚úÖ `docker-compose up` = Production tool (preserves DB)
- ‚úÖ Use `--skip-cleanup` flag to preserve DB with init script
- ‚úÖ Warmup runs automatically on backend start
- ‚úÖ All monitoring tools preserve DB (read-only)

---

**Questions? Check logs:** `docker logs rag-backend`

