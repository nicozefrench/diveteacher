# Application Goal - DiveTeacher RAG Knowledge Graph System

**Project Type:** SaaS Platform for Dive Training with RAG + Knowledge Graph  
**Target Users:** Dive Instructors, Students, Dive Centers  
**Target Deployment:** Vercel (Frontend) + DigitalOcean GPU (Backend)  
**LLM Provider:** Mistral 7B-instruct-Q5_K_M (local on GPU) - Agnostic architecture  
**Organizations:** FFESSM (France) + SSI (International) - V1

---

## ğŸ¯ Primary Objective

Build a **production-ready SaaS application** that allows dive instructors, students, and dive centers to:
1. **Access** comprehensive knowledge from official FFESSM and SSI dive training materials
2. **Upload** dive training documents (MFT FFESSM, SSI manuals, safety procedures, theory documents)
3. **Process** documents automatically using Dockling (PDF/PPT â†’ Markdown conversion)
4. **Ingest** processed content into a Neo4j Knowledge Graph using Graphiti
5. **Query** the knowledge base in French or English using natural language via an LLM-powered RAG system
6. **Stream** precise responses in real-time with exact citations from source documents
7. **Manage** conversations history for continuous learning and review
8. **Visualize** (Admin) the knowledge graph to understand relationships between certifications, procedures, and concepts

### Value Proposition
> **"Master scuba diving with AI - Instant access to FFESSM & SSI knowledge, precise answers to all your dive training questions, backed by official documentation."**

---

## ğŸ”‘ Key Features

### **1. Document Processing Pipeline**
- **Upload:** Drag & drop PDF/PPT files via React UI
- **Storage:** Original files stored on DigitalOcean (persistent volume)
- **Processing:** Dockling converts documents to markdown
- **Ingestion:** Graphiti builds knowledge graph in Neo4j
- **Status:** Real-time upload progress and processing status

### **2. Knowledge Graph**
- **Database:** Neo4j 5.x with APOC and Graph Data Science plugins
- **Schema:** Graphiti-managed entity and relationship extraction
- **Context:** Maintains document provenance and metadata
- **Query:** Cypher-based retrieval for RAG context

### **3. RAG System**
- **Retrieval:** Query Neo4j knowledge graph for relevant context
- **Augmentation:** Inject retrieved context into LLM prompt
- **Generation:** LLM generates response based on retrieved knowledge only
- **Grounding:** Responses strictly based on ingested documents (no hallucination)

### **4. LLM Integration (Agnostic)**
- **Primary:** Ollama (local models: llama3, mistral, mixtral)
- **Alternative:** Claude, OpenAI (API-based, configurable via env)
- **Abstraction:** Unified interface for switching providers
- **Streaming:** Real-time token streaming for better UX

### **5. Modern Web Interface**
- **Frontend:** React 18 + Vite + TailwindCSS + shadcn/ui
- **Upload UI:** Drag & drop, multi-file support, progress bars
- **Chat UI:** Streaming responses, markdown rendering, code highlighting
- **Responsive:** Mobile-first design, dark/light mode
- **Deployment:** Vercel (free tier, CDN, auto-deploy from GitHub)

### **6. Monitoring & Observability**
- **Error Tracking:** Sentry integration (backend + frontend)
- **Logging:** Structured logs with correlation IDs
- **Metrics:** Document processing stats, query performance
- **Health Checks:** API endpoints, database connectivity

---

## ğŸ—ï¸ Technical Architecture

### **Frontend (Vercel)**
```
React + Vite
â”œâ”€â”€ shadcn/ui components (modern UI primitives)
â”œâ”€â”€ TailwindCSS (utility-first styling)
â”œâ”€â”€ React Query (state management)
â”œâ”€â”€ SSE / WebSocket (streaming responses)
â””â”€â”€ File upload (multipart/form-data)
```

### **Backend (DigitalOcean Droplet)**
```
FastAPI + Docker Compose
â”œâ”€â”€ FastAPI (Python web framework)
â”‚   â”œâ”€â”€ /upload - Document upload endpoint
â”‚   â”œâ”€â”€ /query - RAG query endpoint (streaming)
â”‚   â”œâ”€â”€ /status - Processing status
â”‚   â””â”€â”€ /health - Health check
â”‚
â”œâ”€â”€ Neo4j 5.x (Knowledge Graph)
â”‚   â”œâ”€â”€ APOC plugin (advanced queries)
â”‚   â””â”€â”€ GDS plugin (graph algorithms)
â”‚
â”œâ”€â”€ Ollama (LLM Server)
â”‚   â”œâ”€â”€ llama3:8b (default model)
â”‚   â””â”€â”€ Custom models (configurable)
â”‚
â””â”€â”€ Dockling + Graphiti (Processing pipeline)
    â”œâ”€â”€ PDF/PPT â†’ Markdown
    â””â”€â”€ Markdown â†’ Neo4j KG
```

---

## ğŸŒ Deployment Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  VERCEL (Frontend)                      â”‚
â”‚  - React app (static build)             â”‚
â”‚  - Global CDN                           â”‚
â”‚  - HTTPS automatic                      â”‚
â”‚  - Environment: VITE_API_URL            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚ HTTPS API calls
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DIGITALOCEAN (Backend)                 â”‚
â”‚  - Docker Compose                       â”‚
â”‚  - Nginx reverse proxy (SSL)            â”‚
â”‚  - Persistent volumes:                  â”‚
â”‚    â€¢ /uploads (original files)          â”‚
â”‚    â€¢ /neo4j-data (graph database)       â”‚
â”‚    â€¢ /ollama-models (LLM models)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¨ User Experience Flow

### **1. Document Upload**
```
User uploads PDF/PPT â†’ Frontend
  â†“
Multipart upload â†’ Backend /upload endpoint
  â†“
Save to /uploads volume â†’ Return upload_id
  â†“
Background processing starts
  â†“
Dockling: PDF â†’ Markdown
  â†“
Graphiti: Markdown â†’ Neo4j Knowledge Graph
  â†“
Status: "completed" (WebSocket notification)
```

### **2. Knowledge Query**
```
User types question â†’ Frontend
  â†“
POST /query (streaming) â†’ Backend
  â†“
Query Neo4j (retrieve relevant context)
  â†“
Build RAG prompt (context + question)
  â†“
Stream LLM response â†’ Frontend (SSE)
  â†“
Display streaming answer (token by token)
```

---

## ğŸ” Security & Configuration

### **Environment Variables (All services)**
```bash
# LLM Configuration
LLM_PROVIDER=ollama           # ollama, claude, openai
OLLAMA_MODEL=llama3:8b
OLLAMA_BASE_URL=http://ollama:11434

# Optional: External LLM APIs
ANTHROPIC_API_KEY=sk-ant-...  # If using Claude
OPENAI_API_KEY=sk-...         # If using OpenAI

# Neo4j Database
NEO4J_URI=bolt://neo4j:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=secure_password_here

# Graphiti
GRAPHITI_ENABLED=true

# Monitoring (Sentry)
SENTRY_DSN_BACKEND=https://...@sentry.io/...
SENTRY_DSN_FRONTEND=https://...@sentry.io/...
SENTRY_ENVIRONMENT=production  # or development
SENTRY_TRACES_SAMPLE_RATE=1.0

# Backend API
API_HOST=0.0.0.0
API_PORT=8000
CORS_ORIGINS=https://your-app.vercel.app

# Frontend (Vercel)
VITE_API_URL=https://api.your-domain.com
VITE_SENTRY_DSN=https://...@sentry.io/...
```

---

## ğŸ“Š Success Metrics

### **Functionality**
- âœ… Upload PDF/PPT â†’ Processed â†’ Ingested (end-to-end)
- âœ… Query returns accurate answers from uploaded documents
- âœ… Streaming responses work smoothly (no lag)
- âœ… No hallucinations (answers only from knowledge graph)

### **Performance**
- âš¡ Document processing: < 30s per document
- âš¡ Query latency: < 2s to first token
- âš¡ Streaming: Smooth token delivery (30-50 tokens/s)

### **Reliability**
- ğŸ›¡ï¸ Error tracking (Sentry captures all errors)
- ğŸ›¡ï¸ Health checks (backend, Neo4j, Ollama)
- ğŸ›¡ï¸ Graceful degradation (fallback to status messages)

### **Usability**
- ğŸ¨ Modern UI (shadcn/ui components)
- ğŸ¨ Responsive design (mobile + desktop)
- ğŸ¨ Real-time feedback (upload progress, processing status)

---

## ğŸš€ Getting Started (Quick)

### **Local Development (5 min)**
```bash
# 1. Clone boilerplate
git clone <repo>
cd rag-kg-boilerplate

# 2. Configure environment
cp .env.template .env
# Edit .env with your settings

# 3. Start all services
docker-compose -f docker/docker-compose.dev.yml up -d

# 4. Pull Ollama model (first time)
docker exec rag-ollama ollama pull llama3:8b

# 5. Access application
# Frontend: http://localhost:5173
# Backend: http://localhost:8000
# Neo4j: http://localhost:7474
```

### **Production Deployment (20 min)**
```bash
# 1. Deploy backend to DigitalOcean
ssh your-droplet
cd /opt/rag-app
docker-compose -f docker-compose.prod.yml up -d

# 2. Deploy frontend to Vercel
cd frontend
vercel --prod
# Or: Push to GitHub (auto-deploy)
```

---

## ğŸ¯ Target Users

### **Primary Use Cases**
1. **Research:** Academic papers â†’ Knowledge graph â†’ Q&A
2. **Legal:** Contracts/policies â†’ Compliance queries
3. **Business:** Reports/presentations â†’ Executive summaries
4. **Technical:** Documentation â†’ Developer assistance

### **Ideal For**
- Teams needing private document Q&A (no external APIs)
- Organizations with compliance requirements (data stays on-prem)
- Developers building RAG applications (boilerplate foundation)
- Researchers processing large document collections

---

## ğŸ“š Documentation Structure

```
docs/
â”œâ”€â”€ SETUP.md              # Installation guide (local + prod)
â”œâ”€â”€ DEPLOYMENT.md         # DigitalOcean + Vercel deployment
â”œâ”€â”€ ARCHITECTURE.md       # System design deep-dive
â”œâ”€â”€ API.md                # Backend API reference
â”œâ”€â”€ TROUBLESHOOTING.md    # Common issues + solutions
â””â”€â”€ EXAMPLES.md           # Usage examples + screenshots
```

---

## ğŸ”® Future Enhancements (Post-MVP)

- [ ] Multi-user support (authentication)
- [ ] Document collections (organize uploads)
- [ ] Advanced search (filters, metadata)
- [ ] Export conversations (PDF, JSON)
- [ ] Knowledge graph visualization (D3.js, vis.js)
- [ ] Batch processing (upload multiple docs)
- [ ] LangChain integration (agent workflows)
- [ ] Vector search (hybrid retrieval)

---

**Status:** ğŸš§ **Boilerplate in development**  
**Target:** Production-ready foundation for RAG + KG applications  
**License:** MIT (open-source, reusable)

