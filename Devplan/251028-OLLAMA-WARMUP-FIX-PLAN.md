# ğŸ”§ PLAN COMPLET: Fix Ollama + Warm-up Docling

**Date:** 28 Octobre 2025  
**Objectif:** RÃ©parer Docker Ollama (modÃ¨le Qwen2.5-7B-Instruct-Q8_0) + Valider warm-up Docling  
**Status:** PLAN PRÃŠT - EN ATTENTE D'EXÃ‰CUTION

---

## ğŸ“‹ TABLE DES MATIÃˆRES

1. [Analyse de la Situation](#analyse-de-la-situation)
2. [ProblÃ¨mes IdentifiÃ©s](#problÃ¨mes-identifiÃ©s)
3. [Plan de Fix](#plan-de-fix)
4. [Validation Warm-up Docling](#validation-warm-up-docling)
5. [Checklist de Validation](#checklist-de-validation)

---

## ğŸ” ANALYSE DE LA SITUATION

### Ã‰tat Actuel Docker Compose

**Fichier:** `docker/docker-compose.dev.yml`

```yaml
ollama:
  image: ollama/ollama:latest
  container_name: rag-ollama
  environment:
    - OLLAMA_HOST=0.0.0.0:11434
    - OLLAMA_ORIGINS=*
    - OLLAMA_KEEP_ALIVE=5m
    - OLLAMA_MAX_LOADED_MODELS=1
    - OLLAMA_NUM_PARALLEL=4
    - OLLAMA_MAX_QUEUE=128
  deploy:
    resources:
      limits:
        memory: 16G
  healthcheck:
    test: ["CMD", "curl", "-f", "http://localhost:11434/api/version"]
    interval: 10s
    timeout: 5s
    retries: 5
  depends_on:
    ollama:
      condition: service_healthy  # â† Backend dÃ©pend d'Ollama
```

### Configuration Backend

**Fichier:** `backend/app/core/config.py`

```python
OLLAMA_BASE_URL: str = "http://ollama:11434"
OLLAMA_MODEL: str = "qwen2.5:7b-instruct-q8_0"  # Q8_0 pour qualitÃ© RAG optimale
DOCLING_TIMEOUT: int = 900  # 15 minutes
```

### Ã‰tat Warm-up Docling

**Fichier:** `backend/warmup_docling.py`

âœ… **Fix dÃ©jÃ  implÃ©mentÃ©**: Utilise `DoclingSingleton.get_converter()` pour initialiser le singleton directement.

```python
from integrations.dockling import DoclingSingleton

def warmup_docling_models():
    converter = DoclingSingleton.get_converter()  # âœ… Initialise singleton
    logger.info("âœ… DocumentConverter initialized - models cached!")
```

---

## âš ï¸ PROBLÃˆMES IDENTIFIÃ‰S

### ProblÃ¨me 1: Ollama Unhealthy

**SymptÃ´me:**
```
dependency failed to start: container rag-ollama is unhealthy
```

**Causes possibles:**
1. **ModÃ¨le non tÃ©lÃ©chargÃ©**: Qwen2.5:7b-instruct-q8_0 n'existe pas dans le container
2. **Healthcheck trop rapide**: 10s interval insuffisant pour premier dÃ©marrage
3. **Erreur de nom de modÃ¨le**: Format incorrect ou tag manquant

**Diagnostic:**
```bash
# VÃ©rifier si Ollama dÃ©marre
docker logs rag-ollama

# VÃ©rifier si le modÃ¨le existe
docker exec rag-ollama ollama list

# Tester l'API
docker exec rag-ollama curl http://localhost:11434/api/version
```

### ProblÃ¨me 2: Backend dÃ©pend d'Ollama mais Ollama n'est pas critique pour l'ingestion

**Impact:**
- Si Ollama est unhealthy, le backend ne dÃ©marre pas
- Mais Ollama n'est utilisÃ© que pour les requÃªtes RAG (downstream)
- L'ingestion (upstream) fonctionne sans Ollama

**Recommandation Best Practice (Guide Ollama):**
- Ollama devrait Ãªtre optionnel pour le dÃ©marrage du backend
- Seul le endpoint `/api/query` nÃ©cessite Ollama

### ProblÃ¨me 3: Warm-up Docling non vÃ©rifiÃ© dans les logs

**Impact:**
- On ne sait pas si le warm-up s'exÃ©cute correctement
- Pas de confirmation visuelle que les modÃ¨les Docling sont en cache

---

## ğŸ”§ PLAN DE FIX

### Phase 1: Diagnostic Ollama (5 min)

**Objectif:** Identifier pourquoi Ollama est unhealthy

#### Ã‰tape 1.1: VÃ©rifier les logs Ollama

```bash
# ArrÃªter tous les services
cd /Users/nicozefrench/Dropbox/AI/rag-knowledge-graph-starter
docker compose -f docker/docker-compose.dev.yml down

# DÃ©marrer seulement Ollama
docker compose -f docker/docker-compose.dev.yml up ollama -d

# Surveiller les logs
docker logs -f rag-ollama
```

**VÃ©rifications:**
- [ ] Ollama dÃ©marre sans erreur
- [ ] API rÃ©pond sur `/api/version`
- [ ] Aucune erreur de permission ou rÃ©seau

#### Ã‰tape 1.2: VÃ©rifier le modÃ¨le

```bash
# Attendre 30s pour que Ollama soit prÃªt
sleep 30

# Lister les modÃ¨les installÃ©s
docker exec rag-ollama ollama list

# Si vide, le modÃ¨le n'est pas tÃ©lÃ©chargÃ©
```

**RÃ©sultat attendu:**
```
NAME                            ID              SIZE      MODIFIED
qwen2.5:7b-instruct-q8_0       abc123          7.87 GB   X hours ago
```

**Si le modÃ¨le n'existe pas:**
```bash
# Pull le modÃ¨le (peut prendre 10-20 min)
docker exec rag-ollama ollama pull qwen2.5:7b-instruct-q8_0
```

#### Ã‰tape 1.3: Tester le healthcheck

```bash
# Tester manuellement
docker exec rag-ollama curl -f http://localhost:11434/api/version

# RÃ©sultat attendu: {"version":"0.5.x"}
```

---

### Phase 2: Fix Docker Compose (10 min)

**Objectif:** Corriger la configuration Ollama et dÃ©pendances

#### Fix 2.1: Augmenter le dÃ©lai healthcheck

**ProblÃ¨me:** 10s est trop court pour le premier dÃ©marrage d'Ollama.

**Solution:**

```yaml
# docker/docker-compose.dev.yml
ollama:
  # ... (config existante)
  healthcheck:
    test: ["CMD", "curl", "-f", "http://localhost:11434/api/version"]
    interval: 10s
    timeout: 5s
    retries: 10        # â† Augmenter de 5 Ã  10 (100s total)
    start_period: 60s  # â† Ajouter pÃ©riode de grÃ¢ce (60s avant de compter les Ã©checs)
```

**Justification (Best Practices Guide):**
- Premier dÃ©marrage Ollama: 20-30s
- Chargement initial du modÃ¨le: peut prendre 1-2 min
- `start_period: 60s` = le healthcheck ne compte pas comme Ã©chec pendant 60s

#### Fix 2.2: Rendre Ollama optionnel pour le backend (RECOMMANDÃ‰)

**ProblÃ¨me:** Backend ne peut pas dÃ©marrer si Ollama est unhealthy, mais Ollama n'est nÃ©cessaire que pour `/api/query`.

**Solution Option A (Rapide):** Supprimer la dÃ©pendance stricte

```yaml
# docker/docker-compose.dev.yml
backend:
  # ... (config existante)
  depends_on:
    neo4j:
      condition: service_healthy
    # REMOVED: ollama dependency
    # Ollama est optionnel - seulement pour RAG query
```

**Solution Option B (Meilleure):** DÃ©pendance conditionnelle

```yaml
backend:
  depends_on:
    neo4j:
      condition: service_healthy
    ollama:
      condition: service_started  # â† Started, pas healthy
```

**Recommandation:** Utiliser **Option B** pour MVP (le backend attend que Ollama dÃ©marre, mais pas qu'il soit healthy).

#### Fix 2.3: Pre-load du modÃ¨le Qwen (OPTIONNEL mais RECOMMANDÃ‰)

**ProblÃ¨me:** Le modÃ¨le doit Ãªtre tÃ©lÃ©chargÃ© manuellement ou lors du premier appel.

**Solution:** Service d'initialisation

```yaml
# docker/docker-compose.dev.yml
services:
  # ... (ollama service)
  
  # Service d'initialisation Ollama (run once)
  ollama-init:
    image: ollama/ollama:latest
    container_name: rag-ollama-init
    depends_on:
      ollama:
        condition: service_started
    entrypoint: /bin/sh
    command: >
      -c "
      echo 'â³ Waiting for Ollama to be ready...';
      sleep 30;
      echo 'ğŸ“¥ Pulling Qwen2.5-7B-Instruct-Q8_0...';
      ollama pull qwen2.5:7b-instruct-q8_0;
      echo 'âœ… Model ready!';
      ollama list;
      "
    environment:
      - OLLAMA_HOST=ollama:11434
    restart: "no"  # Run once only
```

**Note:** Ce service se termine aprÃ¨s le pull du modÃ¨le. Il ne bloque pas le dÃ©marrage des autres services.

---

### Phase 3: Validation Configuration Backend (5 min)

**Objectif:** VÃ©rifier que le backend utilise la bonne config Ollama

#### Ã‰tape 3.1: VÃ©rifier les variables d'environnement

```bash
# VÃ©rifier que le modÃ¨le est bien configurÃ©
docker exec rag-backend env | grep OLLAMA

# RÃ©sultat attendu:
# OLLAMA_BASE_URL=http://ollama:11434
# OLLAMA_MODEL=qwen2.5:7b-instruct-q8_0
```

#### Ã‰tape 3.2: Tester le LLM provider

```bash
# Tester le healthcheck du query endpoint
curl http://localhost:8000/api/query/health | jq

# RÃ©sultat attendu:
# {
#   "status": "healthy",
#   "provider": "ollama",
#   "model": "qwen2.5:7b-instruct-q8_0",
#   "test_response": "..."
# }
```

#### Ã‰tape 3.3: VÃ©rifier les logs backend

```bash
# Chercher les erreurs Ollama
docker logs rag-backend 2>&1 | grep -i ollama

# Chercher les erreurs de connection
docker logs rag-backend 2>&1 | grep -E "connection|refused|timeout"
```

---

## âœ… VALIDATION WARM-UP DOCLING

### Objectif: VÃ©rifier que le warm-up Docling fonctionne correctement

#### Ã‰tape 4.1: Ajouter des logs de validation dans warm-up

**ProblÃ¨me actuel:** Pas assez de logs pour confirmer que le singleton est bien initialisÃ©.

**Solution:**

```python
# backend/warmup_docling.py (AMÃ‰LIORATION)

def warmup_docling_models():
    """
    Download and cache Docling models before first document processing
    This prevents timeout issues during the first upload
    """
    logger.info("=" * 60)
    logger.info("ğŸ”¥ WARMING UP DOCLING MODELS")
    logger.info("=" * 60)
    
    try:
        # Import singleton
        logger.info("ğŸ“¦ Importing DoclingSingleton...")
        from integrations.dockling import DoclingSingleton
        
        logger.info("âœ… DoclingSingleton imported successfully")
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # CRITICAL: Initialize the SINGLETON directly
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        logger.info("ğŸ”„ Initializing DocumentConverter SINGLETON...")
        logger.info("â³ This may take several minutes on first run...")
        logger.info("ğŸ“ Config: OCR=True, Tables=True, Mode=ACCURATE")
        logger.info("")
        
        # This initializes the singleton with the EXACT config used in production
        converter = DoclingSingleton.get_converter()
        
        logger.info("âœ… DocumentConverter SINGLETON initialized!")
        logger.info("")
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # VALIDATION: Verify singleton is properly initialized
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        logger.info("ğŸ” VALIDATING SINGLETON STATE:")
        
        # Check if singleton instance exists
        from integrations.dockling import DoclingSingleton as Singleton
        if Singleton._instance is not None:
            logger.info("âœ… Singleton._instance is NOT None (GOOD)")
            logger.info(f"âœ… Singleton type: {type(Singleton._instance)}")
        else:
            logger.warning("âš ï¸  Singleton._instance is None (BAD - should not happen)")
        
        logger.info("")
        logger.info("=" * 60)
        logger.info("ğŸ‰ DOCLING WARM-UP COMPLETE!")
        logger.info("=" * 60)
        logger.info("")
        logger.info("â„¹ï¸  Models are now cached and ready for use")
        logger.info("â„¹ï¸  Subsequent document processing will be fast")
        logger.info("â„¹ï¸  Singleton is initialized and shared across requests")
        logger.info("")
        
        return True
        
    except Exception as e:
        logger.error("=" * 60)
        logger.error(f"âŒ WARM-UP FAILED: {e}")
        logger.error("=" * 60)
        logger.error("")
        logger.error("âš ï¸  Document processing may be slower on first upload")
        logger.error("âš ï¸  Models will be downloaded on-demand")
        logger.error("")
        return False
```

#### Ã‰tape 4.2: VÃ©rifier les logs du warm-up

```bash
# Rebuild backend avec les nouveaux logs
cd /Users/nicozefrench/Dropbox/AI/rag-knowledge-graph-starter
docker compose -f docker/docker-compose.dev.yml build backend --no-cache

# Restart backend et surveiller le warm-up
docker compose -f docker/docker-compose.dev.yml up backend -d
docker logs -f rag-backend 2>&1 | grep -E "WARMING|DOCLING|Singleton|âœ…|âŒ|ğŸ”¥|ğŸ‰"
```

**RÃ©sultat attendu (warm-up rÃ©ussi):**
```
ğŸ”¥ WARMING UP DOCLING MODELS
ğŸ“¦ Importing DoclingSingleton...
âœ… DoclingSingleton imported successfully
ğŸ”„ Initializing DocumentConverter SINGLETON...
â³ This may take several minutes on first run...
ğŸ“ Config: OCR=True, Tables=True, Mode=ACCURATE
âœ… DocumentConverter SINGLETON initialized!
ğŸ” VALIDATING SINGLETON STATE:
âœ… Singleton._instance is NOT None (GOOD)
âœ… Singleton type: <class 'docling.document_converter.DocumentConverter'>
ğŸ‰ DOCLING WARM-UP COMPLETE!
â„¹ï¸  Models are now cached and ready for use
â„¹ï¸  Subsequent document processing will be fast
â„¹ï¸  Singleton is initialized and shared across requests
```

**RÃ©sultat attendu (si models dÃ©jÃ  en cache - RAPIDE ~3s):**
```
ğŸ”¥ WARMING UP DOCLING MODELS
ğŸ“¦ Importing DoclingSingleton...
âœ… DoclingSingleton imported successfully
ğŸ”„ Initializing DocumentConverter SINGLETON...
âœ… DocumentConverter SINGLETON initialized!
ğŸ” VALIDATING SINGLETON STATE:
âœ… Singleton._instance is NOT None (GOOD)
ğŸ‰ DOCLING WARM-UP COMPLETE!
```

**Si Ã©chec:**
```
ğŸ”¥ WARMING UP DOCLING MODELS
ğŸ“¦ Importing DoclingSingleton...
âŒ WARM-UP FAILED: [error message]
âš ï¸  Document processing may be slower on first upload
âš ï¸  Models will be downloaded on-demand
```

#### Ã‰tape 4.3: VÃ©rifier que le singleton est rÃ©utilisÃ©

**Objectif:** Confirmer que les appels suivants Ã  `get_converter()` retournent l'instance en cache.

**Test:**
```bash
# Test 1: Upload un petit document (test.pdf 2 pages)
curl -X POST http://localhost:8000/api/upload \
  -F "file=@TestPDF/test.pdf"

# Surveiller les logs - la conversion devrait Ãªtre RAPIDE (pas de re-download)
docker logs -f rag-backend 2>&1 | grep -E "\[_convert_sync\]|Getting converter|Converter obtained"
```

**RÃ©sultat attendu:**
```
[upload_id] [_convert_sync] ğŸ”„ START conversion: test.pdf
[upload_id] [_convert_sync] ğŸ“¦ Getting converter singleton...
[upload_id] [_convert_sync] âœ… Converter obtained  # â† INSTANTANÃ‰ si singleton fonctionne
[upload_id] [_convert_sync] ğŸš€ Starting conversion...
[upload_id] [_convert_sync] âœ… Conversion complete
```

**Si le singleton ne fonctionne pas (BAD):**
```
[upload_id] [_convert_sync] ğŸ“¦ Getting converter singleton...
Fetching 9 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:17<00:00, 1.90s/it]  # â† Re-download !
[upload_id] [_convert_sync] âœ… Converter obtained  # â† Trop lent
```

---

## ğŸ“‹ CHECKLIST DE VALIDATION

### Pre-Flight Checks

- [ ] Git status clean (ou commits sauvegardÃ©s)
- [ ] Docker Desktop running avec 16GB RAM allouÃ©
- [ ] Ports disponibles: 7687, 11434, 8000, 5173
- [ ] Fichier `.env` prÃ©sent avec `NEO4J_PASSWORD` et `DOCLING_TIMEOUT=900`

### Phase 1: Diagnostic Ollama

- [ ] Ollama dÃ©marre sans erreur
- [ ] API `/api/version` rÃ©pond
- [ ] ModÃ¨le `qwen2.5:7b-instruct-q8_0` est listÃ©
- [ ] Test `ollama run qwen2.5:7b-instruct-q8_0` fonctionne

### Phase 2: Docker Compose

- [ ] `start_period: 60s` ajoutÃ© au healthcheck Ollama
- [ ] `retries: 10` ajoutÃ© au healthcheck Ollama
- [ ] DÃ©pendance backend â†’ ollama modifiÃ©e en `service_started` (ou supprimÃ©e)
- [ ] (Optionnel) Service `ollama-init` ajoutÃ© pour pre-load

### Phase 3: Backend Config

- [ ] Variable `OLLAMA_BASE_URL=http://ollama:11434` confirmÃ©e
- [ ] Variable `OLLAMA_MODEL=qwen2.5:7b-instruct-q8_0` confirmÃ©e
- [ ] Healthcheck `/api/query/health` retourne `status: healthy`
- [ ] Logs backend sans erreur Ollama

### Phase 4: Warm-up Docling

- [ ] Logs de validation ajoutÃ©s dans `warmup_docling.py`
- [ ] Backend rebuild avec `--no-cache`
- [ ] Logs warm-up montrent `âœ… Singleton._instance is NOT None`
- [ ] Logs warm-up montrent `ğŸ‰ DOCLING WARM-UP COMPLETE!`
- [ ] Premier upload test.pdf : conversion rapide (~10s, pas de re-download)
- [ ] Logs `_convert_sync` montrent `âœ… Converter obtained` instantanÃ©ment

---

## ğŸš€ EXÃ‰CUTION DU PLAN

### Commandes RÃ©sumÃ©es

```bash
# 1. ArrÃªter tous les services
cd /Users/nicozefrench/Dropbox/AI/rag-knowledge-graph-starter
docker compose -f docker/docker-compose.dev.yml down

# 2. DÃ©marrer Ollama seul et diagnostiquer
docker compose -f docker/docker-compose.dev.yml up ollama -d
sleep 30
docker exec rag-ollama ollama list

# 3. Si modÃ¨le manquant, le tÃ©lÃ©charger
docker exec rag-ollama ollama pull qwen2.5:7b-instruct-q8_0

# 4. Appliquer les fixes Docker Compose (Ã©diter le fichier)
# - Ajouter start_period: 60s
# - Augmenter retries: 10
# - Modifier depends_on: service_started

# 5. Appliquer les logs de validation warm-up (Ã©diter warmup_docling.py)

# 6. Rebuild backend
docker compose -f docker/docker-compose.dev.yml build backend --no-cache

# 7. Restart tous les services
docker compose -f docker/docker-compose.dev.yml up -d

# 8. Surveiller les logs warm-up
docker logs -f rag-backend 2>&1 | grep -E "WARMING|DOCLING|Singleton|âœ…|âŒ|ğŸ”¥|ğŸ‰"

# 9. Tester Ollama
curl http://localhost:8000/api/query/health | jq

# 10. Tester l'ingestion avec test.pdf
curl -X POST http://localhost:8000/api/upload -F "file=@TestPDF/test.pdf"
docker logs -f rag-backend 2>&1 | grep -E "\[_convert_sync\]|Getting converter"
```

---

## ğŸ“ NOTES IMPORTANTES

### Best Practices Ollama (depuis le guide)

1. **Context Window:** Qwen2.5 supporte jusqu'Ã  128K tokens, mais on utilise 4096 pour l'Ã©quilibre performance/mÃ©moire
2. **Temperature:** 0.7 est optimal pour RAG (balance crÃ©ativitÃ©/prÃ©cision)
3. **Model Tag:** `qwen2.5:7b-instruct-q8_0` est le format correct pour Q8_0
4. **First Load:** Le premier chargement du modÃ¨le peut prendre 1-2 min (10GB en VRAM)
5. **Healthcheck:** `/api/version` est plus fiable que `/api/tags` (ne nÃ©cessite pas de modÃ¨le chargÃ©)

### Timing Attendu

- **Ollama startup:** 20-30s (premier dÃ©marrage)
- **Model pull:** 10-20 min (si non prÃ©sent, 7.87 GB download)
- **Model load (first time):** 1-2 min (chargement en mÃ©moire)
- **Warm-up Docling (first time):** 10-15 min (download models)
- **Warm-up Docling (cached):** 3-5s (instantanÃ©)
- **Subsequent conversions:** 10-30s (selon taille document)

### CritÃ¨res de SuccÃ¨s

âœ… **Ollama OK si:**
- Container `rag-ollama` status = `healthy`
- `ollama list` montre `qwen2.5:7b-instruct-q8_0`
- `/api/query/health` retourne `status: healthy`

âœ… **Warm-up OK si:**
- Logs montrent `âœ… Singleton._instance is NOT None`
- Logs montrent `ğŸ‰ DOCLING WARM-UP COMPLETE!`
- Premier upload: conversion en ~10s (pas de `Fetching 9 files`)

âœ… **SystÃ¨me OK si:**
- Ingestion end-to-end rÃ©ussit avec test.pdf (2 pages)
- Logs montrent toutes les Ã©tapes (validation â†’ conversion â†’ chunking â†’ graphiti)
- Status API retourne `completed` aprÃ¨s ~2-3 min

---

## ğŸ›‘ STOP CONDITION

**On s'arrÃªte ici** quand:
1. âœ… Ollama est healthy et le modÃ¨le Qwen2.5-7B-Q8_0 est chargÃ©
2. âœ… Backend dÃ©marre sans Ãªtre bloquÃ© par Ollama
3. âœ… Warm-up Docling logs montrent singleton initialisÃ©
4. âœ… Warm-up logs montrent validation rÃ©ussie

**On NE teste PAS l'ingestion complÃ¨te** (l'utilisateur veut Ãªtre prÃ©sent pour ce test).

---

## ğŸ“Œ PROCHAINES Ã‰TAPES (APRÃˆS VALIDATION)

1. Test ingestion end-to-end avec test.pdf (avec l'utilisateur)
2. Monitoring des logs en temps rÃ©el
3. Validation que le singleton Docling fonctionne (pas de re-download)
4. Debug si nÃ©cessaire

---

**FIN DU PLAN**

Ce plan est prÃªt Ã  Ãªtre exÃ©cutÃ©. Tous les fichiers Ã  modifier sont clairement identifiÃ©s. Les commandes sont prÃªtes Ã  copier-coller.

