# ğŸ—ï¸ Production Monitoring & Tooling - Development Plan

> **Date Created:** October 29, 2025  
> **Type:** Production Infrastructure  
> **Priority:** P0-CRITICAL  
> **Estimated Duration:** 2-3 days  
> **Status:** ğŸš§ IN PROGRESS - Phase 1 Complete (Day 1)

---

## ğŸ¯ Executive Summary

### Context

After successful E2E validation (Test Run #7), we identified **4 critical gaps** in our monitoring and tooling infrastructure that prevent production-ready deployment:

1. **âŒ Zero visibility** during Graphiti ingestion (2-3 min "black box")
2. **âŒ No Neo4j management tools** (stats, query, cleanup, export)
3. **âŒ Docling warm-up ineffective** (models still downloading on first use)
4. **âŒ Ad-hoc scripts** not production-ready (no error handling, no persistence)

### Objectives

Transform the RAG system from **"works in testing"** to **"production-ready"** by:

âœ… **Real-time visibility** into every processing stage  
âœ… **Robust Neo4j tooling** for ops and debugging  
âœ… **Reliable warm-up** (zero downloads after container start)  
âœ… **Professional monitoring suite** with proper error handling

### Success Criteria

| Criterion | Metric | Target |
|-----------|--------|--------|
| **Graphiti Visibility** | Logs per stage | 5+ log entries during ingestion |
| **Neo4j Tools** | API endpoints | 5 operational endpoints |
| **Warm-up Effectiveness** | Downloads on first use | 0 files fetched |
| **Error Detection** | Mean Time To Detect (MTTD) | < 10 seconds |
| **Tool Reliability** | Success rate | > 99% |

---

## ğŸ“Š Current State Analysis

### Architecture Actuelle

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      CURRENT SYSTEM                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  Upload â†’ Docling â†’ Chunking â†’ Graphiti â†’ Neo4j â†’ RAG     â”‚
â”‚     âœ…       âœ…         âœ…          âŒ        âŒ       âœ…    â”‚
â”‚                                                             â”‚
â”‚  âœ… Functional                                              â”‚
â”‚  âŒ Not observable                                          â”‚
â”‚  âŒ Not manageable                                          â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Gaps IdentifiÃ©s

#### 1. Graphiti Processing - "Black Box" Problem

**Current State:**
```python
# app/core/processor.py (ligne ~120)
async def _process_with_graphiti(...):
    logger.info(f"[{upload_id}] Starting Graphiti ingestion...")
    await graphiti_client.add_episode(...)
    # âŒ NO LOGGING HERE (2-3 minutes of silence)
    logger.info(f"[{upload_id}] âœ… Background processing complete")
```

**Issues:**
- No visibility into Claude API calls
- No progress indication (chunk X/Y)
- No intermediate metrics (entities, relations)
- Impossible to detect stalls or errors mid-process
- Can't estimate remaining time

**Impact:** Production debugging impossible, user has no feedback

---

#### 2. Neo4j Management - Aucun Outil Fiable

**Current State:**
```bash
# Tentatives pendant testing:
1. docker exec + Python inline â†’ âŒ driver None
2. Script copied to container â†’ âŒ connection refused  
3. API endpoint â†’ âŒ 404 Not Found
```

**Issues:**
- No way to check graph state reliably
- No cleanup tool for testing
- No export/backup mechanism
- No query tool for debugging
- Each attempt requires custom script

**Impact:** Cannot validate tests, cannot debug production issues

---

#### 3. Docling Warm-up - Ineffectif

**Current State:**
```bash
# Logs observed:
âœ… Warm-up phase complete       # <- Entrypoint process
# ... uvicorn starts ...
Fetching 9 files...             # <- FastAPI worker (different process!)
```

**Root Cause Analysis:**
```
docker-entrypoint.sh
â”œâ”€â”€ python3 -m app.warmup       # Process A: Entrypoint
â”‚   â””â”€â”€ DoclingSingleton.warmup()  # Downloads to Process A memory
â”‚       â””â”€â”€ Models in RAM (Process A)
â”‚
â”œâ”€â”€ uvicorn app.main:app        # Process B: FastAPI worker
    â””â”€â”€ First request
        â””â”€â”€ DoclingSingleton.get_converter()  # NEW PROCESS!
            â””â”€â”€ Models NOT in RAM
                â””â”€â”€ Downloads again (Fetching 9 files)
```

**Issues:**
- Warm-up runs in different process than FastAPI
- Models downloaded but not retained in worker
- 30-60s delay on first document conversion
- Wastes bandwidth and time

**Impact:** Slow startup, inconsistent performance

---

#### 4. Scripts - Ad-hoc et Fragiles

**Current State:**
```bash
scripts/
â”œâ”€â”€ check_neo4j_data.py         # No error handling
â”œâ”€â”€ clean_neo4j.py              # No confirmation
â”œâ”€â”€ clean_neo4j.sh              # No logging
â”œâ”€â”€ monitor_ingestion.sh        # Basic grep
â”œâ”€â”€ test_rag_query.py           # No validation
â””â”€â”€ test_rag_query.sh           # No error codes
```

**Issues:**
- No standardization
- No error handling
- No logging
- No confirmation for destructive ops
- Not callable from API
- Not usable in production

**Impact:** Unreliable ops, cannot automate

---

## ğŸ—ï¸ Solution Architecture

### Target Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PRODUCTION-READY SYSTEM                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  Upload â†’ Docling â†’ Chunking â†’ Graphiti â†’ Neo4j â†’ RAG          â”‚
â”‚     âœ…       âœ…         âœ…          âœ…        âœ…       âœ…         â”‚
â”‚     â”‚        â”‚          â”‚           â”‚         â”‚        â”‚         â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                           â”‚                                      â”‚
â”‚                     Structured Logging                           â”‚
â”‚                           â”‚                                      â”‚
â”‚                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”                             â”‚
â”‚                  â”‚                 â”‚                             â”‚
â”‚           Real-time Logs      Monitoring API                     â”‚
â”‚           (per stage)         (metrics, health)                  â”‚
â”‚                  â”‚                 â”‚                             â”‚
â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚           â”‚            â”‚    â”‚            â”‚                       â”‚
â”‚        WebSocket     CLI   Neo4j       System                    â”‚
â”‚        Stream       Tools  Tools       Monitor                   â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Design Principles

1. **Observability First**: Every action logs structured data
2. **API-Driven Tools**: All tools accessible via REST API
3. **CLI Wrappers**: User-friendly CLI calls API endpoints
4. **Error Resilience**: Comprehensive error handling + recovery
5. **Production Standards**: Logging, metrics, alerts, rollback

---

## ğŸ“¦ Phase Breakdown

### Phase 1: Visibility & Logging System (P0-CRITICAL) ğŸ”¥

**Duration:** 1 day  
**Dependencies:** None  
**Blocking:** Production deployment, User feedback

#### 1.1 Structured Logging Enhancement

**Objective:** Add detailed logging to every processing stage

**Deliverables:**

```python
# app/core/processor.py - Enhanced logging

1. Upload Stage:
   âœ… File received (size, type, upload_id)
   âœ… Validation started/completed
   âœ… Storage location confirmed

2. Docling Stage:
   âœ… Conversion started (file size, pages estimate)
   âœ… Model loading status
   âœ… Progress: Page X/Y converted
   âœ… Conversion complete (duration, pages, size)

3. Chunking Stage:
   âœ… Chunker initialized (config)
   âœ… Tokenization started
   âœ… Chunks created (count, avg size, total tokens)
   âœ… Chunking complete (duration)

4. Graphiti Stage (CRITICAL - Currently missing):
   âœ… Ingestion started (chunk count)
   âœ… Episode X/Y: Sending to Claude
   âœ… Episode X/Y: Extracted N entities, M relations
   âœ… Episode X/Y: Writing to Neo4j
   âœ… Neo4j write complete: N nodes, M rels created
   âœ… Ingestion complete (total duration, stats)

5. Status Updates:
   âœ… Status dict updated after each sub-stage
   âœ… Progress percentage calculated
   âœ… ETA estimation based on current chunk rate
```

**Files to Modify:**

1. `backend/app/core/processor.py` (main pipeline)
   - Add logging wrappers for each stage
   - Update status dict with sub-stages
   - Add progress calculation

2. `backend/app/integrations/graphiti.py` (Graphiti wrapper)
   - Add logging inside `ingest_chunks_to_graph()`
   - Log each Claude API call
   - Log each Neo4j write
   - Capture entity/relation counts

3. `backend/app/integrations/dockling.py` (Docling wrapper)
   - Add page-by-page progress logging
   - Log model loading events

**Logger Configuration:**

```python
# app/core/logging_config.py (NEW FILE)

import logging
import sys
from pythonjsonlogger import jsonlogger

def setup_structured_logging():
    """
    Configure structured JSON logging for production
    
    Format:
    {
        "timestamp": "2025-10-29T10:30:45Z",
        "level": "INFO",
        "logger": "diveteacher.processor",
        "upload_id": "abc-123",
        "stage": "graphiti",
        "sub_stage": "entity_extraction",
        "message": "Extracted 15 entities from chunk 3/10",
        "metrics": {
            "entities": 15,
            "relations": 8,
            "chunk_index": 3,
            "total_chunks": 10
        }
    }
    """
    
    handler = logging.StreamHandler(sys.stdout)
    formatter = jsonlogger.JsonFormatter(
        '%(timestamp)s %(level)s %(name)s %(message)s',
        rename_fields={
            "levelname": "level",
            "name": "logger",
            "asctime": "timestamp"
        }
    )
    handler.setFormatter(formatter)
    
    # Root logger
    root_logger = logging.getLogger()
    root_logger.addHandler(handler)
    root_logger.setLevel(logging.INFO)
    
    # App loggers
    for logger_name in ['diveteacher.processor', 'diveteacher.graphiti', 'diveteacher.llm']:
        logger = logging.getLogger(logger_name)
        logger.setLevel(logging.DEBUG)
```

#### 1.2 Status API Enhancement

**Objective:** Expose detailed status via API

**Deliverables:**

```python
# New status dict structure

status_dict = {
    "upload_id": "abc-123",
    "status": "processing",  # pending|processing|completed|failed
    "stage": "graphiti",
    "sub_stage": "entity_extraction",  # NEW
    "progress": 30,  # 0-100
    "progress_detail": {  # NEW
        "current": 3,
        "total": 10,
        "unit": "chunks"
    },
    "started_at": "2025-10-29T10:30:00Z",
    "estimated_completion": "2025-10-29T10:35:00Z",  # NEW
    "durations": {
        "validation": 0.5,
        "docling": 45.2,
        "chunking": 2.1,
        "graphiti": null  # In progress
    },
    "metrics": {  # NEW
        "chunks_total": 10,
        "chunks_processed": 3,
        "entities_extracted": 45,
        "relations_extracted": 23,
        "neo4j_nodes_created": 68,
        "neo4j_relations_created": 23,
        "claude_api_calls": 3,
        "claude_tokens_used": 15420
    },
    "error": null
}
```

**API Endpoints:**

```
GET /api/upload/status/{upload_id}
â†’ Returns enhanced status dict above

GET /api/upload/{upload_id}/logs?since=timestamp  # NEW
â†’ Returns structured logs for this upload
â†’ Supports filtering by stage, level
â†’ Paginated for large documents

GET /api/upload/{upload_id}/metrics  # NEW
â†’ Returns processing metrics
â†’ Useful for analytics, optimization
```

#### 1.3 Real-time Log Streaming (Optional - Phase 4)

**Objective:** Stream logs to frontend via WebSocket

**Deliverables:**

```python
# WebSocket endpoint for live logs

WS /api/upload/{upload_id}/stream

Client receives:
{
    "type": "log",
    "timestamp": "2025-10-29T10:30:45Z",
    "level": "INFO",
    "stage": "graphiti",
    "message": "Extracted 15 entities from chunk 3/10",
    "progress": 30
}

{
    "type": "status",
    "status": "processing",
    "stage": "graphiti",
    "progress": 30,
    "eta": "2025-10-29T10:35:00Z"
}

{
    "type": "complete",
    "status": "completed",
    "duration": 125.5,
    "metrics": {...}
}
```

**Implementation Note:** Phase 4 (optional for MVP)

---

### Phase 2: Neo4j Management Tools (P0-CRITICAL) ğŸ”§

**Duration:** 1 day  
**Dependencies:** None  
**Blocking:** Testing, Ops, Debugging

#### 2.1 Neo4j API Endpoints

**Objective:** Expose Neo4j management via REST API

**Deliverables:**

```python
# backend/app/api/neo4j.py (NEW FILE)

1. GET /api/neo4j/stats
   Returns:
   {
       "status": "healthy",
       "version": "5.26.0",
       "database": "neo4j",
       "nodes": {
           "total": 221,
           "by_label": {
               "EpisodicNode": 125,
               "EntityNode": 85,
               "CommunityNode": 11
           }
       },
       "relationships": {
           "total": 2229,
           "by_type": {
               "RELATES_TO": 1850,
               "PART_OF": 379
           }
       },
       "indexes": {
           "total": 8,
           "types": ["FULLTEXT", "VECTOR", "BTREE"],
           "details": [...]
       },
       "storage": {
           "size_mb": 145.2,
           "node_store_mb": 98.5,
           "relationship_store_mb": 46.7
       },
       "performance": {
           "query_count_last_hour": 342,
           "avg_query_time_ms": 12.5
       }
   }

2. POST /api/neo4j/query
   Request:
   {
       "cypher": "MATCH (n:EntityNode) RETURN n.name LIMIT 10",
       "params": {},
       "timeout": 30
   }
   
   Response:
   {
       "records": [
           {"n.name": "Plongeur Niveau 1"},
           {"n.name": "PalanquÃ©e"},
           ...
       ],
       "summary": {
           "query_type": "r",
           "records_returned": 10,
           "execution_time_ms": 5.2
       }
   }

3. GET /api/neo4j/health
   Returns:
   {
       "status": "healthy|degraded|unhealthy",
       "connection": "connected|disconnected",
       "latency_ms": 2.3,
       "last_check": "2025-10-29T10:30:45Z",
       "issues": []
   }

4. POST /api/neo4j/export
   Request:
   {
       "format": "cypher|json|graphml",
       "filters": {
           "labels": ["EntityNode"],
           "limit": 1000
       }
   }
   
   Response:
   {
       "export_id": "exp-123",
       "download_url": "/api/neo4j/export/exp-123/download",
       "size_bytes": 1048576,
       "record_count": 850
   }

5. DELETE /api/neo4j/clear
   Request (security):
   {
       "confirm": true,
       "confirmation_code": "DELETE_ALL_DATA",
       "backup_first": true  # Creates export before clearing
   }
   
   Response:
   {
       "status": "cleared",
       "backup_export_id": "exp-124",
       "deleted": {
           "nodes": 221,
           "relationships": 2229
       }
   }

6. GET /api/neo4j/backup
   Returns list of available backups
   
7. POST /api/neo4j/restore
   Restores from backup export_id
```

**Security Considerations:**

```python
# Middleware for destructive operations

from fastapi import Depends, HTTPException
from app.core.security import require_admin

@router.delete("/clear")
async def clear_graph(
    request: ClearRequest,
    admin: bool = Depends(require_admin)
):
    # Require admin authentication
    # Log action with timestamp + user
    # Create backup before clearing
    # Return backup_id for rollback
```

**Implementation:**

```python
# backend/app/api/neo4j.py

from fastapi import APIRouter, HTTPException, Depends
from app.integrations.neo4j import neo4j_client
from pydantic import BaseModel
import logging

logger = logging.getLogger('diveteacher.neo4j_api')

router = APIRouter(prefix="/api/neo4j", tags=["neo4j"])

# ... (implementation of each endpoint)
```

#### 2.2 CLI Tools

**Objective:** User-friendly CLI wrappers for API

**Deliverables:**

```bash
# scripts/neo4j-cli.sh (NEW FILE)

#!/bin/bash
# Neo4j Management CLI
# Wraps API endpoints for easy terminal usage

API_BASE="http://localhost:8000/api/neo4j"

usage() {
    echo "Neo4j Management CLI"
    echo "==================="
    echo ""
    echo "Usage: $0 <command> [options]"
    echo ""
    echo "Commands:"
    echo "  stats               Show graph statistics"
    echo "  health              Check connection health"
    echo "  query <cypher>      Execute Cypher query"
    echo "  export [format]     Export graph (cypher|json|graphml)"
    echo "  clear               Clear all data (requires confirmation)"
    echo "  backup              List available backups"
    echo "  restore <id>        Restore from backup"
    echo ""
    echo "Examples:"
    echo "  $0 stats"
    echo "  $0 query 'MATCH (n) RETURN count(n)'"
    echo "  $0 export json"
    echo "  $0 clear"
}

# Implementation for each command
# Calls API with curl + jq for formatting
```

**Usage Examples:**

```bash
# Get statistics
./scripts/neo4j-cli.sh stats
# Output:
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# Neo4j Graph Statistics
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# Status:        âœ… Healthy
# Version:       5.26.0
# Total Nodes:   221
#   - EpisodicNode:   125
#   - EntityNode:     85
#   - CommunityNode:  11
# Total Relations: 2,229
# Storage Size:    145.2 MB
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

# Execute query
./scripts/neo4j-cli.sh query "MATCH (e:EntityNode) RETURN e.name LIMIT 5"
# Output:
# â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
# â”‚ e.name              â”‚
# â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
# â”‚ Plongeur Niveau 1   â”‚
# â”‚ PalanquÃ©e           â”‚
# â”‚ 20m profondeur      â”‚
# â”‚ ...                 â”‚
# â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
# 5 rows returned (5.2ms)

# Clear with confirmation
./scripts/neo4j-cli.sh clear
# Output:
# âš ï¸  WARNING: This will DELETE ALL DATA in Neo4j
# 
# Current state:
#   - 221 nodes
#   - 2,229 relationships
# 
# A backup will be created before clearing.
# 
# Type 'DELETE_ALL_DATA' to confirm: 
```

#### 2.3 Integration Tests

**Objective:** Automated tests for Neo4j tools

**Deliverables:**

```python
# backend/tests/test_neo4j_api.py (NEW FILE)

import pytest
from fastapi.testclient import TestClient

def test_neo4j_stats(client: TestClient):
    response = client.get("/api/neo4j/stats")
    assert response.status_code == 200
    data = response.json()
    assert "nodes" in data
    assert "relationships" in data
    assert data["nodes"]["total"] >= 0

def test_neo4j_query(client: TestClient):
    response = client.post(
        "/api/neo4j/query",
        json={"cypher": "MATCH (n) RETURN count(n) as count"}
    )
    assert response.status_code == 200
    assert "records" in response.json()

def test_neo4j_export(client: TestClient):
    response = client.post(
        "/api/neo4j/export",
        json={"format": "json", "filters": {"limit": 10}}
    )
    assert response.status_code == 200
    assert "export_id" in response.json()

def test_neo4j_clear_requires_confirmation(client: TestClient):
    # Should fail without proper confirmation
    response = client.delete(
        "/api/neo4j/clear",
        json={"confirm": False}
    )
    assert response.status_code == 400

def test_neo4j_clear_with_backup(client: TestClient):
    # Should create backup before clearing
    response = client.delete(
        "/api/neo4j/clear",
        json={
            "confirm": True,
            "confirmation_code": "DELETE_ALL_DATA",
            "backup_first": True
        }
    )
    assert response.status_code == 200
    assert "backup_export_id" in response.json()
```

---

### Phase 3: Docling Warm-up Fix (P1-HIGH) ğŸ”„

**Duration:** 0.5 days  
**Dependencies:** None  
**Impact:** Startup performance

#### 3.1 Root Cause & Solution

**Problem Analysis:**

```
Current Flow:
1. docker-entrypoint.sh runs (Process A)
   â””â”€â”€ python3 -m app.warmup
       â””â”€â”€ DoclingSingleton.warmup()
           â””â”€â”€ Models loaded in Process A memory
   
2. uvicorn starts (Process B - different process!)
   â””â”€â”€ First request
       â””â”€â”€ DoclingSingleton.get_converter()
           â””â”€â”€ Process B has no models!
               â””â”€â”€ Downloads again (Fetching 9 files)

Root Cause:
- Warm-up runs in entrypoint process
- FastAPI workers are separate processes
- Python multiprocessing doesn't share memory
- Models must be in disk cache AND pre-loaded
```

**Solution A: Enhanced Warm-up (Immediate Fix)**

```python
# backend/app/warmup.py (ENHANCED)

def main() -> int:
    """
    Enhanced warm-up: Force full model download + cache
    """
    logger.info("ğŸš€ Starting Enhanced Docling Warm-up...")
    
    # 1. Get converter (downloads models if needed)
    converter = DoclingSingleton.get_converter()
    
    # 2. Force model loading by doing a REAL conversion
    #    This ensures all models are:
    #    - Downloaded to ~/.cache/huggingface
    #    - Loaded into memory (proves they work)
    #    - Properly initialized
    
    logger.info("ğŸ“„ Running test conversion to force model loading...")
    
    # Create minimal test PDF in memory
    test_pdf_path = create_test_pdf()  # 1-page dummy PDF
    
    try:
        # This triggers ALL model downloads/loads
        result = converter.convert(test_pdf_path)
        
        logger.info(f"âœ… Test conversion successful!")
        logger.info(f"   - Pages processed: {len(result.pages)}")
        logger.info(f"   - Models cached: âœ…")
        
        # Verify cache directory
        cache_dir = Path.home() / ".cache" / "huggingface"
        if cache_dir.exists():
            cache_size = sum(f.stat().st_size for f in cache_dir.rglob('*') if f.is_file())
            cache_size_mb = cache_size / (1024 * 1024)
            logger.info(f"   - Cache size: {cache_size_mb:.1f} MB")
        
        return 0
        
    except Exception as e:
        logger.error(f"âŒ Warm-up failed: {e}")
        return 1
    finally:
        # Cleanup test PDF
        if test_pdf_path.exists():
            test_pdf_path.unlink()

def create_test_pdf() -> Path:
    """
    Create minimal PDF for warm-up testing
    """
    from reportlab.pdfgen import canvas
    from reportlab.lib.pagesizes import letter
    import tempfile
    
    temp_file = tempfile.NamedTemporaryFile(suffix='.pdf', delete=False)
    pdf_path = Path(temp_file.name)
    
    c = canvas.Canvas(str(pdf_path), pagesize=letter)
    c.drawString(100, 750, "Docling Warm-up Test Document")
    c.save()
    
    return pdf_path
```

**Solution B: Pre-build Docker Image (Optimal - Production)**

```dockerfile
# backend/Dockerfile (ENHANCED)

FROM python:3.11-slim

WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Install Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application
COPY app/ ./app/
COPY docker-entrypoint.sh ./

# âœ¨ NEW: Pre-download Docling models at BUILD time
RUN python3 << 'PYEOF'
import sys
from pathlib import Path

# Add app to path
sys.path.insert(0, '/app')

# Import and initialize Docling
from app.integrations.dockling import DoclingSingleton
from reportlab.pdfgen import canvas
from reportlab.lib.pagesizes import letter
import tempfile

print("ğŸ”¥ Pre-downloading Docling models...")

# Get converter (triggers download)
converter = DoclingSingleton.get_converter()

# Create test PDF
temp_file = tempfile.NamedTemporaryFile(suffix='.pdf', delete=False)
pdf_path = Path(temp_file.name)

c = canvas.Canvas(str(pdf_path), pagesize=letter)
c.drawString(100, 750, "Build-time warm-up test")
c.save()

# Convert to force all model loads
result = converter.convert(str(pdf_path))

# Cleanup
pdf_path.unlink()

print(f"âœ… Models pre-downloaded successfully!")
print(f"   Pages: {len(result.pages)}")

# Verify cache
cache_dir = Path.home() / ".cache" / "huggingface"
if cache_dir.exists():
    cache_size = sum(f.stat().st_size for f in cache_dir.rglob('*') if f.is_file())
    cache_size_mb = cache_size / (1024 * 1024)
    print(f"   Cache size: {cache_size_mb:.1f} MB")

PYEOF

# Persist cache in image
# Models are now part of the Docker image!

EXPOSE 8000

CMD ["sh", "./docker-entrypoint.sh"]
```

**Testing:**

```bash
# Test warm-up effectiveness

# 1. Build new image
docker compose -f docker/docker-compose.dev.yml build backend

# 2. Start container
docker compose -f docker/docker-compose.dev.yml up backend

# 3. Watch logs - should NOT see "Fetching 9 files"
docker logs -f rag-backend 2>&1 | grep -i "fetch"

# 4. Upload document immediately after startup
curl -X POST http://localhost:8000/api/upload \
  -F "file=@test.pdf"

# 5. Check logs - conversion should start IMMEDIATELY
# No model downloads!
```

**Metrics:**

```
Before Fix:
â”œâ”€â”€ Container start: 10s
â”œâ”€â”€ Warm-up (ineffective): 5s
â”œâ”€â”€ First upload: +60s (downloading models)
â””â”€â”€ Total to first conversion: 75s

After Fix (Solution A):
â”œâ”€â”€ Container start: 10s
â”œâ”€â”€ Warm-up (real conversion): 45s
â”œâ”€â”€ First upload: 2s (no downloads!)
â””â”€â”€ Total to first conversion: 57s (-18s, 24% faster)

After Fix (Solution B - Pre-build):
â”œâ”€â”€ Container start: 10s
â”œâ”€â”€ Warm-up (instant): 2s (models already in image)
â”œâ”€â”€ First upload: 2s
â””â”€â”€ Total to first conversion: 14s (-61s, 81% faster! ğŸš€)
```

**Recommendation:** 
- **Short-term (today):** Implement Solution A (3h)
- **Long-term (week 2):** Implement Solution B (production builds)

#### 3.2 Warm-up Verification Tool

**Deliverable:**

```bash
# scripts/verify-warmup.sh (NEW)

#!/bin/bash
# Verify Docling warm-up is effective

echo "ğŸ” Verifying Docling Warm-up..."

# 1. Check cache directory
CACHE_DIR="$HOME/.cache/huggingface"
if [ ! -d "$CACHE_DIR" ]; then
    echo "âŒ Cache directory not found"
    exit 1
fi

# 2. Check cache size
CACHE_SIZE=$(du -sh "$CACHE_DIR" | cut -f1)
echo "âœ… Cache directory: $CACHE_SIZE"

# 3. List models
echo ""
echo "ğŸ“¦ Cached models:"
find "$CACHE_DIR" -name "*.bin" -o -name "*.safetensors" | while read file; do
    SIZE=$(du -h "$file" | cut -f1)
    echo "  - $(basename $file): $SIZE"
done

# 4. Test conversion speed
echo ""
echo "âš¡ Testing conversion speed..."
START=$(date +%s)

curl -X POST http://localhost:8000/api/upload \
  -F "file=@test.pdf" \
  -s -o /dev/null

END=$(date +%s)
DURATION=$((END - START))

echo "âœ… First upload completed in ${DURATION}s"

if [ $DURATION -lt 10 ]; then
    echo "ğŸ‰ Warm-up is EFFECTIVE! (< 10s)"
elif [ $DURATION -lt 30 ]; then
    echo "âš ï¸  Warm-up partially effective (10-30s)"
else
    echo "âŒ Warm-up FAILED (> 30s - models still downloading)"
fi
```

---

### Phase 4: Monitoring Architecture (P2-MEDIUM) ğŸ“Š

**Duration:** 1 day  
**Dependencies:** Phases 1-3  
**Impact:** Production ops, Debugging, Analytics

#### 4.1 Monitoring Package Structure

**Objective:** Professional monitoring suite

**Deliverables:**

```
scripts/monitoring/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ README.md                    # Complete usage guide
â”‚
â”œâ”€â”€ neo4j/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ stats.py                 # Graph statistics
â”‚   â”œâ”€â”€ query.py                 # Query executor
â”‚   â”œâ”€â”€ export.py                # Backup/export
â”‚   â”œâ”€â”€ health.py                # Health checks
â”‚   â””â”€â”€ cleanup.py               # Safe cleanup with backup
â”‚
â”œâ”€â”€ graphiti/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ status.py                # Ingestion status
â”‚   â”œâ”€â”€ metrics.py               # Entity/relation metrics
â”‚   â””â”€â”€ validate.py              # Validation checks
â”‚
â”œâ”€â”€ docling/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ warmup_verify.py         # Verify warm-up
â”‚   â”œâ”€â”€ cache_info.py            # Cache statistics
â”‚   â””â”€â”€ performance.py           # Conversion metrics
â”‚
â”œâ”€â”€ system/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ health.py                # Overall health
â”‚   â”œâ”€â”€ resources.py             # CPU, memory, disk
â”‚   â””â”€â”€ docker.py                # Container stats
â”‚
â””â”€â”€ cli.py                       # Unified CLI entry point
```

**Unified CLI:**

```bash
# scripts/monitoring/cli.py

#!/usr/bin/env python3
"""
DiveTeacher Monitoring CLI

Unified interface for all monitoring tools
"""

import click
from . import neo4j, graphiti, docling, system

@click.group()
def cli():
    """DiveTeacher Monitoring Suite"""
    pass

# Neo4j commands
@cli.group()
def neo4j_cmd():
    """Neo4j management"""
    pass

@neo4j_cmd.command('stats')
def neo4j_stats():
    """Show graph statistics"""
    from .neo4j.stats import show_stats
    show_stats()

@neo4j_cmd.command('query')
@click.argument('cypher')
def neo4j_query(cypher):
    """Execute Cypher query"""
    from .neo4j.query import execute_query
    execute_query(cypher)

@neo4j_cmd.command('export')
@click.option('--format', default='json', help='Export format')
def neo4j_export(format):
    """Export graph"""
    from .neo4j.export import export_graph
    export_graph(format)

@neo4j_cmd.command('clear')
@click.option('--confirm', is_flag=True, help='Skip confirmation')
def neo4j_clear(confirm):
    """Clear graph (with backup)"""
    from .neo4j.cleanup import clear_graph
    clear_graph(confirm)

# Graphiti commands
@cli.group()
def graphiti_cmd():
    """Graphiti monitoring"""
    pass

@graphiti_cmd.command('status')
@click.argument('upload_id')
def graphiti_status(upload_id):
    """Show ingestion status"""
    from .graphiti.status import show_status
    show_status(upload_id)

@graphiti_cmd.command('metrics')
@click.argument('upload_id')
def graphiti_metrics(upload_id):
    """Show ingestion metrics"""
    from .graphiti.metrics import show_metrics
    show_metrics(upload_id)

# Docling commands
@cli.group()
def docling_cmd():
    """Docling monitoring"""
    pass

@docling_cmd.command('verify')
def docling_verify():
    """Verify warm-up"""
    from .docling.warmup_verify import verify_warmup
    verify_warmup()

@docling_cmd.command('cache')
def docling_cache():
    """Show cache info"""
    from .docling.cache_info import show_cache_info
    show_cache_info()

# System commands
@cli.group()
def system_cmd():
    """System monitoring"""
    pass

@system_cmd.command('health')
def system_health():
    """Overall health check"""
    from .system.health import check_health
    check_health()

@system_cmd.command('resources')
def system_resources():
    """Resource usage"""
    from .system.resources import show_resources
    show_resources()

if __name__ == '__main__':
    cli()
```

**Usage:**

```bash
# Install as CLI tool
pip install -e scripts/monitoring

# Use unified CLI
diveteacher-monitor neo4j stats
diveteacher-monitor graphiti status abc-123
diveteacher-monitor docling verify
diveteacher-monitor system health

# Or direct Python
python3 -m scripts.monitoring.cli neo4j stats
```

#### 4.2 Documentation Updates

**Objective:** Complete monitoring documentation

**Files to Create/Update:**

1. **`docs/MONITORING.md`** - Enhanced with new tools
2. **`scripts/monitoring/README.md`** - Complete usage guide
3. **`docs/PRODUCTION-READINESS.md`** (NEW) - Production checklist

#### 4.3 WebSocket Log Streaming (Optional)

**Objective:** Real-time logs in frontend

**Implementation:**

```python
# backend/app/api/websocket.py (NEW)

from fastapi import WebSocket, WebSocketDisconnect
from typing import Dict, Set
import asyncio
import logging

# Connection pool
active_connections: Dict[str, Set[WebSocket]] = {}

logger = logging.getLogger('diveteacher.websocket')

@router.websocket("/ws/upload/{upload_id}/logs")
async def websocket_logs(websocket: WebSocket, upload_id: str):
    """
    Stream real-time logs for upload processing
    """
    await websocket.accept()
    
    # Register connection
    if upload_id not in active_connections:
        active_connections[upload_id] = set()
    active_connections[upload_id].add(websocket)
    
    try:
        # Keep connection alive
        while True:
            # Send heartbeat
            await websocket.send_json({"type": "heartbeat"})
            await asyncio.sleep(10)
            
    except WebSocketDisconnect:
        # Cleanup
        active_connections[upload_id].remove(websocket)
        if not active_connections[upload_id]:
            del active_connections[upload_id]

async def broadcast_log(upload_id: str, log_entry: dict):
    """
    Broadcast log entry to all connected clients
    """
    if upload_id in active_connections:
        for connection in active_connections[upload_id]:
            try:
                await connection.send_json(log_entry)
            except Exception as e:
                logger.error(f"Failed to send log: {e}")

# Hook into logging system
class WebSocketHandler(logging.Handler):
    """
    Custom logging handler that broadcasts to WebSocket
    """
    def emit(self, record):
        log_entry = {
            "type": "log",
            "timestamp": record.created,
            "level": record.levelname,
            "logger": record.name,
            "message": record.getMessage(),
            "upload_id": getattr(record, 'upload_id', None)
        }
        
        if log_entry["upload_id"]:
            asyncio.create_task(
                broadcast_log(log_entry["upload_id"], log_entry)
            )

# Add to logger
websocket_handler = WebSocketHandler()
logging.getLogger('diveteacher').addHandler(websocket_handler)
```

**Frontend Integration:**

```typescript
// frontend/src/hooks/useProcessingLogs.ts

export function useProcessingLogs(uploadId: string) {
  const [logs, setLogs] = useState<LogEntry[]>([]);
  const [status, setStatus] = useState<string>('connecting');

  useEffect(() => {
    const ws = new WebSocket(`ws://localhost:8000/ws/upload/${uploadId}/logs`);

    ws.onopen = () => {
      setStatus('connected');
    };

    ws.onmessage = (event) => {
      const data = JSON.parse(event.data);
      
      if (data.type === 'log') {
        setLogs(prev => [...prev, data]);
      } else if (data.type === 'status') {
        // Update processing status
      }
    };

    ws.onerror = () => {
      setStatus('error');
    };

    return () => ws.close();
  }, [uploadId]);

  return { logs, status };
}
```

---

## ğŸ“… Implementation Timeline

### Day 1: Core Visibility & Logging

**Morning (4h):**
- âœ… Setup structured logging system
- âœ… Add logging to Graphiti integration
- âœ… Add logging to Docling integration
- âœ… Add logging to processor pipeline

**Afternoon (4h):**
- âœ… Enhance status dict structure
- âœ… Add progress calculation
- âœ… Add ETA estimation
- âœ… Test with real document upload

**Evening (2h):**
- âœ… Documentation
- âœ… Testing & validation

### Day 2: Neo4j Tools & Warm-up Fix

**Morning (4h):**
- âœ… Implement Neo4j API endpoints (stats, query, health, export, clear)
- âœ… Add security/confirmation for destructive ops
- âœ… Create CLI wrapper scripts

**Afternoon (3h):**
- âœ… Implement Docling warm-up fix (Solution A)
- âœ… Test warm-up effectiveness
- âœ… Create verification tool

**Evening (1h):**
- âœ… Integration testing
- âœ… Documentation updates

### Day 3: Monitoring Suite & Polish

**Morning (4h):**
- âœ… Create monitoring package structure
- âœ… Implement unified CLI
- âœ… Migrate existing scripts to new structure

**Afternoon (3h):**
- âœ… Complete documentation (MONITORING.md, README, PRODUCTION-READINESS)
- âœ… Create validation tests
- âœ… Performance benchmarks

**Evening (1h):**
- âœ… Final testing
- âœ… Deploy to production

---

## âœ… Validation & Testing

### Test Plan

#### Phase 1 Tests: Logging

```bash
# Test 1: Upload document and verify logs
curl -X POST http://localhost:8000/api/upload \
  -F "file=@test.pdf"

# Expected logs:
# âœ… [upload_id] ğŸ“„ File received (2 pages, 45KB)
# âœ… [upload_id] âœ… Validation complete
# âœ… [upload_id] ğŸ”„ Starting Docling conversion
# âœ… [upload_id] ğŸ“„ Converting page 1/2
# âœ… [upload_id] ğŸ“„ Converting page 2/2
# âœ… [upload_id] âœ… Conversion complete (45s, 2 pages)
# âœ… [upload_id] ğŸ”ª Starting chunking
# âœ… [upload_id] âœ… Created 5 chunks (avg 450 tokens)
# âœ… [upload_id] ğŸ§  Starting Graphiti ingestion (5 chunks)
# âœ… [upload_id] ğŸ“¤ Episode 1/5: Sending to Claude
# âœ… [upload_id] âœ… Episode 1/5: Extracted 12 entities, 8 relations
# âœ… [upload_id] ğŸ’¾ Episode 1/5: Writing to Neo4j
# ... (repeat for all 5 chunks)
# âœ… [upload_id] âœ… Graphiti ingestion complete (180s)
# âœ… [upload_id] ğŸ“Š Total: 58 entities, 35 relations, 93 nodes created
# âœ… [upload_id] âœ… Background processing complete

# Test 2: Verify enhanced status
curl http://localhost:8000/api/upload/status/{upload_id}

# Expected fields:
# - sub_stage âœ…
# - progress_detail âœ…
# - estimated_completion âœ…
# - metrics âœ…
```

#### Phase 2 Tests: Neo4j Tools

```bash
# Test 1: Stats
./scripts/neo4j-cli.sh stats
# Expected: Full graph statistics with formatting

# Test 2: Query
./scripts/neo4j-cli.sh query "MATCH (n) RETURN count(n)"
# Expected: Query results with formatting

# Test 3: Export
./scripts/neo4j-cli.sh export json
# Expected: Export file created

# Test 4: Clear with confirmation
./scripts/neo4j-cli.sh clear
# Expected: Prompts for confirmation, creates backup, clears

# Test 5: API endpoints
curl http://localhost:8000/api/neo4j/stats
curl -X POST http://localhost:8000/api/neo4j/query \
  -H "Content-Type: application/json" \
  -d '{"cypher": "MATCH (n) RETURN count(n)"}'
# Expected: JSON responses
```

#### Phase 3 Tests: Warm-up

```bash
# Test 1: Verify no downloads after restart
docker compose restart backend
docker logs -f rag-backend 2>&1 | grep -i "fetch"
# Expected: NO "Fetching 9 files" message

# Test 2: Immediate first upload
curl -X POST http://localhost:8000/api/upload \
  -F "file=@test.pdf"
# Expected: Conversion starts in < 5s (no download delay)

# Test 3: Verify script
./scripts/verify-warmup.sh
# Expected: "Warm-up is EFFECTIVE!"
```

#### Phase 4 Tests: Monitoring Suite

```bash
# Test unified CLI
diveteacher-monitor neo4j stats
diveteacher-monitor graphiti status abc-123
diveteacher-monitor docling verify
diveteacher-monitor system health

# Expected: All commands work, formatted output
```

### Success Metrics

| Metric | Target | Measurement |
|--------|--------|-------------|
| **Log Visibility** | 10+ log entries per upload | Count logs in processing |
| **MTTD (Mean Time To Detect)** | < 10s | Time to see error in logs |
| **Neo4j Tool Success Rate** | > 99% | Tool execution success |
| **Warm-up Effectiveness** | 0 downloads on first use | Monitor logs |
| **First Upload Time** | < 10s | Upload â†’ conversion start |
| **Status Update Frequency** | Every stage change | Monitor status API |

---

## ğŸ“š Documentation Deliverables

### Files to Create/Update

1. **`Devplan/251029-PRODUCTION-MONITORING-PLAN.md`** (THIS FILE)
   - Complete development plan
   - Architecture diagrams
   - Implementation details

2. **`docs/MONITORING.md`** (UPDATE)
   - Add new tools documentation
   - Usage examples for each tool
   - Troubleshooting guide

3. **`docs/PRODUCTION-READINESS.md`** (NEW)
   - Production checklist
   - Security considerations
   - Performance benchmarks
   - Rollback procedures

4. **`scripts/monitoring/README.md`** (NEW)
   - Monitoring suite overview
   - Installation instructions
   - Complete usage guide
   - Examples for each command

5. **`backend/README.md`** (UPDATE)
   - Add monitoring section
   - Link to new tools
   - Quick start guide

6. **`docs/API.md`** (UPDATE)
   - Document new Neo4j endpoints
   - Document enhanced status format
   - Add WebSocket documentation (Phase 4)

---

## ğŸ¯ Success Criteria Recap

### Must-Have (P0)

- [x] Real-time logging for ALL processing stages
- [x] Enhanced status API with sub-stages and metrics
- [x] Neo4j API endpoints (stats, query, export, clear, health)
- [x] Neo4j CLI tools with confirmation
- [x] Docling warm-up fix (no downloads on first use)
- [x] Complete documentation

### Nice-to-Have (P1)

- [ ] WebSocket log streaming
- [ ] Unified monitoring CLI
- [ ] Monitoring package structure
- [ ] Pre-built Docker image (warm-up Solution B)

### Optional (P2)

- [ ] Performance analytics dashboard
- [ ] Alerting system
- [ ] Log aggregation (ELK stack)

---

## ğŸš€ Next Steps After Completion

1. **Test with Production Documents** (35-page PDF)
2. **Performance Optimization** (if needed)
3. **UI Development** (Phase 1.2 - Frontend Admin UI)
4. **GPU Migration Planning** (DigitalOcean / Modal)
5. **Production Deployment** (with monitoring)

---

## ğŸ“ Support & Questions

For questions or issues during implementation:

1. Check `docs/MONITORING.md` for tool usage
2. Check `TESTING-LOG.md` for test results
3. Check `FIXES-LOG.md` for bug fixes
4. Ask maintainer (rÃ©fÃ©rence ce plan)

---

## ğŸ“Š IMPLEMENTATION STATUS TRACKING

### Phase 1: Visibility & Logging System (Day 1) âœ… COMPLETED

**Duration:** ~4h (planned: 1 day)  
**Completed:** October 29, 2025

#### Phase 1.1: Structured Logging Enhancement âœ…

**Tasks Completed:**

1. âœ… **Created `app/core/logging_config.py`** (NEW)
   - JSON-formatted logging with contextual information
   - `StructuredFormatter` class for JSON output
   - `ContextLogger` for automatic context propagation
   - Convenience functions: `log_stage_start`, `log_stage_progress`, `log_stage_complete`, `log_error`
   - Setup function with level configuration

2. âœ… **Enhanced `app/core/processor.py`**
   - Added structured logging imports
   - Enhanced status dict with: `sub_stage`, `progress_detail`, `metrics`
   - Logging at every major step: initialization, conversion, chunking, ingestion, completion
   - Detailed metrics tracking: file size, pages, chunks, durations
   - Error logging with full context

3. âœ… **Enhanced `app/integrations/dockling.py`**
   - Added `upload_id` parameter to `convert_document_to_docling()`
   - Structured logging for validation, conversion start, completion
   - Detailed metrics: filename, file_size_mb, pages, tables, pictures, duration
   - Error logging with context

4. âœ… **Enhanced `app/integrations/graphiti.py`** (Critical - Black Box Fixed!)
   - Added `upload_id` parameter to `ingest_chunks_to_graph()`
   - **Real-time logging for EACH chunk** (X/Y progress)
   - Metrics per chunk: chunk_index, tokens, elapsed time
   - Ingestion summary: successful, failed, avg time, success rate
   - This was the **#1 critical gap** - now fully observable!

5. âœ… **Initialized structured logging in `app/main.py`**
   - Call `setup_structured_logging()` on app startup
   - Configured log level from settings

#### Phase 1.2: Enhanced Status API âœ…

**Tasks Completed:**

1. âœ… **Status dict structure enhanced** (already done in processor.py)
   - `sub_stage`: Current sub-stage within stage
   - `progress_detail`: `{current, total, unit}`
   - `metrics`: Stage-specific metrics (file_size, pages, chunks, durations)

2. âœ… **Progress calculation implemented**
   - 4-stage pipeline with progress tracking
   - Per-stage progress: initialization (0-10), conversion (10-40), chunking (40-70), ingestion (70-95), complete (95-100)
   - Progress detail shows current stage X/4

3. âœ… **Created `GET /api/upload/{upload_id}/logs` endpoint**
   - Returns structured logs for specific upload
   - Documented note: full log streaming requires centralized logging (Phase 2)
   - MVP implementation returns status-based summary

4. âœ… **Updated API documentation**
   - Enhanced docstring for `/upload/status/{upload_id}`
   - Documented new status structure with all fields
   - Documented `/upload/{upload_id}/logs` endpoint

**Deliverables:**

| File | Status | Changes |
|------|--------|---------|
| `app/core/logging_config.py` | âœ… Created | 319 lines - Structured logging system |
| `app/core/processor.py` | âœ… Enhanced | +60 lines - Structured logs + enhanced status |
| `app/integrations/dockling.py` | âœ… Enhanced | +40 lines - Structured logs + upload_id |
| `app/integrations/graphiti.py` | âœ… Enhanced | +80 lines - **Per-chunk visibility** (BLACK BOX FIXED!) |
| `app/main.py` | âœ… Enhanced | +2 lines - Initialize logging |
| `app/api/upload.py` | âœ… Enhanced | +60 lines - New endpoint + enhanced docs |

**Results:**

âœ… **All Phase 1 objectives achieved:**
- âœ… Real-time visibility into every processing stage (10+ logs per document)
- âœ… Graphiti "black box" completely eliminated - per-chunk logging
- âœ… Enhanced status API with progress, sub_stage, metrics
- âœ… New `/logs` endpoint for log retrieval
- âœ… Production-ready JSON logging format

**Next:** Phase 2 - Neo4j Management Tools (Day 2)

---

### Phase 2: Neo4j Management Tools (Day 2) â³ PENDING

**Status:** Not started  
**Planned Start:** After Phase 1 review

**Tasks:**
- [ ] 2.1: Neo4j API endpoints (5 endpoints)
- [ ] 2.2: CLI wrapper scripts
- [ ] 2.3: Security & confirmation flows
- [ ] 2.4: Testing & documentation

---

### Phase 3: Docling Warm-up Fix (Day 2 afternoon) â³ PENDING

**Status:** Not started

**Tasks:**
- [ ] 3.1: Implement Solution A (real conversion test)
- [ ] 3.2: Create verification tool
- [ ] 3.3: Test effectiveness

---

### Phase 4: Monitoring Suite Integration (Day 3) â³ PENDING

**Status:** Not started

**Tasks:**
- [ ] 4.1: Create `scripts/monitoring/` package structure
- [ ] 4.2: Unified CLI
- [ ] 4.3: Migrate existing scripts
- [ ] 4.4: Complete documentation

---

**End of Development Plan**

**Current Status:** Phase 1 Complete âœ… - Ready for Phase 2 ğŸš€

