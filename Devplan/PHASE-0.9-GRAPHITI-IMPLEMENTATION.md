# üîó Phase 0.9 - Graphiti Knowledge Graph Implementation

> **Objectif:** Finaliser l'int√©gration Graphiti avec Docling et Neo4j pour un knowledge graph temporel fonctionnel  
> **Dur√©e Estim√©e:** 3-4 heures (r√©duite gr√¢ce √† OpenAI)  
> **Priorit√©:** CRITIQUE (bloque Phase 1)  
> **Date:** Octobre 27, 2025

> **üéØ Architecture LLM:**  
> - **Graphiti (Knowledge Graph):** OpenAI **GPT-5-nano** + text-embedding-3-small  
> - **RAG (User Queries):** Ollama Mistral 7b (local)  
> - **S√©paration totale:** Pas de m√©lange entre les deux stacks!  
> - **Performance:** 2M tokens/min (rate limit maximal OpenAI) - Ultra rapide!

---

## üìã Table des Mati√®res

- [Vue d'Ensemble](#vue-densemble)
- [Audit de l'Impl√©mentation Actuelle](#audit-de-limpl√©mentation-actuelle)
- [Probl√®mes Identifi√©s](#probl√®mes-identifi√©s)
- [Architecture Cible](#architecture-cible)
- [Plan d'Impl√©mentation D√©taill√©](#plan-dimpl√©mentation-d√©taill√©)
- [Tests de Validation](#tests-de-validation)
- [M√©triques de Succ√®s](#m√©triques-de-succ√®s)

---

## Vue d'Ensemble

### Contexte

**√âtat Actuel (Phase 0.8 Complete):**
- ‚úÖ Docling 2.5.1 + HierarchicalChunker op√©rationnel (436 chunks pour 35 pages)
- ‚úÖ Neo4j 5.25.1 + RAG indexes (fulltext, entity, hybrid search)
- ‚ö†Ô∏è  Graphiti 0.3.7 partiellement int√©gr√© (ingestion OK, recherche non optimale)

**Objectif Phase 0.9:**
- üéØ Mise √† jour Graphiti vers version 0.17.x
- üéØ Configuration LLM **OpenAI GPT-5-nano** pour Graphiti (extraction entities/relations + embeddings)
  - ‚ö° **2M tokens/min** - Le plus rapide et moins cher disponible!
- üéØ **Mistral 7b (Ollama)** reste pour RAG/User queries (pas de m√©lange!)
- üéØ Utilisation native `graphiti.search()` pour RAG
- üéØ Optimisation community building (p√©riodique, pas √† chaque upload)
- üéØ Tests E2E complets: PDF ‚Üí Docling ‚Üí Graphiti ‚Üí Neo4j ‚Üí RAG Query

### Valeur Ajout√©e

**Sans Graphiti optimis√©:**
- Recherche limit√©e √† full-text (BM25) + regex entities
- Pas de relations s√©mantiques entre concepts
- Pas de r√©solution temporelle des contradictions

**Avec Graphiti complet:**
- ‚úÖ Extraction automatique entit√©s + relations via LLM
- ‚úÖ Recherche hybride (semantic + BM25 + graph traversal)
- ‚úÖ Knowledge graph √©volutif (ajout incr√©mental sans recomputation)
- ‚úÖ Temporal awareness (valid_at, invalid_at pour contradictions)

---

## Audit de l'Impl√©mentation Actuelle

### ‚úÖ Points Forts

**1. Architecture Singleton Correcte**
```python
# backend/app/integrations/graphiti.py
_graphiti_client: Optional[Graphiti] = None
_indices_built: bool = False

async def get_graphiti_client() -> Graphiti:
    global _graphiti_client, _indices_built
    
    if _graphiti_client is None:
        _graphiti_client = Graphiti(
            uri=settings.NEO4J_URI,
            user=settings.NEO4J_USER,
            password=settings.NEO4J_PASSWORD
        )
    
    if not _indices_built:
        await _graphiti_client.build_indices_and_constraints()
        _indices_built = True
    
    return _graphiti_client
```
‚úÖ **Correct:** Initialisation unique, indices built une seule fois

**2. Param√®tres `add_episode()` Conformes**
```python
await client.add_episode(
    name=f"{metadata['filename']} - Chunk {chunk_index}",
    episode_body=chunk_text,
    source=EpisodeType.text,  # ‚úÖ 'source' pas 'episode_type'
    source_description=f"Document: {metadata['filename']}...",
    reference_time=datetime.now(timezone.utc),  # ‚úÖ datetime object
)
```
‚úÖ **Correct:** Align√© avec API Graphiti 0.17.x

**3. Int√©gration Pipeline**
```python
# backend/app/core/processor.py
await ingest_chunks_to_graph(chunks=chunks, metadata=enriched_metadata)
```
‚úÖ **Correct:** Chunking ‚Üí Graphiti ingestion

**4. Cleanup Proper**
```python
# backend/app/main.py
await close_graphiti_client()
```
‚úÖ **Correct:** Fermeture propre dans shutdown event

### ‚ùå Probl√®mes Identifi√©s

#### Probl√®me 1: Version Graphiti Obsol√®te ‚ö†Ô∏è CRITIQUE

**√âtat Actuel:**
```txt
# backend/requirements.txt
graphiti-core==0.3.7
```

**Cons√©quence:**
- API instable (version tr√®s ancienne, 0.17.x est la stable actuelle)
- Fonctionnalit√©s manquantes (SearchConfig, reranking, custom entities)
- Bugs potentiels corrig√©s dans versions r√©centes

**Solution:**
```txt
graphiti-core==0.17.0  # Version stable octobre 2025
```

---

#### Probl√®me 2: LLM Non Configur√© pour Graphiti ‚ö†Ô∏è BLOQUANT

**√âtat Actuel:**
```python
# backend/app/integrations/graphiti.py
_graphiti_client = Graphiti(
    uri=settings.NEO4J_URI,
    user=settings.NEO4J_USER,
    password=settings.NEO4J_PASSWORD
    # ‚ùå Pas de llm_client, embedder, cross_encoder
)
```

**Cons√©quence:**
- Graphiti utilise OpenAI par d√©faut (appels API qui √©chouent si pas de cl√© configur√©e)
- Pas d'extraction d'entit√©s/relations fonctionnelle
- Ingestion √©choue silencieusement ou avec erreurs API

**Solution: Utiliser OpenAI GPT-5-nano pour Graphiti (OPTIMAL)**

‚úÖ **Architecture Choisie:**
- **Graphiti:** OpenAI **GPT-5-nano** (extraction entities/relations) + text-embedding-3-small (embeddings)
- **RAG/User:** Mistral 7b sur Ollama (queries utilisateur)
- **S√©paration claire:** Pas de m√©lange entre les deux!

**Pourquoi GPT-5-nano pour Graphiti?**
- ‚úÖ **Performance maximale:** 2M tokens/min (rate limit le plus √©lev√© disponible)
- ‚úÖ **Co√ªt minimal:** Le moins cher de la gamme OpenAI
- ‚úÖ **Qualit√© extraction:** Sup√©rieure √† Mistral 7b pour entity extraction
- ‚úÖ Embeddings optimis√©s (text-embedding-3-small, 1536 dim)
- ‚úÖ Latence ultra-faible (~0.5-1s par chunk avec 2M TPM)
- ‚úÖ Support natif Graphiti (pas besoin d'adapter)

```python
from graphiti_core import Graphiti

# Configuration simple: Graphiti utilise OpenAI par d√©faut
_graphiti_client = Graphiti(
    uri=settings.NEO4J_URI,
    user=settings.NEO4J_USER,
    password=settings.NEO4J_PASSWORD
    # ‚úÖ Pas besoin de llm_client custom: utilise OpenAI nativement
    # N√©cessite juste OPENAI_API_KEY dans settings
)
```

---

#### Probl√®me 3: Recherche Graphiti Native Non Utilis√©e ‚ö†Ô∏è SOUS-OPTIMAL

**√âtat Actuel:**
```python
# backend/app/integrations/neo4j.py
def query_context_fulltext(question: str):
    # ‚ùå Queries Cypher manuelles
    records = self.driver.execute_query(
        "CALL db.index.fulltext.queryNodes('episode_content', $search_text)..."
    )
```

**Cons√©quence:**
- On n'utilise pas les capacit√©s de recherche hybride de Graphiti (semantic + BM25 + RRF)
- Pas de reranking intelligent
- Queries manuelles fragiles et difficiles √† maintenir

**Solution:**
Utiliser `graphiti.search()` directement:
```python
# backend/app/integrations/graphiti.py
async def search_knowledge_graph(
    query: str,
    num_results: int = 10,
    search_config: Optional[SearchConfig] = None
) -> List[EntityEdge]:
    """
    Search knowledge graph using Graphiti's native hybrid search
    
    Returns:
        List of EntityEdges (facts) with source/target entities
    """
    client = await get_graphiti_client()
    
    results = await client.search(
        query=query,
        num_results=num_results,
        search_config=search_config or EDGE_HYBRID_SEARCH_RRF
    )
    
    return results
```

---

#### Probl√®me 4: `build_communities()` Appel√© Trop Fr√©quemment ‚ö†Ô∏è PERFORMANCE

**√âtat Actuel:**
```python
# backend/app/integrations/graphiti.py (ligne 136-143)
if successful > 0:
    try:
        logger.info("üèòÔ∏è  Building communities...")
        await client.build_communities()  # ‚ùå Apr√®s CHAQUE upload!
        logger.info("‚úÖ Communities built")
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è  Community building failed: {e}")
```

**Cons√©quence:**
- Co√ªt computationnel √©lev√© (Louvain algorithm sur tout le graphe)
- Ralentit chaque upload (436 chunks ‚Üí attente community building)
- Pas n√©cessaire apr√®s chaque document (communaut√©s √©voluent lentement)

**Solution:**
Community building p√©riodique:
```python
# 1. Supprimer de ingest_chunks_to_graph()

# 2. Ajouter endpoint d√©di√©
@router.post("/graph/build-communities")
async def build_communities_endpoint():
    """
    Build communities (call manually or via cron)
    
    Frequency: 
    - Dev: After every 5-10 documents
    - Prod: Daily cron job
    """
    client = await get_graphiti_client()
    await client.build_communities()
    return {"status": "completed"}
```

---

#### Probl√®me 5: Pas de `group_ids` ‚ö†Ô∏è MULTI-TENANT

**√âtat Actuel:**
```python
await client.add_episode(
    name=f"{metadata['filename']} - Chunk {chunk_index}",
    episode_body=chunk_text,
    source=EpisodeType.text,
    source_description=f"...",
    reference_time=reference_time,
    # ‚ùå Pas de group_id
)
```

**Cons√©quence:**
- Tous les documents dans le m√™me namespace
- Phase 1+ (multi-user): Impossible d'isoler donn√©es par utilisateur
- Queries ram√®nent donn√©es de tous les users

**Solution:**
```python
await client.add_episode(
    name=f"{metadata['filename']} - Chunk {chunk_index}",
    episode_body=chunk_text,
    source=EpisodeType.text,
    source_description=f"...",
    reference_time=reference_time,
    group_id=metadata.get("user_id", "default")  # ‚úÖ Isolation multi-tenant
)
```

---

#### Probl√®me 6: Conflit Potentiel Indexes Neo4j ‚ö†Ô∏è STABILIT√â

**√âtat Actuel:**
```python
# backend/app/integrations/neo4j_indexes.py
create_rag_indexes(driver)
# Cr√©e: episode_content (FULLTEXT), entity_name_idx, episode_date_idx

# backend/app/integrations/graphiti.py
await graphiti.build_indices_and_constraints()
# Cr√©e aussi des indices (uuid, embeddings, etc.)
```

**Cons√©quence:**
- Risque de doublons ou conflits
- Ordre de cr√©ation important

**Solution:**
```python
# 1. Graphiti indices FIRST (au startup)
await graphiti.build_indices_and_constraints()

# 2. PUIS custom RAG indices (si n√©cessaire)
# Note: Graphiti cr√©e d√©j√† des indices vectoriels optimaux
# Nos indices custom peuvent √™tre redondants
```

---

## Architecture Cible

### üéØ S√©paration LLMs: OpenAI vs Ollama

**IMPORTANT: Pas de m√©lange entre les deux stacks!**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    GRAPHITI STACK (OpenAI)                   ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                               ‚îÇ
‚îÇ  PDF Upload ‚Üí Docling ‚Üí Chunks                               ‚îÇ
‚îÇ              ‚Üì                                                ‚îÇ
‚îÇ         Graphiti Ingestion                                    ‚îÇ
‚îÇ              ‚Üì                                                ‚îÇ
‚îÇ    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                  ‚îÇ
‚îÇ    ‚îÇ OpenAI GPT-5-nano (2M TPM)         ‚îÇ                  ‚îÇ
‚îÇ    ‚îÇ - Entity extraction                  ‚îÇ                  ‚îÇ
‚îÇ    ‚îÇ - Relation detection                 ‚îÇ                  ‚îÇ
‚îÇ    ‚îÇ - Entity resolution                  ‚îÇ                  ‚îÇ
‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                  ‚îÇ
‚îÇ              +                                                ‚îÇ
‚îÇ    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                  ‚îÇ
‚îÇ    ‚îÇ OpenAI text-embedding-3-small        ‚îÇ                  ‚îÇ
‚îÇ    ‚îÇ - Episode embeddings (1536 dim)     ‚îÇ                  ‚îÇ
‚îÇ    ‚îÇ - Entity name embeddings             ‚îÇ                  ‚îÇ
‚îÇ    ‚îÇ - Fact embeddings                    ‚îÇ                  ‚îÇ
‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                  ‚îÇ
‚îÇ              ‚Üì                                                ‚îÇ
‚îÇ         Neo4j Storage                                         ‚îÇ
‚îÇ    (Episodes, Entities, Relations)                            ‚îÇ
‚îÇ                                                               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     RAG STACK (Ollama)                       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                               ‚îÇ
‚îÇ  User Question                                                ‚îÇ
‚îÇ       ‚Üì                                                       ‚îÇ
‚îÇ  Graphiti Hybrid Search (utilise embeddings OpenAI)          ‚îÇ
‚îÇ       ‚Üì                                                       ‚îÇ
‚îÇ  Context Retrieved                                            ‚îÇ
‚îÇ       ‚Üì                                                       ‚îÇ
‚îÇ  Build RAG Prompt                                             ‚îÇ
‚îÇ       ‚Üì                                                       ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                    ‚îÇ
‚îÇ  ‚îÇ Ollama Mistral 7b                    ‚îÇ                    ‚îÇ
‚îÇ  ‚îÇ - Answer generation                  ‚îÇ                    ‚îÇ
‚îÇ  ‚îÇ - User conversation                  ‚îÇ                    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                    ‚îÇ
‚îÇ       ‚Üì                                                       ‚îÇ
‚îÇ  Grounded Answer                                              ‚îÇ
‚îÇ                                                               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Pourquoi cette s√©paration?**
- ‚úÖ **Graphiti = OpenAI GPT-5-nano:** Qualit√© + Vitesse (2M TPM) + Co√ªt minimal
- ‚úÖ **RAG = Ollama:** Co√ªt 0‚Ç¨ pour queries utilisateur, confidentialit√©, latence acceptable
- ‚úÖ **Aucun m√©lange:** Chaque stack utilise son propre LLM, pas de confusion
- ‚úÖ **Scalabilit√©:** Graphiti ingestion ultra-rapide (2M TPM) vs RAG queries (temps r√©el) s√©par√©s

**Performance:**
- **Graphiti:** 436 chunks ing√©r√©s en ~20-30s (vs 5-10 min avec Ollama)
- **Co√ªt:** ~$0.50-1.00 par document 35 pages (GPT-5-nano = le moins cher)

**Co√ªts Estim√©s:**
- **Graphiti (OpenAI GPT-5-nano):** ~$0.50-1.00 par document 35 pages
- **RAG (Ollama Mistral):** 0‚Ç¨ (local)

### Sch√©ma Neo4j (G√©r√© par Graphiti)

```
Nodes:
  Episode {
    uuid: string (PK)
    name: string                        # "Niveau 4 GP.pdf - Chunk 12"
    content: string (indexed fulltext)  # Texte du chunk
    source: string                      # "text", "json", "message"
    source_description: string          # "Document: Niveau 4 GP.pdf, Chunk 12/436"
    created_at: datetime                # Syst√®me
    valid_at: datetime                  # R√©f√©rence temporelle
    group_id: string (nullable)         # "user_123" pour isolation
  }

  Entity {
    uuid: string (PK)
    name: string (indexed)              # "Niveau 4", "Pr√©requis", "FFESSM"
    summary: string                     # Description g√©n√©r√©e par LLM
    entity_type: string                 # Type d√©tect√© par LLM
    name_embedding: vector              # Pour recherche s√©mantique
    group_id: string (nullable)
  }

  Community {
    uuid: string (PK)
    name: string
    summary: string                     # R√©sum√© de la communaut√©
  }

Relationships:
  (Episode)-[:HAS_ENTITY]->(Entity)
    Lien bidirectionnel: Episode ‚Üí Entities extraites

  (Entity)-[:RELATES_TO {fact: string, valid_at: datetime, invalid_at: datetime}]->(Entity)
    Relations s√©mantiques avec temporal awareness

  (Entity)-[:MEMBER_OF]->(Community)
    Groupement d'entit√©s similaires
```

### Flux de Donn√©es Complet

```
1. UPLOAD PDF
   ‚Üì
2. DOCLING CONVERSION (280s pour 35 pages)
   DoclingDocument {pages, tables, pictures}
   ‚Üì
3. HIERARCHICAL CHUNKING (5s)
   436 chunks s√©mantiques (avg 127 tokens)
   ‚Üì
4. GRAPHITI INGESTION (30-60s pour 436 chunks)
   Pour chaque chunk:
   ‚îú‚îÄ Cr√©er Episode node
   ‚îú‚îÄ LLM: Extraire entities (ex: "Niveau 4", "Pr√©requis", "MFT")
   ‚îú‚îÄ LLM: D√©tecter relations (ex: "Niveau 4" -[REQUIRES]-> "Niveau 3")
   ‚îú‚îÄ R√©solution entities (d√©doublonnage)
   ‚îî‚îÄ D√©tection contradictions temporelles
   ‚Üì
5. NEO4J STORAGE
   Episodes (436) + Entities (~50-100) + Relations (~100-200)
   ‚Üì
6. COMMUNITY BUILDING (p√©riodique, pas √† chaque upload)
   Louvain algorithm ‚Üí groupes d'entit√©s li√©es
   ‚Üì
7. RAG QUERY
   User question
   ‚Üì
   Graphiti Hybrid Search (semantic + BM25 + RRF)
   ‚îú‚îÄ Search Episodes (chunks texte)
   ‚îú‚îÄ Search Entities (concepts extraits)
   ‚îî‚îÄ Graph traversal (relations)
   ‚Üì
   Top 10 results (EntityEdges avec facts)
   ‚Üì
   Build RAG Prompt (context + question)
   ‚Üì
   Ollama LLM Generation
   ‚Üì
   Grounded Answer avec citations
```

---

## Plan d'Impl√©mentation D√©taill√©

### Phase A: Pr√©paration Environnement (15 min)

#### A.1 - V√©rifier Cl√© OpenAI ‚úÖ PR√âREQUIS

**Objectif:** S'assurer que la cl√© OpenAI est configur√©e pour Graphiti

**V√©rification:**
```bash
# V√©rifier que OPENAI_API_KEY est dans .env
grep "OPENAI_API_KEY" .env

# Output attendu: OPENAI_API_KEY=sk-proj-SDuU8A9vNJEf2i...
```

**Si absente, ajouter dans `.env`:**
```bash
# OpenAI API (for Graphiti knowledge graph)
OPENAI_API_KEY=sk-proj-SDuU8A9vNJEf2i....
```

**Test API:**
```bash
curl https://api.openai.com/v1/models \
  -H "Authorization: Bearer sk-proj-SDuU8A9vNJEf2i...." | jq '.data[] | select(.id | contains("gpt-5-nano"))'

# Output attendu: Mod√®le gpt-5-nano disponible
```

**Crit√®res de Succ√®s:**
- ‚úÖ `OPENAI_API_KEY` pr√©sente dans `.env`
- ‚úÖ API OpenAI r√©pond (200 OK)
- ‚úÖ Mod√®le `gpt-5-nano` accessible
- ‚úÖ Rate limit: 2M tokens/min confirm√©

---

#### A.2 - Mettre √† Jour Graphiti Version ‚ö†Ô∏è CRITIQUE

**Fichier:** `backend/requirements.txt`

**Changement:**
```diff
- graphiti-core==0.3.7
+ graphiti-core==0.17.0
```

**Rebuild Docker:**
```bash
cd /Users/nicozefrench/Dropbox/AI/rag-knowledge-graph-starter
docker compose -f docker/docker-compose.dev.yml build backend --no-cache
docker compose -f docker/docker-compose.dev.yml up -d backend
```

**V√©rification:**
```bash
docker exec rag-backend python -c "import graphiti_core; print(graphiti_core.__version__)"
# Output attendu: 0.17.0
```

---

#### A.3 - Configurer Variables Environnement

**Fichier:** `backend/app/core/config.py`

**V√©rifier que `OPENAI_API_KEY` est charg√©e:**
```python
class Settings(BaseSettings):
    # ... existing fields ...
    
    # OpenAI Configuration (pour Graphiti uniquement)
    OPENAI_API_KEY: Optional[str] = None  # ‚úÖ D√©j√† pr√©sent normalement
    OPENAI_MODEL: str = "gpt-4o"          # ‚úÖ D√©j√† pr√©sent
    
    # Note: Pas besoin de variables sp√©cifiques Graphiti
    # Graphiti utilise automatiquement OPENAI_API_KEY si disponible
```

**Pas de modifications n√©cessaires dans `.env`** - Cl√© d√©j√† pr√©sente!

---

### Phase B: Configuration Graphiti Client (1h)

#### B.1 - Refactor `graphiti.py` avec Configuration OpenAI

**Fichier:** `backend/app/integrations/graphiti.py`

**Changements Majeurs:**

```python
"""
Graphiti Integration avec OpenAI GPT-4o-mini

Architecture:
- Graphiti: OpenAI GPT-4o-mini (entity extraction) + text-embedding-3-small (embeddings)
- RAG/User: Mistral 7b sur Ollama (s√©par√©, pas de m√©lange!)
"""
import logging
from datetime import datetime, timezone
from typing import Dict, Any, List, Optional
from graphiti_core import Graphiti
from graphiti_core.nodes import EpisodeType
from graphiti_core.search.search_config_recipes import EDGE_HYBRID_SEARCH_RRF
from graphiti_core.search.search_config import SearchConfig

from app.core.config import settings

logger = logging.getLogger('diveteacher.graphiti')


# Global Graphiti client (singleton pattern)
_graphiti_client: Optional[Graphiti] = None
_indices_built: bool = False


async def get_graphiti_client() -> Graphiti:
    """
    Get or create Graphiti client singleton avec OpenAI
    
    Returns:
        Initialized Graphiti client
        
    Note:
        - Build indices only once on first call
        - Reuse same client for all operations
        - Uses OpenAI by default (GPT-4o-mini + text-embedding-3-small)
        - Requires OPENAI_API_KEY in environment
    """
    global _graphiti_client, _indices_built
    
    if _graphiti_client is None:
        if not settings.GRAPHITI_ENABLED:
            raise RuntimeError("Graphiti is disabled in settings")
        
        if not settings.OPENAI_API_KEY:
            raise RuntimeError("OPENAI_API_KEY required for Graphiti (not found in settings)")
        
        logger.info("üîß Initializing Graphiti client with OpenAI...")
        
        # Graphiti utilise OpenAI par d√©faut
        # Pas besoin de llm_client custom!
        _graphiti_client = Graphiti(
            uri=settings.NEO4J_URI,
            user=settings.NEO4J_USER,
            password=settings.NEO4J_PASSWORD
            # ‚úÖ Graphiti charge automatiquement:
            #    - LLM: gpt-4o-mini (ou OPENAI_MODEL si d√©fini)
            #    - Embedder: text-embedding-3-small
            #    - Via OPENAI_API_KEY dans environment
        )
        
        logger.info("‚úÖ Graphiti client initialized: OpenAI GPT-5-nano (2M TPM) + text-embedding-3-small")
    
    # Build indices and constraints (only once)
    if not _indices_built:
        logger.info("üî® Building Neo4j indices and constraints (Graphiti)...")
        await _graphiti_client.build_indices_and_constraints()
        _indices_built = True
        logger.info("‚úÖ Graphiti indices and constraints built")
    
    return _graphiti_client


async def close_graphiti_client():
    """Close Graphiti client connection"""
    global _graphiti_client, _indices_built
    
    if _graphiti_client is not None:
        logger.info("üîå Closing Graphiti connection...")
        await _graphiti_client.close()
        _graphiti_client = None
        _indices_built = False
        logger.info("‚úÖ Graphiti connection closed")


async def ingest_chunks_to_graph(
    chunks: List[Dict[str, Any]],
    metadata: Dict[str, Any]
) -> None:
    """
    Ingest semantic chunks to Graphiti knowledge graph
    
    Args:
        chunks: List of chunks from HierarchicalChunker
        metadata: Document-level metadata
        
    Raises:
        RuntimeError: If Graphiti is disabled
        
    Note:
        - Each chunk is ingested as an "episode" in Graphiti
        - Graphiti automatically extracts entities and relationships
        - Failed chunks are logged but don't block the pipeline
        - Community building is NOT called here (too expensive, call periodically)
    """
    if not settings.GRAPHITI_ENABLED:
        logger.warning("‚ö†Ô∏è  Graphiti disabled - skipping ingestion")
        return
    
    logger.info(f"üì• Ingesting {len(chunks)} chunks to Graphiti/Neo4j")
    
    client = await get_graphiti_client()
    
    successful = 0
    failed = 0
    
    # Determine group_id for multi-tenant isolation
    group_id = metadata.get("user_id", "default")  # ‚úÖ Phase 1+: real user IDs
    
    # Pour chaque chunk, appeler Graphiti
    for chunk in chunks:
        chunk_text = chunk["text"]
        chunk_index = chunk["index"]
        
        try:
            # IMPORTANT: Utiliser datetime avec timezone UTC
            reference_time = datetime.now(timezone.utc)
            
            # Ingest chunk comme "episode" dans Graphiti
            await client.add_episode(
                name=f"{metadata['filename']} - Chunk {chunk_index}",
                episode_body=chunk_text,
                source=EpisodeType.text,
                source_description=f"Document: {metadata['filename']}, "
                                 f"Chunk {chunk_index}/{chunk['metadata']['total_chunks']}",
                reference_time=reference_time,
                group_id=group_id,  # ‚úÖ Multi-tenant isolation
                # TODO Phase 1+: Ajouter entity_types et edge_types custom
            )
            successful += 1
            
            if (chunk_index + 1) % 10 == 0:
                logger.info(f"   Processed {chunk_index + 1}/{len(chunks)} chunks...")
            
        except Exception as e:
            logger.error(f"Failed to ingest chunk {chunk_index}: {e}", exc_info=True)
            failed += 1
            # Continue avec chunks suivants (ne pas fail tout le pipeline)
    
    # Log r√©sultats
    if failed > 0:
        logger.warning(f"‚ö†Ô∏è  Ingestion partial: {successful} OK, {failed} failed")
    else:
        logger.info(f"‚úÖ Successfully ingested {successful} chunks")
    
    # ‚ùå NE PAS appeler build_communities() ici (trop co√ªteux)
    # √Ä appeler manuellement via endpoint d√©di√© ou cron job


async def search_knowledge_graph(
    query: str,
    num_results: int = 10,
    group_ids: Optional[List[str]] = None,
    search_config: Optional[SearchConfig] = None
) -> List[Dict[str, Any]]:
    """
    Search knowledge graph using Graphiti's native hybrid search
    
    Args:
        query: User's search query
        num_results: Number of results to return
        group_ids: Filter by group_ids (multi-tenant)
        search_config: Custom search configuration (default: EDGE_HYBRID_SEARCH_RRF)
        
    Returns:
        List of dicts with fact, source_entity, target_entity, score, etc.
        
    Note:
        - Uses Graphiti's hybrid search (semantic + BM25 + RRF)
        - Returns EntityEdges (facts/relations) not just Episodes
        - Much more powerful than manual Neo4j queries
    """
    if not settings.GRAPHITI_ENABLED:
        logger.warning("‚ö†Ô∏è  Graphiti disabled - returning empty results")
        return []
    
    logger.info(f"üîç Graphiti search: '{query}' (num_results={num_results})")
    
    client = await get_graphiti_client()
    
    try:
        # Use Graphiti's native search (hybrid: semantic + BM25 + RRF)
        edge_results = await client.search(
            query=query,
            num_results=num_results,
            group_ids=group_ids,
            search_config=search_config or EDGE_HYBRID_SEARCH_RRF
        )
        
        # Format results pour RAG pipeline
        formatted_results = []
        for edge in edge_results:
            formatted_results.append({
                "fact": edge.fact,
                "source_entity": edge.source_node_uuid,
                "target_entity": edge.target_node_uuid,
                "relation_type": edge.name,
                "valid_at": edge.valid_at.isoformat() if edge.valid_at else None,
                "invalid_at": edge.invalid_at.isoformat() if edge.invalid_at else None,
                "episodes": edge.episodes,  # Source episodes UUIDs
                # Note: Pour r√©cup√©rer noms entities, faire requ√™te Neo4j s√©par√©e
            })
        
        logger.info(f"‚úÖ Graphiti search returned {len(formatted_results)} results")
        return formatted_results
        
    except Exception as e:
        logger.error(f"‚ùå Graphiti search failed: {e}", exc_info=True)
        return []


async def build_communities() -> bool:
    """
    Build communities in knowledge graph
    
    Returns:
        True if successful, False otherwise
        
    Note:
        - Expensive operation (Louvain algorithm)
        - Call periodically (not after every upload):
          * Dev: Every 5-10 documents
          * Prod: Daily cron job
    """
    if not settings.GRAPHITI_ENABLED:
        logger.warning("‚ö†Ô∏è  Graphiti disabled - skipping community building")
        return False
    
    logger.info("üèòÔ∏è  Building communities (this may take a while)...")
    
    client = await get_graphiti_client()
    
    try:
        await client.build_communities()
        logger.info("‚úÖ Communities built successfully")
        return True
    except Exception as e:
        logger.error(f"‚ùå Community building failed: {e}", exc_info=True)
        return False
```

**Changements Cl√©s:**
1. ‚úÖ Configuration OpenAI simple (utilise defaults Graphiti, pas de custom client)
2. ‚úÖ V√©rification `OPENAI_API_KEY` pr√©sente
3. ‚úÖ Ajout `group_id` pour multi-tenant
4. ‚úÖ Nouvelle fonction `search_knowledge_graph()` utilisant Graphiti nativement
5. ‚úÖ `build_communities()` s√©par√©e (pas dans ingestion)
6. ‚úÖ **S√©paration claire:** Graphiti = OpenAI, RAG/User = Ollama Mistral

---

#### B.2 - Ajouter Endpoint Community Building

**Fichier:** `backend/app/api/graph.py`

**Ajout:**
```python
from fastapi import APIRouter, HTTPException, BackgroundTasks
from fastapi.responses import JSONResponse
from app.integrations.neo4j import neo4j_client
from app.integrations.graphiti import build_communities as graphiti_build_communities

router = APIRouter()

# ... existing endpoints ...

@router.post("/graph/build-communities")
async def build_communities_endpoint(background_tasks: BackgroundTasks):
    """
    Build communities in knowledge graph
    
    Note:
        - Runs in background task (async)
        - Expensive operation (minutes for large graphs)
        - Recommended frequency:
          * Dev: After every 5-10 documents
          * Prod: Daily cron job
    """
    
    async def run_community_building():
        success = await graphiti_build_communities()
        if success:
            logger.info("‚úÖ Background community building completed")
        else:
            logger.error("‚ùå Background community building failed")
    
    background_tasks.add_task(run_community_building)
    
    return JSONResponse(content={
        "status": "started",
        "message": "Community building started in background"
    })


@router.get("/graph/stats")
async def get_graph_stats():
    """
    Get knowledge graph statistics
    
    Returns:
        Graph statistics (node count, relationship count, etc.)
    """
    
    neo4j_client.connect()
    
    # Query pour stats compl√®tes
    query = """
    MATCH (e:Episode)
    WITH count(e) AS episode_count
    MATCH (n:Entity)
    WITH episode_count, count(n) AS entity_count
    MATCH ()-[r:RELATES_TO]->()
    RETURN 
        episode_count,
        entity_count,
        count(r) AS relationship_count
    """
    
    try:
        records, summary, keys = neo4j_client.driver.execute_query(
            query,
            database_=neo4j_client.database
        )
        
        if records:
            data = dict(records[0])
            return JSONResponse(content={
                "episodes": data.get("episode_count", 0),
                "entities": data.get("entity_count", 0),
                "relationships": data.get("relationship_count", 0)
            })
        else:
            return JSONResponse(content={
                "episodes": 0,
                "entities": 0,
                "relationships": 0
            })
            
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
```

---

### Phase C: Int√©gration RAG avec Graphiti Search (1h)

#### C.1 - Refactor `rag.py` pour Utiliser Graphiti Search

**Fichier:** `backend/app/core/rag.py`

**Changements:**

```python
"""
RAG (Retrieval-Augmented Generation) Chain avec Graphiti
"""

from typing import AsyncGenerator, List, Dict, Any
from app.core.llm import get_llm
from app.integrations.graphiti import search_knowledge_graph
from app.core.config import settings


async def retrieve_context(
    question: str, 
    top_k: int = None,
    group_ids: List[str] = None
) -> Dict[str, Any]:
    """
    Retrieve relevant context using Graphiti's hybrid search
    
    Args:
        question: User's question
        top_k: Number of results (default: from settings)
        group_ids: Filter by group_ids (multi-tenant)
        
    Returns:
        Dictionary with facts (EntityEdges) from Graphiti
        {
            "facts": List[Dict],  # Relations extraites
            "total": int
        }
        
    Note:
        - Uses Graphiti native search (semantic + BM25 + RRF)
        - Returns EntityEdges (relations) not just text chunks
        - More semantic than pure full-text search
    """
    if top_k is None:
        top_k = settings.RAG_TOP_K
    
    # Graphiti hybrid search
    facts = await search_knowledge_graph(
        query=question,
        num_results=top_k,
        group_ids=group_ids
    )
    
    return {
        "facts": facts,
        "total": len(facts)
    }


def build_rag_prompt(question: str, context: Dict[str, Any]) -> tuple[str, str]:
    """
    Build RAG prompt from question and Graphiti facts
    
    Args:
        question: User's question
        context: Dictionary with "facts" key (from Graphiti search)
        
    Returns:
        Tuple of (system_prompt, user_prompt)
    """
    
    facts = context.get("facts", [])
    
    # Build context string from facts
    context_parts = []
    
    if facts:
        context_parts.append("=== KNOWLEDGE FROM DIVING MANUALS ===\n")
        for idx, fact_data in enumerate(facts, 1):
            fact = fact_data.get("fact", "")
            relation_type = fact_data.get("relation_type", "")
            valid_at = fact_data.get("valid_at", "")
            
            context_parts.append(
                f"[Fact {idx} - {relation_type}]\n"
                f"{fact}\n"
                f"Valid: {valid_at if valid_at else 'Current'}"
            )
    
    context_str = "\n\n".join(context_parts)
    
    # System prompt (DiveTeacher-specific)
    system_prompt = """You are DiveTeacher, an AI assistant specialized in scuba diving education.

CRITICAL RULES:
1. Answer ONLY using information from the provided knowledge facts
2. If context is insufficient, say "I don't have enough information in the diving manuals to answer that question accurately"
3. NEVER make up or infer information not present in the context
4. Cite facts: [Fact 1], [Fact 2] when answering
5. Be concise but thorough
6. Use technical diving terms accurately
7. For FFESSM/SSI procedures, cite exact source material

Your goal: Provide accurate, grounded answers that diving students and instructors can trust for their training and safety."""
    
    # User prompt
    if context_parts:
        user_prompt = f"""Knowledge from diving manuals:

{context_str}

---

Question: {question}

Answer based ONLY on the knowledge above. Cite your facts:"""
    else:
        user_prompt = f"""No relevant knowledge found in diving manuals.

Question: {question}

Please explain you don't have enough information to answer this accurately."""
    
    return system_prompt, user_prompt


async def rag_stream_response(
    question: str, 
    temperature: float = 0.7,
    max_tokens: int = 2000,
    group_ids: List[str] = None
) -> AsyncGenerator[str, None]:
    """
    RAG chain: Retrieve (Graphiti) ‚Üí Build prompt ‚Üí Stream LLM response
    
    Args:
        question: User's question
        temperature: LLM sampling temperature
        max_tokens: Maximum tokens to generate
        group_ids: Filter by group_ids (multi-tenant)
        
    Yields:
        Response tokens as they are generated
    """
    
    # Step 1: Retrieve context via Graphiti
    context = await retrieve_context(question, group_ids=group_ids)
    
    # Step 2: Build prompt
    system_prompt, user_prompt = build_rag_prompt(question, context)
    
    # Step 3: Stream LLM response
    llm = get_llm()
    async for token in llm.stream_completion(
        prompt=user_prompt,
        system_prompt=system_prompt,
        temperature=temperature,
        max_tokens=max_tokens
    ):
        yield token


async def rag_query(
    question: str, 
    temperature: float = 0.7,
    max_tokens: int = 2000,
    group_ids: List[str] = None
) -> Dict[str, Any]:
    """
    RAG query with full response (non-streaming)
    
    Args:
        question: User's question
        temperature: LLM sampling temperature
        max_tokens: Maximum tokens to generate
        group_ids: Filter by group_ids (multi-tenant)
        
    Returns:
        Dictionary with answer, context, and metadata
    """
    
    # Retrieve context
    context = await retrieve_context(question, group_ids=group_ids)
    
    # Build prompt
    system_prompt, user_prompt = build_rag_prompt(question, context)
    
    # Get LLM response
    llm = get_llm()
    full_response = ""
    async for token in llm.stream_completion(
        prompt=user_prompt,
        system_prompt=system_prompt,
        temperature=temperature,
        max_tokens=max_tokens
    ):
        full_response += token
    
    return {
        "question": question,
        "answer": full_response,
        "context": context,
        "num_sources": len(context.get("facts", []))
    }
```

**Changements Cl√©s:**
1. ‚úÖ Utilise `search_knowledge_graph()` au lieu de Neo4j direct
2. ‚úÖ Contexte bas√© sur "facts" (EntityEdges) pas juste chunks
3. ‚úÖ Support `group_ids` pour multi-tenant
4. ‚úÖ Prompt adapt√© aux facts Graphiti

---

#### C.2 - D√©pr√©cier (mais garder) Anciennes Queries Neo4j

**Fichier:** `backend/app/integrations/neo4j.py`

**Ajout en haut du fichier:**
```python
"""
Neo4j Database Client

‚ö†Ô∏è  DEPRECATED: Direct Neo4j queries for RAG
    ‚Üí Use Graphiti search instead (backend/app/integrations/graphiti.py)
    
This client is kept for:
- Graph stats endpoints
- Direct Cypher queries (debugging)
- Fallback if Graphiti unavailable

For RAG queries, use: graphiti.search_knowledge_graph()
"""
```

---

### Phase D: Tests de Validation (1.5h)

#### D.1 - Test Unitaire: Graphiti Client Initialization

**Fichier:** `backend/tests/test_graphiti_init.py` (NOUVEAU)

```python
"""
Test Graphiti Client Initialization avec Ollama
"""
import pytest
import asyncio
from app.integrations.graphiti import get_graphiti_client, close_graphiti_client
from app.core.config import settings


@pytest.mark.asyncio
async def test_graphiti_client_singleton():
    """Test singleton pattern"""
    client1 = await get_graphiti_client()
    client2 = await get_graphiti_client()
    
    assert client1 is client2, "Should return same instance"
    
    await close_graphiti_client()


@pytest.mark.asyncio
async def test_graphiti_ollama_config():
    """Test Ollama LLM configuration"""
    client = await get_graphiti_client()
    
    # V√©rifier que le client est bien configur√©
    assert client.llm_client is not None, "LLM client should be configured"
    assert client.embedder is not None, "Embedder should be configured"
    
    await close_graphiti_client()


@pytest.mark.asyncio
async def test_graphiti_indices_built():
    """Test that indices are built only once"""
    from app.integrations.graphiti import _indices_built
    
    # Reset
    global _indices_built
    _indices_built = False
    
    # First call should build indices
    await get_graphiti_client()
    assert _indices_built == True
    
    await close_graphiti_client()
```

---

#### D.2 - Test End-to-End: PDF ‚Üí Graphiti ‚Üí RAG Query

**Fichier:** `backend/tests/test_e2e_graphiti_rag.py` (NOUVEAU)

```python
"""
Test End-to-End: PDF Upload ‚Üí Graphiti Ingestion ‚Üí RAG Query
"""
import pytest
import asyncio
from pathlib import Path
from app.integrations.dockling import convert_document_to_docling, extract_document_metadata
from app.services.document_chunker import get_chunker
from app.integrations.graphiti import ingest_chunks_to_graph, search_knowledge_graph
from app.core.rag import retrieve_context, build_rag_prompt


@pytest.mark.asyncio
async def test_e2e_pdf_to_rag():
    """
    Test complet: PDF ‚Üí Docling ‚Üí Chunking ‚Üí Graphiti ‚Üí RAG Query
    
    Note: Utilise un petit PDF de test (pas Niveau 4 GP 35 pages)
    """
    
    # 1. Conversion Docling
    test_pdf = "TestPDF/small_test.pdf"  # √Ä cr√©er: 2-3 pages seulement
    doc = await convert_document_to_docling(test_pdf, timeout=60)
    metadata = extract_document_metadata(doc)
    
    assert doc.num_pages > 0, "Should have pages"
    
    # 2. Chunking
    chunker = get_chunker()
    chunks = chunker.chunk_document(doc, "small_test.pdf", "test-e2e-001")
    
    assert len(chunks) > 0, "Should have chunks"
    print(f"‚úÖ Created {len(chunks)} chunks")
    
    # 3. Graphiti Ingestion
    enriched_metadata = {
        "filename": "small_test.pdf",
        "upload_id": "test-e2e-001",
        "user_id": "test_user",
        **metadata
    }
    
    await ingest_chunks_to_graph(chunks=chunks, metadata=enriched_metadata)
    print(f"‚úÖ Ingested {len(chunks)} chunks to Graphiti")
    
    # Wait for Neo4j to index
    await asyncio.sleep(5)
    
    # 4. RAG Query via Graphiti
    question = "What is this document about?"  # Adapt√© au test PDF
    context = await retrieve_context(question, top_k=5, group_ids=["test_user"])
    
    assert context["total"] > 0, "Should find relevant facts"
    print(f"‚úÖ Found {context['total']} relevant facts")
    
    # 5. Build Prompt
    system_prompt, user_prompt = build_rag_prompt(question, context)
    
    assert "DiveTeacher" in system_prompt, "Should have custom system prompt"
    assert question in user_prompt, "Should include question"
    print(f"‚úÖ Built RAG prompt ({len(user_prompt)} chars)")
    
    print("\nüéâ E2E Test PASSED!")
```

**Commande:**
```bash
docker exec rag-backend pytest tests/test_e2e_graphiti_rag.py -v -s
```

---

#### D.3 - Test Manuel: Upload PDF R√©el via Curl

**Commande:**
```bash
# 1. Upload PDF (petit de pr√©f√©rence pour test rapide)
curl -X POST http://localhost:8000/api/upload \
  -F "file=@TestPDF/Nitrox.pdf" \
  -F "metadata={\"user_id\":\"manual_test\"}"

# Output: {"upload_id": "abc-123", "status": "processing"}

# 2. Attendre fin processing (2-5 min pour Nitrox 42 pages)
sleep 300

# 3. V√©rifier stats graphe
curl http://localhost:8000/api/graph/stats

# Output attendu:
# {
#   "episodes": 500+,
#   "entities": 50-100,
#   "relationships": 100-200
# }

# 4. Query RAG
curl -X POST http://localhost:8000/api/rag/query \
  -H "Content-Type: application/json" \
  -d '{
    "question": "Quels sont les risques du Nitrox?",
    "group_ids": ["manual_test"]
  }'

# Output: {"answer": "...", "context": {...}, "num_sources": 5}
```

---

#### D.4 - Inspection Neo4j Browser

**URL:** http://localhost:7475 (Browser Neo4j)

**Queries de V√©rification:**

```cypher
// 1. Compter Episodes
MATCH (e:Episode)
RETURN count(e) AS episode_count

// Attendu: 436+ pour Niveau 4 GP, 500+ pour Nitrox

// 2. Compter Entities
MATCH (n:Entity)
RETURN count(n) AS entity_count

// Attendu: 50-100

// 3. Voir sample Entities
MATCH (n:Entity)
RETURN n.name, n.summary, n.entity_type
LIMIT 10

// Attendu: "Niveau 4", "Pr√©requis", "FFESSM", "MFT", etc.

// 4. Voir Relations
MATCH (source:Entity)-[r:RELATES_TO]->(target:Entity)
RETURN 
  source.name AS from,
  r.fact AS relationship,
  target.name AS to,
  r.valid_at AS valid_at
LIMIT 20

// Attendu: Relations s√©mantiques extraites par LLM

// 5. V√©rifier group_ids (multi-tenant)
MATCH (e:Episode)
WHERE e.group_id IS NOT NULL
RETURN DISTINCT e.group_id AS group_id, count(e) AS count

// Attendu: "default", "manual_test", etc.
```

---

### Phase E: Documentation et Cleanup (30 min)

#### E.1 - Mettre √† Jour Documentation

**Fichiers √† Updater:**

1. **`docs/NEO4J.md`** (√Ä cr√©er si pas fait en Phase 0.8)
   - Section Graphiti configuration
   - Sch√©ma Neo4j (Episodes, Entities, Relations)
   - SearchConfig examples

2. **`docs/TROUBLESHOOTING.md`** (√Ä cr√©er)
   - Section "Graphiti Errors"
   - Ollama embeddings issues
   - Community building timeouts

3. **`CURRENT-CONTEXT.md`**
   - Update "Phase 0.9 Complete" status
   - Add Graphiti metrics (entities, relations)

4. **`Devplan/PHASE-0.9-GRAPHITI-IMPLEMENTATION.md`** (ce fichier)
   - Section "R√âSULTATS D'IMPL√âMENTATION"
   - Tests valid√©s
   - Metrics observ√©es

---

#### E.2 - Cleanup Code

**Actions:**

1. **Supprimer code mort:**
   - Anciennes queries Neo4j RAG si enti√®rement remplac√©es par Graphiti
   - Fichiers de test obsol√®tes

2. **Ajouter Type Hints:**
   - V√©rifier toutes fonctions Graphiti ont annotations compl√®tes

3. **Logging Consistency:**
   - Uniformiser emojis/format logs Graphiti

4. **Docstrings:**
   - S'assurer toutes fonctions publiques ont docstrings Google-style

---

## Tests de Validation

### Checklist Tests Manuels

- [ ] **A.1** - Mod√®les Ollama install√©s (`mistral`, `nomic-embed-text`)
- [ ] **A.2** - Graphiti 0.17.0 install√© (v√©rifier version)
- [ ] **A.3** - Variables env Ollama configur√©es
- [ ] **B.1** - Graphiti client initialize sans erreur
- [ ] **B.1** - Logs montrent "‚úÖ Graphiti client initialized: LLM=mistral..."
- [ ] **B.2** - Endpoint `/graph/build-communities` existe
- [ ] **B.2** - Endpoint `/graph/stats` retourne data valide
- [ ] **C.1** - `rag.py` utilise `search_knowledge_graph()`
- [ ] **C.1** - Queries RAG retournent facts Graphiti (pas juste chunks)
- [ ] **D.2** - E2E test passe (PDF ‚Üí Graphiti ‚Üí RAG)
- [ ] **D.3** - Upload manuel Nitrox r√©ussit
- [ ] **D.3** - `/graph/stats` montre entities + relations
- [ ] **D.3** - Query RAG retourne answer with facts
- [ ] **D.4** - Neo4j Browser montre Episodes + Entities + RELATES_TO
- [ ] **D.4** - group_ids pr√©sents dans Episodes

### Checklist Tests Automatis√©s

- [ ] **D.1** - `test_graphiti_client_singleton()` ‚úÖ
- [ ] **D.1** - `test_graphiti_ollama_config()` ‚úÖ
- [ ] **D.1** - `test_graphiti_indices_built()` ‚úÖ
- [ ] **D.2** - `test_e2e_pdf_to_rag()` ‚úÖ

**Commande:**
```bash
docker exec rag-backend pytest tests/ -v --cov=app/integrations/graphiti
```

---

## M√©triques de Succ√®s

### M√©triques Techniques

| M√©trique | Cible | Mesure |
|----------|-------|--------|
| **Graphiti Version** | 0.17.0 | `import graphiti_core; print(graphiti_core.__version__)` |
| **OpenAI LLM Config** | GPT-5-nano (2M TPM) | Logs startup backend: "‚úÖ Graphiti client initialized: OpenAI GPT-5-nano" |
| **Embeddings Config** | text-embedding-3-small (1536 dim) | Logs startup backend |
| **Entities Extracted** | 50-100 pour 35 pages PDF | Query Neo4j: `MATCH (n:Entity) RETURN count(n)` |
| **Relations Extracted** | 100-200 | Query Neo4j: `MATCH ()-[r:RELATES_TO]->() RETURN count(r)` |
| **Ingestion Time** | 20-30s pour 436 chunks (2M TPM!) | Logs processor: "‚úÖ Ingestion: Xs" |
| **RAG Query Time** | <2s (sans LLM generation) | Logs rag.py: "‚úÖ Graphiti search returned X results" |
| **Community Building** | <5 min pour 500 episodes | Logs endpoint `/graph/build-communities` |

### M√©triques Qualit√©

| M√©trique | Cible | Validation |
|----------|-------|------------|
| **Entity Quality** | Concepts pertinents extraits | Inspect Neo4j: names comme "Niveau 4", "Pr√©requis", "FFESSM" |
| **Relation Quality** | Relations s√©mantiques correctes | Inspect Neo4j: facts comme "Niveau 4 requires Niveau 3" |
| **RAG Answer Quality** | R√©ponses grounded avec citations | Test manuel: question ‚Üí answer doit citer [Fact 1], [Fact 2] |
| **Multi-tenant Isolation** | group_ids distinct par user | Query Neo4j: `MATCH (e:Episode) RETURN DISTINCT e.group_id` |

### M√©triques Observabilit√©

| M√©trique | Outil | Action |
|----------|-------|--------|
| **Logs Structured** | Docker logs backend | `docker logs rag-backend | grep "üîß Initializing Graphiti"` |
| **Neo4j Health** | Neo4j Browser | http://localhost:7475 ‚Üí MATCH (n) RETURN count(n) |
| **Sentry Errors** | Sentry dashboard | V√©rifier pas d'erreurs Graphiti |
| **API Response Times** | `/api/rag/query` | curl + `time` command |

---

## Probl√®mes Potentiels et Solutions

### Probl√®me: OpenAI API Rate Limiting

**Sympt√¥me:**
```
ERROR: Graphiti ingestion failed: Rate limit exceeded (429)
```

**Causes:**
- Tier OpenAI pas configur√© avec 2M TPM
- (Tr√®s peu probable avec GPT-5-nano qui a rate limit maximal)

**Solutions:**
1. **V√©rifier tier OpenAI:**
   ```bash
   # Checker limits via API
   curl https://api.openai.com/v1/usage \
     -H "Authorization: Bearer $OPENAI_API_KEY"
   
   # GPT-5-nano devrait avoir 2M TPM par d√©faut
   ```

2. **Si vraiment rate limited (rare), batch processing:**
   ```python
   # Ingest par batches de 50 chunks avec pause
   for i in range(0, len(chunks), 50):
       batch = chunks[i:i+50]
       await ingest_chunks_to_graph(batch, metadata)
       await asyncio.sleep(1)  # Pause 1s entre batches
   ```

3. **Augmenter SEMAPHORE_LIMIT dans Graphiti (optionnel):**
   ```bash
   # .env
   SEMAPHORE_LIMIT=20  # Default: 10, augmenter avec 2M TPM!
   ```

---

### Probl√®me: Entities Extraction Rate Faible

**Sympt√¥me:**
```
Neo4j shows:
- 436 Episodes ‚úÖ
- 5 Entities ‚ùå (attendu: 50-100)
- 2 Relations ‚ùå (attendu: 100-200)
```

**Causes:**
- Prompts Graphiti pas optimis√©s pour domaine plong√©e
- Chunks trop longs/courts
- (Tr√®s peu probable avec GPT-5-nano, qualit√© excellente)

**Solutions:**
1. **V√©rifier qualit√© extraction:**
   ```bash
   # Check logs ingestion
   docker logs rag-backend | grep "Graphiti"
   
   # V√©rifier entities dans Neo4j
   # http://localhost:7475
   MATCH (n:Entity) RETURN n.name, n.summary LIMIT 20
   ```

2. **Custom Entity Types (Phase 1+):**
   ```python
   # Dans graphiti.py
   from pydantic import BaseModel, Field
   
   class DivingCertification(BaseModel):
       """Certification de plong√©e"""
       name: str = Field(description="Nom certification (ex: Niveau 4)")
       level: int = Field(description="Niveau (1-4)")
       organization: str = Field(description="FFESSM, SSI, PADI, etc.")
   
   class DivingProcedure(BaseModel):
       """Proc√©dure de plong√©e"""
       name: str = Field(description="Nom proc√©dure")
       description: str = Field(description="Description")
   
   # Utiliser dans add_episode
   await client.add_episode(
       ...,
       entity_types=[DivingCertification, DivingProcedure]
   )
   ```

---

### Probl√®me: Community Building Tr√®s Lent

**Sympt√¥me:**
```
üèòÔ∏è  Building communities...
[5 minutes later...]
Still running...
```

**Causes:**
- Graphe trop grand (1000+ episodes, 500+ entities)
- Algorithme Louvain computationnellement cher

**Solutions:**
1. **Skip community building en dev:**
   ```python
   # Phase 0: Skip, focus sur ingestion + search
   # Phase 1: Activer pour prod seulement
   ```

2. **Background task avec timeout:**
   ```python
   async def run_community_building():
       try:
           await asyncio.wait_for(
               graphiti_build_communities(),
               timeout=300  # 5 min max
           )
       except asyncio.TimeoutError:
           logger.warning("‚ö†Ô∏è  Community building timeout (5 min)")
   ```

3. **Cron job nocturne:**
   ```bash
   # Production: cron daily 3am
   0 3 * * * curl -X POST http://backend:8000/api/graph/build-communities
   ```

---

## Notes Finales

### D√©cisions Architecturales

1. **Graphiti Native Search > Neo4j Manual Queries**
   - Graphiti search() utilise hybrid (semantic + BM25 + RRF) optimis√©
   - Nos queries manuelles Neo4j sont sous-optimales
   - Migration vers Graphiti = meilleure qualit√© RAG

2. **Community Building P√©riodique**
   - Trop co√ªteux pour appeler apr√®s chaque upload
   - B√©n√©fice marginal (communaut√©s √©voluent lentement)
   - Strat√©gie: Cron daily en prod, manuel en dev

3. **Multi-tenant via group_ids**
   - Pr√™t pour Phase 1 (Supabase Auth)
   - Isolation data par user d√®s maintenant
   - Pas de refactor n√©cessaire plus tard

4. **OpenAI GPT-5-nano (Graphiti) + Ollama (RAG) = Hybrid Optimal**
   - Co√ªt OpenAI ~$0.50-1.00/document (extraction one-time, le moins cher!)
   - Co√ªt Ollama 0‚Ç¨ (queries utilisateur ongoing)
   - Performance maximale: 2M tokens/min (ingestion ultra-rapide)
   - Meilleure qualit√© extraction (GPT-5-nano > Mistral 7b)
   - Trade-off optimal: Qualit√© + Vitesse + Co√ªt minimal

### Roadmap Post-Phase 0.9

**Phase 1 - Multi-user:**
- ‚úÖ group_ids d√©j√† impl√©ment√©
- TODO: Lier user_id Supabase ‚Üí group_id Graphiti

**Phase 2 - Custom Entities:**
- TODO: D√©finir types `DivingCertification`, `DivingProcedure`, `SafetyRule`, etc.
- TODO: Edges custom: `REQUIRES`, `RECOMMENDS`, `CONTRADICTS`, `PART_OF`

**Phase 3 - Advanced RAG:**
- TODO: Temporal queries (point-in-time: "What was valid in 2020?")
- TODO: MMR reranking (diversity)
- TODO: Cross-encoder reranking (meilleure qualit√©, plus lent)

---

## üéâ R√©sultats d'Impl√©mentation

> **Section √† compl√©ter apr√®s ex√©cution du plan**

### Status par Phase

| Phase | Status | Dur√©e | Notes |
|-------|--------|-------|-------|
| **A - Pr√©paration** | ‚è≥ | - | - |
| **B - Config Graphiti** | ‚è≥ | - | - |
| **C - RAG Integration** | ‚è≥ | - | - |
| **D - Tests** | ‚è≥ | - | - |
| **E - Documentation** | ‚è≥ | - | - |

### Probl√®mes Rencontr√©s

> √Ä remplir au fur et √† mesure

### Solutions Appliqu√©es

> √Ä remplir au fur et √† mesure

### M√©triques Finales

> √Ä remplir apr√®s tests E2E

---

**üöÄ Pr√™t pour Phase 0.9!**

**Prochaine √âtape:** Ex√©cuter le plan step-by-step avec validation √† chaque phase.

