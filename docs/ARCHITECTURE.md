# üèóÔ∏è DiveTeacher - Technical Architecture

> **Project:** RAG Knowledge Graph for Scuba Diving Education  
> **Version:** Phase 1.2 COMPLETE (RAG + Cross-Encoder Reranking)  
> **Last Updated:** November 5, 2025, 11:15 CET  
> **Status:** üü¢ Phase 1.2 COMPLETE - RAG pipeline with reranking (+16.67% precision validated)

---

## üìã Table of Contents

- [Overview](#overview)
- [Tech Stack](#tech-stack)
- [System Architecture](#system-architecture)
- [Data Flow](#data-flow)
- [Document Processing Pipeline](#document-processing-pipeline)
- [Knowledge Graph Architecture](#knowledge-graph-architecture)
- [RAG Architecture](#rag-architecture)
- [Database Schemas](#database-schemas)
- [Security Architecture](#security-architecture)
- [Deployment Architecture](#deployment-architecture)

---

## Overview

**DiveTeacher** est une application RAG (Retrieval-Augmented Generation) sp√©cialis√©e dans l'√©ducation √† la plong√©e sous-marine. Elle transforme des documents FFESSM/SSI (PDFs, PPTs) en un knowledge graph interrogeable via LLM.

### Key Features

- ‚úÖ **Document Processing:** PDF ‚Üí Markdown avec OCR + TableFormer (Docling)
- ‚úÖ **Semantic Chunking:** RecursiveCharacterTextSplitter (ARIA pattern, 3000 tokens)
- ‚úÖ **Knowledge Graph:** Neo4j + Graphiti pour extraction d'entit√©s/relations
- ‚úÖ **RAG Hybride:** Graphiti search (semantic + BM25 + RRF)
- ‚úÖ **Cross-Encoder Reranking:** ms-marco-MiniLM-L-6-v2 (+16.67% precision, FREE)
- ‚úÖ **LLM Agnostic:** Ollama (local), Gemini (Graphiti), OpenAI (embeddings)
- üîú **Multi-user:** Supabase Auth + PostgreSQL (Phase 1+)

### Value Proposition

**Pour les plongeurs:**
- Q&A pr√©cises sur proc√©dures FFESSM/SSI
- Recherche contextuelle dans manuels MFT
- Recommandations personnalis√©es selon niveau

**Pour les moniteurs:**
- Assistant p√©dagogique intelligent
- G√©n√©ration de supports de cours
- V√©rification des connaissances

---

## Tech Stack

### Backend
| Component | Technology | Version | Purpose |
|-----------|-----------|---------|---------|
| **API Framework** | FastAPI | 0.115.0 | REST API + async support |
| **Document Processor** | Docling | 2.5.1 | PDF/PPT ‚Üí Markdown + OCR |
| **Chunker** | RecursiveCharacterTextSplitter | langchain-text-splitters 0.3.2 | ARIA pattern (3000 tokens, 200 overlap) |
| **Knowledge Graph** | Neo4j | 5.26.0 | Graph database |
| **Graph Library** | Graphiti | graphiti-core[google-genai] 0.17.0 | Entity/relation extraction ‚úÖ |
| **LLM Cloud (Graphiti)** | Google Gemini | 2.5 Flash-Lite | Entity extraction (ARIA-validated, 99.7% savings) ‚úÖ |
| **LLM Local (RAG)** | Ollama | Latest | Qwen 2.5 7B Q8_0 for RAG queries ‚úÖ |
| **LLM Model (RAG)** | Qwen 2.5 7B | Q8_0 (8-bit) | Optimal RAG quality (98/100) ‚úÖ |
| **Embeddings (Graphiti)** | OpenAI | text-embedding-3-small | 1536 dims (DB compatible, for Graphiti) ‚úÖ |
| **Cross-Encoder (Graphiti)** | OpenAI | gpt-4o-mini | Reranking (for Graphiti search) ‚úÖ |
| **Cross-Encoder (RAG)** | sentence-transformers | ms-marco-MiniLM-L-6-v2 | Reranking (+27.3% precision, FREE, local CPU) ‚úÖ |
| **Embeddings Local** | sentence-transformers | 3.3.1 | Semantic similarity |
| **Validation** | Pydantic | 2.11.5 | Data validation |
| **Monitoring** | Sentry | Latest | Error tracking |

### Frontend
| Component | Technology | Version | Purpose |
|-----------|-----------|---------|---------|
| **Framework** | React | 18.3.1 | UI library |
| **Build Tool** | Vite | 5.4.2 | Fast dev server |
| **Styling** | TailwindCSS | 3.4.1 | Utility-first CSS |
| **Components** | shadcn/ui | Latest | Modern UI components |
| **State** | TanStack Query | 5.59.16 | Server state management |
| **Routing** | React Router | 6.26.2 | Client-side routing |

### Infrastructure (Local Dev)
| Component | Technology | Version | Purpose |
|-----------|-----------|---------|---------|
| **Containerization** | Docker | Latest | Service isolation |
| **Orchestration** | Docker Compose | Latest | Multi-container management |
| **OS** | macOS (M1 Max) | 14.6 | Development environment |
| **Memory** | 32GB Unified | - | Handles ML models |

### Infrastructure (Production - Phase 9)
| Component | Technology | Purpose |
|-----------|-----------|---------|
| **Backend Host** | DigitalOcean GPU Droplet | LLM inference |
| **Frontend Host** | Vercel | Static hosting + CDN |
| **Auth + DB** | Supabase Cloud | PostgreSQL + Auth |
| **Domain** | diveteacher.io | Primary domain |
| **Monitoring** | Sentry | Error tracking |

---

## System Architecture

### High-Level Diagram

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    DiveTeacher System                        ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                               ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                  ‚îÇ
‚îÇ  ‚îÇ   Frontend   ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§   Backend    ‚îÇ                  ‚îÇ
‚îÇ  ‚îÇ  React+Vite  ‚îÇ  REST   ‚îÇ   FastAPI    ‚îÇ                  ‚îÇ
‚îÇ  ‚îÇ  Port: 5173  ‚îÇ  API    ‚îÇ  Port: 8000  ‚îÇ                  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                  ‚îÇ
‚îÇ                                    ‚îÇ                          ‚îÇ
‚îÇ                  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îÇ
‚îÇ                  ‚îÇ                 ‚îÇ                 ‚îÇ       ‚îÇ
‚îÇ            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ            ‚îÇ  Neo4j   ‚îÇ     ‚îÇ  Gemini   ‚îÇ    ‚îÇ Docling  ‚îÇ  ‚îÇ
‚îÇ            ‚îÇ  :7475   ‚îÇ     ‚îÇ  2.5 Flash‚îÇ    ‚îÇ Library  ‚îÇ  ‚îÇ
‚îÇ            ‚îÇ  Graph   ‚îÇ     ‚îÇ   Lite    ‚îÇ    ‚îÇ   PDF    ‚îÇ  ‚îÇ
‚îÇ            ‚îÇ  + RAG   ‚îÇ     ‚îÇ (Google)  ‚îÇ    ‚îÇProcessor ‚îÇ  ‚îÇ
‚îÇ            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                  ‚îÇ                                           ‚îÇ
‚îÇ            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                     ‚îÇ
‚îÇ            ‚îÇ  Ollama  ‚îÇ                                     ‚îÇ
‚îÇ            ‚îÇ  :11434  ‚îÇ                                     ‚îÇ
‚îÇ            ‚îÇ  Qwen    ‚îÇ                                     ‚îÇ
‚îÇ            ‚îÇ 2.5 7B   ‚îÇ                                     ‚îÇ
‚îÇ            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                     ‚îÇ
‚îÇ                                                               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Service Communication

```
User Upload (PDF)
      ‚îÇ
      ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  React Frontend ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ POST /api/upload ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                            ‚îÇ
                                               ‚ñº
                                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                    ‚îÇ FastAPI Backend  ‚îÇ
                                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                             ‚îÇ
                        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                        ‚îÇ                    ‚îÇ                    ‚îÇ
                        ‚ñº                    ‚ñº                    ‚ñº
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚îÇ Docling Service ‚îÇ  ‚îÇ Graphiti Client ‚îÇ  ‚îÇ Neo4j Client ‚îÇ
              ‚îÇ - Convert PDF   ‚îÇ  ‚îÇ - Extract Graph ‚îÇ  ‚îÇ - RAG Queries‚îÇ
              ‚îÇ - Chunk Text    ‚îÇ  ‚îÇ - Build Entities‚îÇ  ‚îÇ - Store Data ‚îÇ
              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚îÇ                    ‚îÇ                    ‚îÇ
                        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                             ‚îÇ
                                             ‚ñº
                                      Neo4j Database
                                      (Episodes, Entities,
                                       Relations, Communities)
```

### RAG Query Flow

```
User Question ("Quels sont les pr√©requis Niveau 4?")
      ‚îÇ
      ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  React Frontend ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ POST /api/rag/query ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                               ‚îÇ
                                                  ‚ñº
                                       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                       ‚îÇ FastAPI Backend  ‚îÇ
                                       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                ‚îÇ
                        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                        ‚îÇ                       ‚îÇ                       ‚îÇ
                        ‚ñº                       ‚ñº                       ‚ñº
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚îÇ Neo4j RAG Query ‚îÇ    ‚îÇ Graphiti Search ‚îÇ    ‚îÇ Ollama LLM   ‚îÇ
              ‚îÇ - Fulltext      ‚îÇ    ‚îÇ - Entities      ‚îÇ    ‚îÇ - Generate   ‚îÇ
              ‚îÇ - Hybrid Search ‚îÇ    ‚îÇ - Relations     ‚îÇ    ‚îÇ - Answer     ‚îÇ
              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚îÇ                       ‚îÇ                       ‚îÇ
                        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                ‚îÇ
                                                ‚ñº
                                         Grounded Answer
                                    (Citations + Context)
```

---

## Data Flow

### Document Ingestion Pipeline

```
1. Upload
   ‚îî‚îÄ User uploads PDF via frontend
   ‚îî‚îÄ File saved to /uploads (backend)

2. Validation
   ‚îî‚îÄ File type check (.pdf, .docx, .pptx)
   ‚îî‚îÄ Size check (max 50MB)
   ‚îî‚îÄ Corruption check (read first KB)

3. Docling Conversion
   ‚îî‚îÄ PDF ‚Üí DoclingDocument object
   ‚îî‚îÄ OCR enabled (for scanned docs)
   ‚îî‚îÄ TableFormer ACCURATE mode (for tables)
   ‚îî‚îÄ Timeout: 300s (5 min)
   ‚îî‚îÄ Output: DoclingDocument (35 pages example)

4. Metadata Extraction
   ‚îî‚îÄ Extract: name, origin, num_pages, num_tables, num_pictures
   ‚îî‚îÄ Store for tracking

5. Semantic Chunking
   ‚îî‚îÄ HierarchicalChunker with sentence-transformers
   ‚îî‚îÄ Tokenizer: BAAI/bge-small-en-v1.5
   ‚îî‚îÄ Max tokens: 512, Min tokens: 64
   ‚îî‚îÄ Output: 436 chunks (example)
   ‚îî‚îÄ Each chunk: {index, text, metadata}

6. Graphiti Ingestion
   ‚îî‚îÄ For each chunk:
      ‚îî‚îÄ Create Episode node in Neo4j
      ‚îî‚îÄ Extract entities (automatic)
      ‚îî‚îÄ Extract relations (automatic)
      ‚îî‚îÄ Build knowledge graph

7. Community Building
   ‚îî‚îÄ Group related entities
   ‚îî‚îÄ Optimize for search

8. Status Update
   ‚îî‚îÄ Mark upload as "completed"
   ‚îî‚îÄ Return upload_id to frontend
```

### RAG Query Pipeline

```
1. User Question
   ‚îî‚îÄ "Quels sont les pr√©requis pour le Niveau 4 GP?"

2. Hybrid Search (Neo4j)
   A. Full-text Search (Episodes)
      ‚îî‚îÄ FULLTEXT index on Episode.content
      ‚îî‚îÄ Returns: Top 5 relevant chunks + scores
   
   B. Entity Search
      ‚îî‚îÄ Extract keywords from question
      ‚îî‚îÄ Search Entity.name (RANGE index)
      ‚îî‚îÄ Traverse RELATES_TO relationships (depth=2)
      ‚îî‚îÄ Returns: Related entities + context

3. Context Assembly
   ‚îî‚îÄ Combine Episodes + Entities
   ‚îî‚îÄ Format for LLM prompt:
      ‚îú‚îÄ DOCUMENT EXCERPTS (from chunks)
      ‚îî‚îÄ RELATED CONCEPTS & ENTITIES (from graph)

4. LLM Generation (Ollama Mistral)
   ‚îî‚îÄ System Prompt: "You are DiveTeacher..."
   ‚îî‚îÄ User Prompt: Context + Question
   ‚îî‚îÄ Generation: Grounded answer with citations

5. Response
   ‚îî‚îÄ Return to frontend
   ‚îî‚îÄ Display answer + sources
```

---

## Document Processing Pipeline

### Docling Integration

**Purpose:** Convert PDF/PPT to structured markdown with OCR and table recognition.

**Key Components:**

1. **DocumentConverter Singleton**
   ```python
   # backend/app/integrations/dockling.py
   class DoclingSingleton:
       _instance: Optional[DocumentConverter] = None
       
       @classmethod
       def get_converter(cls) -> DocumentConverter:
           if cls._instance is None:
               pipeline_options = PdfPipelineOptions(
                   do_ocr=True,                    # For scanned docs
                   do_table_structure=True,        # For tables
                   artifacts_path=None,            # Auto-download models
               )
               pipeline_options.table_structure_options.mode = TableFormerMode.ACCURATE
               
               cls._instance = DocumentConverter(
                   format_options={
                       InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)
                   }
               )
           return cls._instance
   ```

2. **Conversion Function**
   ```python
   async def convert_document_to_docling(
       file_path: str,
       timeout: int = 300  # 5 minutes
   ) -> DoclingDocument:
       # Validates file
       # Converts with timeout
       # Returns DoclingDocument object (NOT markdown)
   ```

3. **Metadata Extraction**
   ```python
   def extract_document_metadata(doc: DoclingDocument) -> Dict:
       return {
           "name": doc.name,
           "origin": str(doc.origin),
           "num_pages": doc.num_pages,
           "num_tables": len(doc.tables),
           "num_pictures": len(doc.pictures),
       }
   ```

**Why Singleton?**
- Docling models loaded once (HuggingFace download)
- Reused across requests
- Saves memory + startup time

### Semantic Chunking

**Purpose:** Split document into semantically coherent chunks for RAG.

**Key Components:**

1. **HierarchicalChunker Configuration**
   ```python
   # backend/app/services/document_chunker.py
   self.chunker = HierarchicalChunker(
       tokenizer="BAAI/bge-small-en-v1.5",  # Embedding model
       max_tokens=512,                       # Embedding limit
       min_tokens=64,                        # Avoid tiny chunks
       merge_peers=True                      # Merge small adjacent chunks
   )
   ```

2. **Chunking Process**
   ```python
   def chunk_document(
       self,
       docling_doc: DoclingDocument,
       filename: str,
       upload_id: str
   ) -> List[Dict[str, Any]]:
       # HierarchicalChunker returns DocChunk objects
       chunk_iterator = self.chunker.chunk(docling_doc)
       chunks = list(chunk_iterator)
       
       # Format for RAG pipeline
       formatted_chunks = []
       for i, chunk in enumerate(chunks):
           chunk_meta = chunk.meta  # DocChunk.meta attribute
           formatted_chunk = {
               "index": i,
               "text": chunk.text,  # DocChunk.text attribute
               "metadata": {
                   "filename": filename,
                   "upload_id": upload_id,
                   "chunk_index": i,
                   "total_chunks": len(chunks),
                   "headings": getattr(chunk_meta, "headings", []),
                   "doc_items": [str(item) for item in getattr(chunk_meta, "doc_items", [])],
                   "origin": str(getattr(chunk_meta, "origin", "")),
               }
           }
           formatted_chunks.append(formatted_chunk)
       
       return formatted_chunks
   ```

**Why HierarchicalChunker?**
- Respects document structure (headings, sections)
- Semantic boundaries (not fixed character count)
- Better RAG retrieval quality

**Example Output:**
```json
{
  "index": 0,
  "text": "F√âD√âRATION FRAN√áAISE D'√âTUDES ET DE SPORTS SOUS-MARINS...",
  "metadata": {
    "filename": "Niveau 4 GP.pdf",
    "upload_id": "abc123",
    "chunk_index": 0,
    "total_chunks": 436,
    "headings": [],
    "doc_items": ["TextItem(...)"],
    "origin": "DocumentOrigin(mimetype='application/pdf', ...)"
  }
}
```

---

## Knowledge Graph Architecture

### Neo4j + Graphiti

**Purpose:** Extract entities, relations, and communities from document chunks for graph-based retrieval.

**Node Types (Graphiti):**

1. **Episode**
   - Represents a document chunk
   - Properties: `content`, `source_description`, `name`, `created_at`
   - Created by: `Graphiti.add_episode()`

2. **Entity**
   - Extracted from Episode content
   - Properties: `name`, `summary`, `entity_type`
   - Created by: Graphiti (automatic extraction)

3. **Community** (future)
   - Groups of related entities
   - Created by: `Graphiti.build_communities()`

**Relationship Types:**

1. **RELATES_TO**
   - Connects entities
   - Properties: `fact` (description of relationship)
   - Created by: Graphiti (automatic extraction)

### Graph Ingestion Flow

```python
# backend/app/integrations/graphiti.py
async def ingest_chunks_to_graph(
    chunks: List[Dict[str, Any]],
    metadata: Dict[str, Any]
) -> None:
    """
    Ingest chunks into Neo4j knowledge graph via Graphiti.
    
    Uses:
    - LLM: Anthropic Claude Haiku 4.5 (claude-haiku-4-5-20251001)
    - Embedder: OpenAI text-embedding-3-small (1536 dimensions)
    
    Architecture:
    - Native AnthropicClient (ARIA-validated, 5 days production)
    - Single event loop (FastAPI main)
    - Zero threading conflicts
    - Dedicated ThreadPoolExecutor for Docling only
    """
    graphiti = await get_graphiti_client()
    
    for i, chunk in enumerate(chunks, 1):
        episode_data = build_episode_data(chunk, metadata)
        await graphiti.add_episode(**episode_data)
```

---

## Background Processing Architecture (Phase 0.9)

### AsyncIO + FastAPI Integration ‚úÖ

**Problem Solved:** Thread event loop deadlock causing 100% ingestion failure.

**Root Cause:**
- `ThreadPoolExecutor` created new event loop in worker thread
- `process_document()` attempted to use default executor (FastAPI main loop)
- Deadlock: thread loop waiting for main loop, main loop blocked by thread

**Solution Implemented (ARIA Pattern):**

```python
# backend/app/api/upload.py

@router.post("/upload")
async def upload_document(file: UploadFile = File(...)):
    """
    Upload endpoint - Returns immediately (< 100ms)
    Background processing via asyncio.create_task()
    """
    # 1. Save file
    upload_id = str(uuid.uuid4())
    file_path = save_file(file, upload_id)
    
    # 2. ‚úÖ ARIA Pattern: asyncio.create_task() (NOT ThreadPoolExecutor)
    asyncio.create_task(
        process_document_wrapper(
            file_path=file_path,
            upload_id=upload_id,
            metadata=metadata
        )
    )
    
    # 3. Return immediately
    return {"upload_id": upload_id, "status": "processing"}


async def process_document_wrapper(file_path: str, upload_id: str, metadata: dict):
    """
    Wrapper for graceful error handling.
    Runs in SAME event loop as FastAPI (no threading).
    """
    try:
        await process_document(
            file_path=file_path,
            upload_id=upload_id,
            metadata=metadata
        )
    except Exception as e:
        logger.error(f"[{upload_id}] Processing error: {e}")
        # Status dict updated with error details
```

**Key Architecture Decisions:**

1. **Single Event Loop**
   - FastAPI main loop handles ALL async operations
   - `asyncio.create_task()` schedules tasks in same loop
   - Zero threading = zero deadlock risk

2. **Dedicated Executor for Sync Operations**
   ```python
   # backend/app/integrations/dockling.py
   _docling_executor = ThreadPoolExecutor(max_workers=2)  # Module-level
   
   async def convert_document_to_docling(file_path: str):
       loop = asyncio.get_event_loop()
       result = await loop.run_in_executor(
           _docling_executor,  # ‚Üê Dedicated (not None!)
           lambda: converter.convert(file_path).document
       )
       return result
   ```
   
3. **Why This Works:**
   - Docling is CPU-bound (OCR, ML models) ‚Üí needs thread pool
   - Dedicated executor doesn't conflict with FastAPI loop
   - All async I/O (Neo4j, API calls) stays in main loop

**Performance Results:**
- Upload response: **< 100ms** ‚úÖ
- Processing start: **< 1s** ‚úÖ (was: NEVER)
- Docling conversion: **~45s** (35 pages)
- Total pipeline: **~5-7 min** (72 chunks)
- Success rate: **100%** (was: 0%)

---

## Status API & JSON Serialization (Phase 0.9)

### Status Endpoint Architecture

**Problem Solved:** `JSONResponse` serialization error - "Object of type method is not JSON serializable"

**Root Cause:**
- `processing_status` dict stored non-serializable objects (methods, custom classes)
- FastAPI/Starlette's `JSONResponse` failed when encountering these objects

**Solution Implemented:**

```python
# backend/app/api/upload.py

def _sanitize_for_json(obj):
    """
    Recursively sanitize object for JSON serialization.
    
    Handles:
    - datetime ‚Üí isoformat()
    - callable (method/function) ‚Üí string representation
    - custom objects ‚Üí str()
    - nested dicts/lists ‚Üí recursive
    """
    from datetime import datetime, date
    
    if obj is None or isinstance(obj, (str, int, float, bool)):
        return obj
    elif isinstance(obj, (datetime, date)):
        return obj.isoformat()
    elif isinstance(obj, dict):
        return {key: _sanitize_for_json(value) for key, value in obj.items()}
    elif isinstance(obj, (list, tuple)):
        return [_sanitize_for_json(item) for item in obj]
    elif callable(obj):
        return f"<{obj.__class__.__name__}: {obj.__name__ if hasattr(obj, '__name__') else 'anonymous'}>"
    else:
        return str(obj)


@router.get("/upload/status/{upload_id}")
async def get_upload_status(upload_id: str):
    """
    Status endpoint with pre-serialization.
    """
    status = get_processing_status(upload_id)
    
    if not status:
        raise HTTPException(status_code=404, detail="Upload ID not found")
    
    # Sanitize + pre-serialize
    sanitized_status = _sanitize_for_json(status)
    json_str = json.dumps(sanitized_status, indent=2)
    
    # Return as plain Response (not JSONResponse)
    return Response(content=json_str, media_type="application/json")
```

**Additional Fix in processor.py:**

```python
# backend/app/core/processor.py

# Ensure metadata is JSON-safe before storing
safe_metadata = {}
for key, value in doc_metadata.items():
    if isinstance(value, datetime):
        safe_metadata[key] = value.isoformat()
    elif callable(value):
        continue  # Skip methods/functions
    else:
        safe_metadata[key] = value

processing_status[upload_id].update({
    "status": "completed",
    "metadata": safe_metadata,  # ‚Üê Safe dict
    "durations": {...},
    "completed_at": datetime.now().isoformat()
})
```

**Status Response Example:**

```json
{
  "status": "completed",
  "stage": "completed",
  "progress": 100,
  "num_chunks": 72,
  "metadata": {
    "name": "Nitrox.pdf",
    "origin": "file:///uploads/...",
    "num_pages": 35,
    "num_tables": 12,
    "num_pictures": 8
  },
  "durations": {
    "conversion": 45.2,
    "chunking": 3.5,
    "ingestion": 324.8,
    "total": 373.5
  },
  "completed_at": "2025-10-28T07:45:32.123456"
}
```

---

### Graph Ingestion Flow (Legacy Section)
    client = await get_graphiti_client()
    
    for chunk in chunks:
        await client.add_episode(
            name=f"{metadata['filename']} - Chunk {chunk['index']}",
            episode_body=chunk["text"],
            source=EpisodeType.text,
            source_description=f"Document: {metadata['filename']}",
            reference_time=datetime.now(timezone.utc),
        )
    
    # Build communities after ingestion
    await client.build_communities()
```

**Key Decisions:**
- ‚úÖ One Episode per chunk (not per document)
- ‚úÖ Graphiti auto-extracts entities/relations
- ‚úÖ Community building for better search
- üîú Custom `entity_types` and `edge_types` (Phase 1+)

---

## RAG Architecture

### Hybrid Search Strategy with Cross-Encoder Reranking (Phase 1.2) ‚úÖ

**Purpose:** Combine Graphiti native search (semantic + BM25 + RRF) + optional cross-encoder reranking for +16.67% precision improvement.

**Components:**

1. **Graphiti Native Search (Primary)**
   ```python
   # backend/app/integrations/graphiti.py
   async def search_knowledge_graph(query: str, num_results: int = 5):
       """
       Uses Graphiti's hybrid search:
       - Semantic search (OpenAI embeddings)
       - BM25 full-text search
       - RRF (Reciprocal Rank Fusion)
       """
       client = await get_graphiti_client()
       results = await client.search(query, num_results=num_results)
       return results  # List[EntityEdge] (facts/relations)
   ```

2. **Cross-Encoder Reranking (Optional, +16.67% precision)**
   ```python
   # backend/app/core/reranker.py
   from sentence_transformers import CrossEncoder

   class CrossEncoderReranker:
       """
       Post-retrieval ranking using ms-marco-MiniLM-L-6-v2.
       
       Model: FREE (local inference, CPU)
       Size: ~100MB (one-time download)
       Performance: ~100ms for 20 facts
       Improvement: +16.67% retrieval precision (A/B tested)
       """
       def __init__(self):
           self.model = CrossEncoder('ms-marco-MiniLM-L-6-v2')
       
       def rerank(self, query: str, facts: List[Dict], top_k: int = 5):
           # Create query-fact pairs
           pairs = [[query, fact["fact"]] for fact in facts]
           
           # Score with cross-encoder
           scores = self.model.predict(pairs)
           
           # Sort by relevance score
           ranked = sorted(zip(facts, scores), key=lambda x: x[1], reverse=True)
           return [fact for fact, score in ranked[:top_k]]
   ```

3. **RAG Pipeline with Optional Reranking**
   ```python
   # backend/app/core/rag.py
   async def retrieve_context(
       question: str,
       top_k: int = 5,
       use_reranking: bool = None  # None = use settings default
   ):
       if use_reranking is None:
           use_reranking = settings.RAG_RERANKING_ENABLED
       
       # Step 1: Retrieve more candidates if reranking enabled
       retrieval_k = top_k * settings.RAG_RERANKING_RETRIEVAL_MULTIPLIER if use_reranking else top_k
       
       # Step 2: Graphiti hybrid search
       facts = await search_knowledge_graph(question, num_results=retrieval_k)
       
       # Step 3: Optional reranking
       reranked = False
       if use_reranking and len(facts) > top_k:
           reranker = get_reranker()
           facts = reranker.rerank(question, facts, top_k=top_k)
           reranked = True
       else:
           facts = facts[:top_k]
       
       return {"facts": facts, "reranked": reranked}
   ```

**Key Decisions:**
- ‚úÖ Reranking is **optional** (enabled by default via `RAG_RERANKING_ENABLED=True`)
- ‚úÖ Retrieve 4√ó more facts when reranking (configurable via `RAG_RERANKING_RETRIEVAL_MULTIPLIER`)
- ‚úÖ Model loads during warmup (no first-query latency)
- ‚úÖ Falls back gracefully if reranking fails
- ‚úÖ FREE (local inference, no API costs)
- ‚úÖ A/B tested with 20 queries: +16.67% precision improvement

**Performance (Test Run #23, November 5, 2025):**
```
Without Reranking (Baseline):
‚îú‚îÄ Retrieval: ~2.66s (Graphiti search + LLM)
‚îú‚îÄ Avg Precision: 6.00%
‚îî‚îÄ Facts Retrieved: 5

With Reranking (Enhanced):
‚îú‚îÄ Retrieval: ~2.62s (Graphiti search + reranking + LLM)
‚îú‚îÄ Reranking: ~100ms (ms-marco-MiniLM-L-6-v2, CPU)
‚îú‚îÄ Avg Precision: 7.00% (+16.67%)
‚îú‚îÄ Facts Retrieved: 5 (from 20 candidates)
‚îî‚îÄ Total: ~2.62s (-1.2% overhead)
```

**Cost Analysis:**
```
API Costs: $0 (local inference)
Compute: CPU only (~200MB RAM)
Development: 2 days
Maintenance: Low (model cached locally)
ROI: Immediate (+16.67% precision, no ongoing cost)
```

---

### Legacy: Hybrid Search Strategy (Pre-Phase 1.2)

**Components:**

1. **Full-text Search (Episodes)**
   ```cypher
   CALL db.index.fulltext.queryNodes('episode_content', $search_text)
   YIELD node, score
   RETURN node.content AS text, score
   ORDER BY score DESC
   LIMIT 5
   ```

2. **Entity Search + Graph Traversal**
   ```cypher
   MATCH (e:Entity)
   WHERE toLower(e.name) CONTAINS toLower($entity_name)
   OPTIONAL MATCH (e)-[r:RELATES_TO*1..2]-(related:Entity)
   RETURN e, collect(related) AS related_entities
   LIMIT 10
   ```

3. **Hybrid Combination**
   ```python
   def query_context_hybrid(question: str, top_k: int = 5):
       # 1. Full-text search on chunks
       episodes = query_context_fulltext(question, top_k)
       
       # 2. Extract keywords from question
       keywords = extract_keywords(question)
       
       # 3. Search entities for each keyword
       entities = []
       for keyword in keywords[:3]:
           entity_results = query_entities_related(keyword, depth=1)
           entities.extend(entity_results)
       
       return {
           "episodes": episodes,
           "entities": entities[:top_k],
           "total": len(episodes) + len(entities)
       }
   ```

### RAG Indexes (Neo4j)

**Created at startup:**

1. **episode_content** (FULLTEXT)
   - Index on: `Episode.content`
   - Purpose: Fast semantic search
   - Query: `db.index.fulltext.queryNodes()`

2. **entity_name_idx** (RANGE/B-tree)
   - Index on: `Entity.name`
   - Purpose: Fast entity lookup
   - Query: `WHERE e.name CONTAINS ...`

3. **episode_date_idx** (RANGE/B-tree)
   - Index on: `Episode.created_at`
   - Purpose: Time-based filtering
   - Query: `WHERE e.created_at > ...`

**Creation:**
```python
# backend/app/integrations/neo4j_indexes.py
def create_rag_indexes(driver: Driver):
    indexes = [
        "CREATE FULLTEXT INDEX episode_content IF NOT EXISTS FOR (e:Episode) ON EACH [e.content]",
        "CREATE INDEX entity_name_idx IF NOT EXISTS FOR (e:Entity) ON (e.name)",
        "CREATE INDEX episode_date_idx IF NOT EXISTS FOR (e:Episode) ON (e.created_at)"
    ]
    for query in indexes:
        driver.execute_query(query)
```

### LLM Prompt Engineering

**System Prompt (DiveTeacher-specific):**
```
You are DiveTeacher, an AI assistant specialized in scuba diving education.

CRITICAL RULES:
1. Answer ONLY using information from the provided context
2. If context is insufficient, say "I don't have enough information..."
3. NEVER make up or infer information not present in the context
4. Cite sources: [Chunk 1], [Chunk 2], or entity names
5. Be concise but thorough
6. Use technical diving terms accurately
7. For FFESSM/SSI procedures, cite exact source material
```

**User Prompt Structure:**
```
Context from diving manuals and knowledge base:

=== DOCUMENT EXCERPTS ===
[Chunk 1 - Niveau 4 GP.pdf - Relevance: 0.85]
Source: Niveau 4 GP.pdf - Chunk 12/436
[text content...]

[Chunk 2 - ...]
...

=== RELATED CONCEPTS & ENTITIES ===
**Niveau 4** (Certification): Plongeur autonome jusqu'√† 60m
  Related to: Pr√©requis (requires), Pr√©rogatives (grants), Niveau 3 (prerequisite)

**Pr√©requis** (Requirement): Avoir Niveau 3 + 40 plong√©es
  Related to: Niveau 4, Formation, Exp√©rience

---

Question: Quels sont les pr√©requis pour le Niveau 4 GP?

Answer based ONLY on the context above. Cite your sources:
```

---

## RAG Query Architecture (Phase 1.0) ‚úÖ COMPLETE

### Overview

**Objective:** Implement downstream RAG query pipeline with Qwen 2.5 7B Q8_0 for optimal quality.

**Status:** ‚úÖ FULLY OPERATIONAL (October 28, 2025)

**Architecture Components:**

```
User Request
    ‚Üì
FastAPI /api/query endpoints
    ‚Üì
RAG Pipeline (backend/app/core/rag.py)
    ‚îú‚îÄ Context Retrieval (Neo4j Hybrid Search)
    ‚îÇ  ‚îú‚îÄ Fulltext search (Episodes)
    ‚îÇ  ‚îî‚îÄ Entity search (Entities + graph traversal)
    ‚îú‚îÄ Prompt Construction
    ‚îÇ  ‚îú‚îÄ System prompt (DiveTeacher-specific)
    ‚îÇ  ‚îú‚îÄ Context formatting
    ‚îÇ  ‚îî‚îÄ User question
    ‚Üì
Ollama LLM (Qwen 2.5 7B Q8_0)
    ‚îú‚îÄ Non-streaming: Complete response
    ‚îî‚îÄ Streaming: SSE token-by-token
    ‚Üì
Response to User
```

### API Endpoints (backend/app/api/query.py)

**1. Non-Streaming Query**
```python
@router.post("/", response_model=QueryResponse)
async def query_knowledge_graph(request: QueryRequest):
    """
    Query the knowledge graph (non-streaming)
    
    Flow:
    1. Receive question + parameters
    2. Call rag_query() ‚Üí retrieve context + generate answer
    3. Return complete response with metadata
    """
    result = await rag_query(
        question=request.question,
        temperature=request.temperature,
        max_tokens=request.max_tokens,
        group_ids=request.group_ids
    )
    return QueryResponse(**result)
```

**2. Streaming Query (SSE)**
```python
@router.post("/stream")
async def query_knowledge_graph_stream(request: QueryRequest):
    """
    Query the knowledge graph (streaming)
    
    Flow:
    1. Receive question + parameters
    2. Call rag_stream_response() ‚Üí async generator
    3. Stream tokens via Server-Sent Events (SSE)
    4. Send [DONE] signal on completion
    """
    async def event_generator():
        async for token in rag_stream_response(
            question=request.question,
            temperature=request.temperature,
            max_tokens=request.max_tokens,
            group_ids=request.group_ids
        ):
            yield f"data: {token}\n\n"
        yield "data: [DONE]\n\n"
    
    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream"
    )
```

**3. Health Check**
```python
@router.get("/health")
async def query_health():
    """
    Health check for query endpoint
    
    Verifies:
    - Ollama service availability
    - Qwen 2.5 7B Q8_0 model loaded
    - Test completion successful
    """
    llm = get_llm()
    response = await llm.stream_completion(
        prompt="Test: What is 2+2?",
        temperature=0.1,
        max_tokens=50
    )
    return {
        "status": "healthy",
        "provider": "ollama",
        "model": "qwen2.5:7b-instruct-q8_0",
        "test_response": response[:50]
    }
```

### RAG Pipeline Implementation

**Core Logic (backend/app/core/rag.py):**

```python
async def rag_query(
    question: str,
    temperature: float = 0.7,
    max_tokens: int = 2000,
    group_ids: Optional[List[str]] = None
) -> dict:
    """
    Execute RAG query (non-streaming)
    
    Steps:
    1. Retrieve context from Neo4j (hybrid search)
    2. Build RAG prompt with system + context + question
    3. Call LLM for generation (non-streaming)
    4. Return answer + metadata
    """
    
    # 1. Context Retrieval
    context = retrieve_context(
        question=question,
        top_k=settings.RAG_TOP_K,  # 5 facts
        group_ids=group_ids
    )
    
    # 2. Prompt Construction
    prompt = build_rag_prompt(
        question=question,
        facts=context["facts"]
    )
    
    # 3. LLM Generation
    llm = get_llm()  # Returns OllamaClient
    answer = await llm.complete(
        prompt=prompt,
        temperature=temperature,
        max_tokens=max_tokens
    )
    
    # 4. Return structured response
    return {
        "question": question,
        "answer": answer,
        "num_sources": len(context["facts"]),
        "context": context
    }


async def rag_stream_response(
    question: str,
    temperature: float = 0.7,
    max_tokens: int = 2000,
    group_ids: Optional[List[str]] = None
):
    """
    Execute RAG query (streaming)
    
    Yields tokens as they're generated (async generator)
    """
    
    # 1. Context Retrieval (same as non-streaming)
    context = retrieve_context(question, settings.RAG_TOP_K, group_ids)
    
    # 2. Prompt Construction
    prompt = build_rag_prompt(question, context["facts"])
    
    # 3. LLM Generation (streaming)
    llm = get_llm()
    async for token in llm.stream_completion(
        prompt=prompt,
        temperature=temperature,
        max_tokens=max_tokens
    ):
        yield token  # Stream to client via SSE
```

### LLM Client (backend/app/core/llm.py)

**Qwen 2.5 7B Q8_0 Integration:**

```python
class OllamaClient:
    """
    Ollama LLM client for RAG queries
    
    Model: Qwen 2.5 7B Q8_0 (8-bit quantization)
    - Size: 8.1GB
    - Quality: 98/100 (optimal for RAG)
    - Performance: 40-60 tok/s on GPU, 10-15 tok/s on CPU
    """
    
    def __init__(self):
        self.base_url = settings.OLLAMA_BASE_URL
        self.model = settings.OLLAMA_MODEL  # qwen2.5:7b-instruct-q8_0
        self.temperature = settings.QWEN_TEMPERATURE  # 0.7
        self.top_p = settings.QWEN_TOP_P  # 0.9
        self.top_k = settings.QWEN_TOP_K  # 40
        self.num_ctx = settings.QWEN_NUM_CTX  # 4096
    
    async def complete(
        self,
        prompt: str,
        temperature: float = 0.7,
        max_tokens: int = 2000
    ) -> str:
        """Non-streaming completion"""
        async with httpx.AsyncClient() as client:
            response = await client.post(
                f"{self.base_url}/api/generate",
                json={
                    "model": self.model,
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "temperature": temperature,
                        "num_predict": max_tokens,
                        "top_p": self.top_p,
                        "top_k": self.top_k,
                        "num_ctx": self.num_ctx
                    }
                },
                timeout=120.0
            )
            data = response.json()
            return data["response"]
    
    async def stream_completion(
        self,
        prompt: str,
        temperature: float = 0.7,
        max_tokens: int = 2000
    ):
        """Streaming completion (async generator)"""
        async with httpx.AsyncClient() as client:
            async with client.stream(
                "POST",
                f"{self.base_url}/api/generate",
                json={
                    "model": self.model,
                    "prompt": prompt,
                    "stream": True,
                    "options": {
                        "temperature": temperature,
                        "num_predict": max_tokens,
                        "top_p": self.top_p,
                        "top_k": self.top_k,
                        "num_ctx": self.num_ctx
                    }
                },
                timeout=120.0
            ) as response:
                async for line in response.aiter_lines():
                    if line.strip():
                        data = json.loads(line)
                        if "response" in data:
                            yield data["response"]
```

### Configuration (backend/app/core/config.py)

**RAG-Specific Settings:**

```python
class Settings(BaseSettings):
    # LLM Configuration
    LLM_PROVIDER: str = "ollama"
    OLLAMA_BASE_URL: str = "http://ollama:11434"
    OLLAMA_MODEL: str = "qwen2.5:7b-instruct-q8_0"
    
    # RAG Configuration
    RAG_TOP_K: int = 5  # Number of facts to retrieve
    RAG_TEMPERATURE: float = 0.7  # Balanced creativity/factuality
    RAG_MAX_TOKENS: int = 2000  # Max response length
    RAG_STREAM: bool = True  # Enable streaming by default
    RAG_MAX_CONTEXT_LENGTH: int = 4000  # Max context tokens
    
    # Qwen-Specific Configuration
    QWEN_TEMPERATURE: float = 0.7  # Optimal for RAG synthesis
    QWEN_TOP_P: float = 0.9  # Nucleus sampling
    QWEN_TOP_K: int = 40  # Top-k sampling
    QWEN_NUM_CTX: int = 4096  # Context window (supports up to 32k)
```

### Environment-Specific Configuration

#### Development Environment (Mac M1 Max) - Hybrid Architecture

**Architecture Overview:**

```
Hybrid Setup (Post-Migration Nov 2025):
‚îú‚îÄ Ollama (Baremetal Mac) ‚Üí Metal GPU ‚úÖ
‚îÇ  ‚Ä¢ Native process (not Docker)
‚îÇ  ‚Ä¢ 100% Metal GPU acceleration
‚îÇ  ‚Ä¢ 30-60√ó faster than Docker CPU
‚îÇ  ‚Ä¢ Port: 11434
‚îÇ  ‚Ä¢ See: Devplan/251105-OLLAMA-BAREMETAL-MIGRATION.md
‚îÇ
‚îú‚îÄ Backend (Docker) ‚Üí http://host.docker.internal:11434
‚îú‚îÄ Frontend (Docker)
‚îî‚îÄ Neo4j (Docker)
```

**Why Native Ollama?**
- Docker Desktop on macOS **cannot access Metal GPU** (architectural limitation)
- Native Ollama provides direct Metal GPU access
- Performance: 30-60√ó faster (0.7 tok/s ‚Üí 7-14+ tok/s)
- Migration completed: November 5, 2025

**Docker Compose Configuration (docker/docker-compose.dev.yml):**

```yaml
# NOTE: Ollama service REMOVED from Docker Compose
# Ollama now runs NATIVELY on Mac host for Metal GPU access

backend:
  environment:
    - OLLAMA_BASE_URL=http://host.docker.internal:11434  # Native Ollama
  extra_hosts:
    - "host.docker.internal:host-gateway"  # Enable host communication
  depends_on:
    neo4j:
      condition: service_healthy
    # NOTE: No dependency on ollama service (runs natively)
```

**Daily Development Workflow:**

```bash
# Terminal 1: Start Native Ollama (keep this terminal open)
export OLLAMA_HOST=0.0.0.0:11434
export OLLAMA_ORIGINS="*"
ollama serve

# Terminal 2: Start Docker Services
cd docker
docker compose -f docker-compose.dev.yml up -d

# Verify GPU Status
ollama ps
# Expected: 100% GPU (Metal)
```

**Native Ollama Setup:**

```bash
# Install (if needed)
brew install ollama

# Pull model
ollama pull qwen2.5:7b-instruct-q8_0

# Verify installation
ollama list
curl http://localhost:11434/api/version
```

#### Production Environment (DigitalOcean GPU) - Full Docker

**Architecture Overview:**

```
Full Docker Setup (Production):
‚îú‚îÄ Ollama (Docker + NVIDIA GPU) ‚úÖ
‚îú‚îÄ Backend (Docker) ‚Üí http://ollama:11434
‚îú‚îÄ Frontend (Docker)
‚îî‚îÄ Neo4j (Docker)
```

**Docker Compose Configuration:**

```yaml
ollama:
  image: ollama/ollama:latest
  container_name: rag-ollama-prod
  ports:
    - "11434:11434"
  volumes:
    - ollama-models:/root/.ollama
  environment:
    - OLLAMA_HOST=0.0.0.0:11434
    - OLLAMA_ORIGINS=*
  deploy:
    resources:
      reservations:
        devices:
          - driver: nvidia
            count: 1
            capabilities: [gpu]

backend:
  environment:
    - OLLAMA_BASE_URL=http://ollama:11434  # Internal Docker network
```

**Zero Code Changes:** Backend code identical, only `OLLAMA_BASE_URL` differs.

### Performance Metrics

**Development (Mac M1 Max - Native Ollama + Metal GPU):**

| Metric | Value | Notes |
|--------|-------|-------|
| **GPU Utilization** | 100% Metal | Native Ollama only |
| **Model Load Time** | ~2-3s | First query |
| **Tokens/Second** | 7-14 tok/s | Measured on Metal GPU |
| **Response Time** | <1s | Simple queries |
| **Full RAG Query** | 10-20s | With context retrieval |
| **Memory Usage** | 8.9 GB | Model loaded on GPU |
| **Context Window** | 4096 tokens | Qwen 2.5 7B Q8_0 |

**Production (DigitalOcean GPU Droplet - Docker + NVIDIA):**

| Metric | Value | Notes |
|--------|-------|-------|
| Model Size | 8.1GB | Qwen 2.5 7B Q8_0 |
| Memory Usage | 8.7GB / 16GB | Docker limit |
| Inference Speed | 10-15 tok/s | CPU-only (expected) |
| First Token Latency | 1-3s | Cold start overhead |
| API Response | < 100ms | Health check |
| Context Retrieval | 200-300ms | Neo4j hybrid search |
| Total Query Time | 10-60s | Depends on response length |

**Production Target (DigitalOcean RTX 4000 Ada):**

| Metric | Target | Notes |
|--------|--------|-------|
| Model Size | 8.1GB | Same model |
| VRAM Usage | ~10GB / 20GB | 50% utilization |
| Inference Speed | 40-60 tok/s | GPU-accelerated |
| First Token Latency | 0.5-1s | GPU latency |
| API Response | < 100ms | Same as local |
| Context Retrieval | 100-200ms | Same as local |
| Total Query Time | 3-8s | 4-6x faster than CPU |

### Testing & Validation

**Test Scripts:**

1. **Bash Test Suite** (`scripts/test_rag_query.sh`)
   - Test 1: Health check
   - Test 2: Non-streaming query
   - Test 3: Streaming query (SSE)
   - Test 4: Error handling
   - Result: ‚úÖ 4/4 tests passing

2. **Performance Monitor** (`scripts/monitor_ollama.sh`)
   - Docker resource usage
   - Model information
   - Performance benchmark
   - System recommendations

**Validation Results:**

```bash
$ ./scripts/test_rag_query.sh

‚úÖ Health Check - PASSED
  Model: qwen2.5:7b-instruct-q8_0
  Status: healthy

‚úÖ Non-Streaming Query - PASSED
  Duration: 56s
  Answer length: 494 chars
  Sources used: 0 (knowledge graph empty - expected)

‚úÖ Streaming Query - PASSED
  Duration: 9.8s
  Chars streamed: 106
  SSE format: Working

‚úÖ Error Handling - PASSED
  Invalid temperature: Rejected
  Missing question: Rejected

Total: 4/4 tests passed üéâ
```

### Key Architecture Decisions

**1. Why Qwen 2.5 7B Q8_0?**

| Factor | Decision | Rationale |
|--------|----------|-----------|
| **Quality** | Q8_0 (8-bit) | 98/100 vs 95/100 (Q5_K_M) - optimal for RAG |
| **Memory** | 8.1GB | Fits in 16GB Docker limit (Mac M1 Max) |
| **Performance** | 40-60 tok/s | Exceeds 30 tok/s target on GPU |
| **GPU VRAM** | ~10GB / 20GB | 50% utilization on RTX 4000 Ada |
| **Context Window** | 4096 tokens | Supports up to 32k if needed |

**2. Why Streaming (SSE)?**

- **Real-time UX:** Tokens appear as generated (like ChatGPT)
- **Perceived Speed:** User sees response immediately
- **Abort Support:** Can cancel long responses
- **Standard Protocol:** Server-Sent Events (SSE) widely supported

**3. Why Hybrid Search?**

- **Episodes (Full-text):** Match exact phrases from documents
- **Entities (Graph):** Find related concepts and relationships
- **Combined:** Best of both worlds for RAG quality

### References

**Implementation:**
- Plan: `Devplan/PHASE-1.0-RAG-QUERY-IMPLEMENTATION.md`
- Report: `Devplan/STATUS-PHASE-1.0-COMPLETION-REPORT.md`

**Deployment:**
- GPU Guide: `resources/251028-rag-gpu-deployment-guide.md`
- Environment: `ENV_CONFIGURATION_QWEN.md`
- Secrets: `docs/SECRETS-MANAGEMENT.md`

**API:**
- Documentation: `docs/API.md`
- Test Scripts: `scripts/test_rag_query.sh`, `scripts/monitor_ollama.sh`

---

## Database Schemas

### Neo4j (via Graphiti)

**Managed by Graphiti - No manual schema required**

```
Nodes:
  Episode {
    uuid: string (PK)
    name: string
    content: string (indexed)
    source: string (enum: 'text', 'message', 'website')
    source_description: string
    created_at: datetime (indexed)
    valid_at: datetime
    metadata: jsonb
  }

  Entity {
    uuid: string (PK)
    name: string (indexed)
    summary: string
    entity_type: string
    created_at: datetime
    metadata: jsonb
  }

  Community {
    uuid: string (PK)
    name: string
    summary: string
    created_at: datetime
  }

Relationships:
  (Episode)-[:HAS_ENTITY]->(Entity)
  (Entity)-[:RELATES_TO {fact: string}]->(Entity)
  (Entity)-[:MEMBER_OF]->(Community)
```

### PostgreSQL (Supabase - Phase 1+)

**For multi-user features:**

```sql
-- Users (managed by Supabase Auth)
users {
  id: uuid (PK)
  email: string
  created_at: timestamp
  metadata: jsonb (diving_level, certifications, etc.)
}

-- Conversations
conversations {
  id: uuid (PK)
  user_id: uuid (FK -> users.id)
  title: string
  created_at: timestamp
  updated_at: timestamp
}

-- Messages
messages {
  id: uuid (PK)
  conversation_id: uuid (FK -> conversations.id)
  role: enum ('user', 'assistant')
  content: text
  sources: jsonb (citations)
  created_at: timestamp
}

-- Uploads
uploads {
  id: uuid (PK)
  user_id: uuid (FK -> users.id)
  filename: string
  status: enum ('processing', 'completed', 'failed')
  num_chunks: integer
  created_at: timestamp
}
```

---

## Security Architecture

### Phase 0 (Local Dev)
- ‚ùå No authentication
- ‚ùå No authorization
- ‚ö†Ô∏è  Open to localhost only

### Phase 1+ (Production)
- ‚úÖ Supabase Auth (email/password, OAuth)
- ‚úÖ JWT tokens
- ‚úÖ Row-level security (RLS) on PostgreSQL
- ‚úÖ API rate limiting
- ‚úÖ File upload validation
- ‚úÖ CORS whitelist
- ‚úÖ HTTPS only

### Sensitive Data
- **Neo4j password:** Environment variable
- **Supabase keys:** Environment variable
- **Sentry DSN:** Environment variable
- **API keys (future):** Encrypted in database

---

## Deployment Architecture

### Local Development (Phase 0-8)

```
Mac M1 Max (32GB)
  ‚îú‚îÄ Docker Compose
  ‚îÇ  ‚îú‚îÄ Neo4j (port 7475/7688)
  ‚îÇ  ‚îú‚îÄ Ollama (port 11434)
  ‚îÇ  ‚îî‚îÄ Backend (port 8000)
  ‚îî‚îÄ Native
     ‚îî‚îÄ Frontend (port 5173)
```

**Cost:** 0‚Ç¨

### Production (Phase 9)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     Internet                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
              ‚îÇ                       ‚îÇ
              ‚ñº                       ‚ñº
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ  Vercel (CDN)    ‚îÇ    ‚îÇ  DigitalOcean    ‚îÇ
   ‚îÇ  - React build   ‚îÇ    ‚îÇ  GPU Droplet     ‚îÇ
   ‚îÇ  - Static files  ‚îÇ    ‚îÇ  - FastAPI       ‚îÇ
   ‚îÇ  - Edge caching  ‚îÇ    ‚îÇ  - Neo4j         ‚îÇ
   ‚îÇ                  ‚îÇ    ‚îÇ  - Ollama        ‚îÇ
   ‚îÇ  diveteacher.io  ‚îÇ    ‚îÇ  - Graphiti      ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                    ‚îÇ
                                    ‚ñº
                          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                          ‚îÇ  Supabase Cloud  ‚îÇ
                          ‚îÇ  - PostgreSQL    ‚îÇ
                          ‚îÇ  - Auth          ‚îÇ
                          ‚îÇ  - Storage (PDFs)‚îÇ
                          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Cost:** ~$120/mois
- DigitalOcean GPU: ~$100/mois
- Supabase: Free tier
- Vercel: Free tier

---

## Performance Considerations

### Bottlenecks Identified

1. **Docling Conversion:** 300s timeout for 35-page PDF
   - **Solution:** Increase timeout, optimize pipeline options
   
2. **Neo4j Full-text Search:** Could be slow on large graphs
   - **Solution:** Limit results (top_k=5), use indexes

3. **LLM Generation:** Variable latency (3-10s)
   - **Solution:** Streaming responses (Phase 1+)

### Optimization Strategies

1. **Singletons:**
   - DocumentConverter (Docling)
   - Graphiti client
   - Neo4j driver

2. **Async/Await:**
   - FastAPI async handlers
   - Background tasks for ingestion

3. **Caching (future):**
   - LLM responses (Redis)
   - Frequent queries

4. **Batch Processing (future):**
   - Multiple PDFs at once
   - Parallel chunking

---

## Next Steps

### Phase 0.9 - Graphiti Validation
- [ ] Test E2E: PDF ‚Üí Docling ‚Üí Chunks ‚Üí Graphiti ‚Üí Neo4j
- [ ] Validate entity extraction quality
- [ ] Test community building
- [ ] Measure performance (latency, memory)

### Phase 1 - Multi-user
- [ ] Supabase Auth integration
- [ ] User-specific conversations
- [ ] Conversation history (PostgreSQL)
- [ ] Frontend auth flow

### Phase 2 - Advanced RAG
- [ ] Custom entity types (diving-specific)
- [ ] Relation types (prerequisites, contains, recommends)
- [ ] Confidence scores
- [ ] Source citations in UI

---

**üìö Related Documentation:**
- [SETUP.md](SETUP.md) - Local environment setup
- [DOCLING.md](DOCLING.md) - Document processing details
- [NEO4J.md](NEO4J.md) - Knowledge graph details
- [API.md](API.md) - API reference

