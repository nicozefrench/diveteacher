# Cloud Migration Guide - Ollama from Baremetal to Docker GPU

**Date:** November 5, 2025  
**Context:** Migration from Mac M1 Max (Metal GPU) to DigitalOcean GPU Droplet (NVIDIA)  
**Status:** ğŸ“‹ **READY FOR FUTURE MIGRATION**

---

## ğŸ¯ OBJECTIF

Migrer Ollama de l'architecture hybride actuelle (native Mac) vers une architecture full-Docker en production avec GPU NVIDIA, **sans modifier une seule ligne de code**.

---

## ğŸ“ ARCHITECTURE ACTUELLE vs CIBLE

### Actuel (Dev Local - Mac M1 Max)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Ollama Natif    â”‚ â† InstallÃ© via brew, tourne sur Mac
â”‚ :11434 (Metal)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ HTTP API
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Docker Services                 â”‚
â”‚ â”œâ”€ Backend â†’ http://host.docker.internal:11434
â”‚ â”œâ”€ Frontend                     â”‚
â”‚ â””â”€ Neo4j                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Performance:** 7-14 tok/s (Metal GPU)

### Cible (Production - DigitalOcean GPU Droplet)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Docker Stack                    â”‚
â”‚ â”œâ”€ Ollama â†’ :11434 (NVIDIA GPU)â”‚
â”‚ â”œâ”€ Backend â†’ http://ollama:11434
â”‚ â”œâ”€ Frontend                     â”‚
â”‚ â””â”€ Neo4j                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Performance attendue:** 40-60 tok/s (NVIDIA GPU)

---

## ğŸ”‘ POINTS CLÃ‰S - Pourquoi C'est Sans Risque

### 1. API Ollama Identique

L'API REST d'Ollama est **strictement identique** qu'elle tourne:
- En natif sur Mac (Metal GPU)
- Dans Docker CPU
- Dans Docker GPU (NVIDIA)

**MÃªme endpoints, mÃªme format JSON, mÃªme comportement.**

### 2. Abstraction par URL

Le backend ne connaÃ®t qu'**une seule variable d'environnement:**

```bash
OLLAMA_BASE_URL=<url_ollama>
```

Cette URL change selon l'environnement:
- **Dev:** `http://host.docker.internal:11434` (pointe vers Mac host)
- **Prod:** `http://ollama:11434` (pointe vers container Docker)

**ZÃ©ro modification de code nÃ©cessaire.**

### 3. Docker Compose Overrides

Pattern standard Docker Compose avec fichiers multiples:
- `docker-compose.dev.yml` â†’ Overrides dev (pas de service Ollama)
- `docker-compose.prod.yml` â†’ Overrides prod (service Ollama avec NVIDIA GPU)

---

## ğŸš€ CE QUI SE PASSE LORS DE LA MIGRATION CLOUD

### Changements Effectifs

#### 1. Ollama passe de Natif â†’ Docker avec GPU

**Local (avant):**
```bash
# Terminal 1
ollama serve  # Natif sur Mac, port :11434, Metal GPU
```

**Cloud (aprÃ¨s):**
```bash
# Tout dans Docker
docker compose -f docker-compose.prod.yml up -d
# Ollama dans container avec NVIDIA GPU, port :11434
```

#### 2. Une Seule Variable Change

**`.env` (local - actuel):**
```bash
OLLAMA_BASE_URL=http://host.docker.internal:11434
OLLAMA_MODEL=qwen2.5:7b-instruct-q8_0
```

**`.env.prod` (cloud - futur):**
```bash
OLLAMA_BASE_URL=http://ollama:11434
OLLAMA_MODEL=qwen2.5:7b-instruct-q8_0
```

#### 3. Commande de Lancement DiffÃ©rente

**Local (actuel):**
```bash
# Terminal 1: Ollama natif
export OLLAMA_HOST=0.0.0.0:11434
export OLLAMA_ORIGINS="*"
ollama serve

# Terminal 2: Services Docker
docker compose -f docker/docker-compose.dev.yml up -d
```

**Cloud (futur):**
```bash
# Une seule commande, tout dans Docker
docker compose -f docker/docker-compose.prod.yml up -d
```

### Ce Qui NE Change PAS

| Composant | Status |
|-----------|--------|
| **Code backend/frontend** | âœ… Identique (0 ligne modifiÃ©e) |
| **API calls Ã  Ollama** | âœ… Identique (mÃªme format JSON) |
| **ModÃ¨les utilisÃ©s** | âœ… Identiques (qwen2.5:7b-instruct-q8_0) |
| **Base de donnÃ©es Neo4j** | âœ… Identique (mÃªme config) |
| **Logique RAG** | âœ… Identique |
| **Performance GPU** | âœ… GPU dans les 2 cas (Metal local â†’ NVIDIA cloud) |

---

## ğŸ“‹ PROCÃ‰DURE DE MIGRATION (Step-by-Step)

### Phase 1: PrÃ©paration (Sur Mac Local)

#### Ã‰tape 1.1: Tester la Config Production Localement

```bash
# Tester docker-compose.prod.yml en local (CPU, mais valide la config)
docker compose -f docker/docker-compose.prod.yml up

# VÃ©rifier que tous les services dÃ©marrent
# Note: Ollama sera sur CPU (pas de NVIDIA sur Mac), mais la config est validÃ©e
```

#### Ã‰tape 1.2: Pousser le Code

```bash
git push origin main
```

#### Ã‰tape 1.3: Configurer les Variables d'Environnement Production

CrÃ©er `.env.prod` avec:
```bash
# Ollama
OLLAMA_BASE_URL=http://ollama:11434
OLLAMA_MODEL=qwen2.5:7b-instruct-q8_0

# Neo4j
NEO4J_URI=bolt://neo4j:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=<production-password>

# Gemini
GEMINI_API_KEY=<your-production-key>

# OpenAI
OPENAI_API_KEY=<your-production-key>

# Sentry
SENTRY_DSN_BACKEND=<your-production-dsn>
SENTRY_ENVIRONMENT=production
```

### Phase 2: Sur le Droplet DigitalOcean

#### Ã‰tape 2.1: Provisionner le GPU Droplet

**Specs RecommandÃ©es:**
| Setting | Value | Reason |
|---------|-------|--------|
| **Image** | Ubuntu 22.04 LTS x64 | Stable, Docker support |
| **Plan** | GPU-Optimized Droplets | Needed for Qwen 2.5 7B |
| **GPU Type** | Basic AI/ML (8GB VRAM min) | Qwen 2.5 7B Q8_0 needs ~8.1GB |
| **RAM** | 16GB minimum | Backend + Neo4j + Ollama |
| **CPU** | 4+ vCPUs | Processing documents |
| **Storage** | 100GB SSD | Models + Neo4j data |

**CoÃ»t estimÃ©:** ~$100-150/month

#### Ã‰tape 2.2: VÃ©rifier GPU Disponible

```bash
# Sur le droplet
nvidia-smi

# Doit afficher:
# +-----------------------------------------------------------------------------+
# | NVIDIA-SMI XXX.XX       Driver Version: XXX.XX       CUDA Version: 11.8     |
# |-------------------------------+----------------------+----------------------+
# | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
# ...
```

#### Ã‰tape 2.3: VÃ©rifier Docker GPU Support

```bash
# Tester nvidia-docker2
docker run --rm --gpus all nvidia/cuda:11.8.0-base nvidia-smi

# Doit afficher la mÃªme sortie nvidia-smi que ci-dessus
# Si erreur â†’ installer nvidia-docker2:
# sudo apt-get install -y nvidia-docker2
# sudo systemctl restart docker
```

#### Ã‰tape 2.4: Cloner le Repo

```bash
git clone https://github.com/nicozefrench/diveteacher.git /opt/diveteacher
cd /opt/diveteacher
```

#### Ã‰tape 2.5: Configurer l'Environnement

```bash
# Copier le fichier .env.prod prÃ©parÃ©
cp .env.prod .env

# OU crÃ©er directement sur le serveur
nano .env
# (coller les variables d'environnement production)
```

#### Ã‰tape 2.6: Lancer les Services

```bash
# Lancer en production (mode dÃ©tachÃ©)
docker compose -f docker/docker-compose.prod.yml up -d

# Suivre les logs
docker compose -f docker/docker-compose.prod.yml logs -f
```

#### Ã‰tape 2.7: Attendre le DÃ©marrage d'Ollama

```bash
# Suivre les logs Ollama (~30-60 secondes)
docker logs -f rag-ollama-prod

# Doit afficher:
# time=... level=INFO source=routes.go msg="Listening on [::]:11434"
# time=... level=INFO source=types.go msg="inference compute" id=0 library=cuda ...
```

#### Ã‰tape 2.8: Charger le ModÃ¨le

```bash
# Pull le modÃ¨le dans le container Ollama
docker exec rag-ollama-prod ollama pull qwen2.5:7b-instruct-q8_0

# DurÃ©e: ~5-10 minutes (8.1 GB)
```

#### Ã‰tape 2.9: VÃ‰RIFICATION CRITIQUE - GPU Active

```bash
docker exec rag-ollama-prod ollama ps

# âœ… SUCCÃˆS si affiche:
# NAME                        ID              SIZE      PROCESSOR           UNTIL
# qwen2.5:7b-instruct-q8_0    2d9500c94841    8.9 GB    100% GPU            ...
#                                                        ^^^^^^^^
#                                                        CRITICAL: Doit Ãªtre "100% GPU"
```

**Si `ollama ps` affiche "100% GPU" â†’ âœ… Migration rÃ©ussie!**

**Si `ollama ps` affiche "100% CPU" â†’ âŒ ProblÃ¨me GPU, voir Troubleshooting ci-dessous**

---

## â±ï¸ TEMPS DE MIGRATION ESTIMÃ‰

| Ã‰tape | DurÃ©e |
|-------|-------|
| Setup droplet (si nouveau) | ~5 min |
| Transfert code | ~1 min |
| Configuration .env | ~2 min |
| Premier `docker compose up` | ~3-5 min (pull images) |
| Chargement modÃ¨le Ollama | ~5-10 min (8.1 GB) |
| Tests de validation | ~5 min |
| **TOTAL** | **~20-30 minutes** |

RedÃ©ploiements suivants: **~2-3 minutes** (juste rebuild backend si besoin).

---

## ğŸ”§ TROUBLESHOOTING

### âŒ Si `ollama ps` montre CPU au lieu de GPU

**Cause:** Docker ne peut pas accÃ©der au GPU NVIDIA.

**Fix:**
```bash
# 1. VÃ©rifier NVIDIA runtime
docker run --rm --gpus all nvidia/cuda:11.8.0-base nvidia-smi

# 2. Si erreur â†’ installer nvidia-docker2
sudo apt-get update
sudo apt-get install -y nvidia-docker2
sudo systemctl restart docker

# 3. Re-tester
docker run --rm --gpus all nvidia/cuda:11.8.0-base nvidia-smi

# 4. Si OK, redÃ©marrer Ollama
docker compose -f docker/docker-compose.prod.yml restart ollama
docker exec rag-ollama-prod ollama ps  # Doit montrer 100% GPU
```

### âŒ Si containers ne dÃ©marrent pas

```bash
# Checker les logs
docker compose -f docker/docker-compose.prod.yml logs

# ProblÃ¨me courant: ports dÃ©jÃ  utilisÃ©s
sudo netstat -tulpn | grep :11434
sudo netstat -tulpn | grep :8000
sudo netstat -tulpn | grep :7687

# Si port utilisÃ©, tuer le processus:
sudo kill -9 <PID>
```

### âŒ Si backend ne peut pas joindre Ollama

```bash
# VÃ©rifier networking Docker
docker network ls
docker network inspect diveteacher_default

# Tester depuis le backend
docker exec rag-backend curl http://ollama:11434/api/tags

# Si timeout â†’ vÃ©rifier que ollama est dans le mÃªme rÃ©seau
docker compose -f docker/docker-compose.prod.yml ps
```

### âŒ Si performance < 40 tok/s (GPU sous-utilisÃ©)

```bash
# VÃ©rifier utilisation GPU
nvidia-smi

# Si GPU Ã  100% mais tok/s faible â†’ VRAM insuffisante
# VÃ©rifier mÃ©moire disponible:
nvidia-smi --query-gpu=memory.used,memory.total --format=csv

# Si VRAM < 10GB libre â†’ upgrader GPU Droplet
```

---

## âœ… CHECKLIST DE VALIDATION POST-MIGRATION

### Tests Automatiques

```bash
# 1. Tous les containers running
docker ps --format "table {{.Names}}\t{{.Status}}"
# âœ… Doit montrer: rag-backend-prod, rag-neo4j-prod, rag-ollama-prod (tous "Up")

# 2. GPU utilisÃ© par Ollama
docker exec rag-ollama-prod ollama ps
# âœ… Doit afficher: PROCESSOR: 100% GPU

# 3. GPU visible par nvidia-smi
nvidia-smi
# âœ… Doit montrer Ollama dans la liste des processus GPU

# 4. API Ollama rÃ©pond
curl http://localhost:11434/api/tags
# âœ… Doit retourner JSON avec la liste des modÃ¨les

# 5. Backend peut joindre Ollama
docker exec rag-backend curl http://ollama:11434/api/tags
# âœ… Doit retourner JSON

# 6. Health check backend
curl http://localhost:8000/api/health
# âœ… Doit retourner: {"status": "healthy", "ollama_model": "qwen2.5:7b-instruct-q8_0"}
```

### Test RAG Query

```bash
# Test simple RAG query
curl -X POST http://localhost:8000/api/query \
  -H "Content-Type: application/json" \
  -d '{
    "question": "What is DiveTeacher?",
    "stream": false
  }'

# âœ… Doit retourner une rÃ©ponse JSON avec answer, num_sources, etc.
# âœ… DurÃ©e attendue: <20 secondes (GPU NVIDIA)
```

### Validation Performance

```bash
# Benchmark Ollama
time docker exec rag-ollama-prod ollama run qwen2.5:7b-instruct-q8_0 "Hello" --verbose 2>/dev/null

# âœ… Performance attendue: 40-60 tok/s (NVIDIA GPU)
# âŒ Si < 10 tok/s â†’ GPU pas utilisÃ©, revoir troubleshooting
```

---

## ğŸ“Š CHECKLIST COMPLÃˆTE

### PrÃ©-Migration

- [ ] Test local avec `docker-compose.prod.yml` (valide config)
- [ ] Variables d'environnement `.env.prod` prÃ©parÃ©es
- [ ] GPU Droplet provisionnÃ© (8GB VRAM min)
- [ ] SSH access au droplet configurÃ©
- [ ] `nvidia-smi` fonctionne sur le droplet
- [ ] `docker --gpus all` fonctionne sur le droplet

### Post-DÃ©ploiement

- [ ] Tous les containers running (`docker ps`)
- [ ] `ollama ps` affiche "100% GPU"
- [ ] `nvidia-smi` montre Ollama utilisant le GPU
- [ ] API Ollama rÃ©pond (`curl http://localhost:11434/api/tags`)
- [ ] Backend health check OK (`curl http://localhost:8000/api/health`)
- [ ] RAG query fonctionne (test avec `curl`)
- [ ] Performance 40-60 tok/s (benchmark Ollama)

---

## ğŸ’° COÃ›TS ESTIMÃ‰S

### DigitalOcean GPU Droplet

| Component | Specs | Monthly Cost |
|-----------|-------|--------------|
| GPU Droplet | Basic AI/ML (8GB VRAM) | ~$100-150 |
| Storage | 100GB SSD | Included |
| Bandwidth | 5TB | Included |
| Backups (opt) | +20% | +$20-30 |

**Total:** ~$120-180/month

### Comparaison avec Alternatives

| Provider | GPU | VRAM | Monthly | Notes |
|----------|-----|------|---------|-------|
| **DigitalOcean** | NVIDIA A100 | 8GB | ~$150 | âœ… RecommandÃ© |
| AWS EC2 g4dn.xlarge | NVIDIA T4 | 16GB | ~$180 | Plus cher, plus VRAM |
| GCP n1-standard-4 + T4 | NVIDIA T4 | 16GB | ~$200 | Plus cher |
| Runpod | NVIDIA A100 | 40GB | ~$100 | Moins stable |

---

## ğŸ¯ POURQUOI C'EST SANS RISQUE

1. **API identique** - Ollama API reste la mÃªme (natif Mac ou Docker cloud)
2. **Abstraction propre** - Code dÃ©pend de l'interface (`OLLAMA_BASE_URL`), pas de l'implÃ©mentation
3. **Testable avant prod** - `docker compose -f docker-compose.prod.yml up` valide la config localement
4. **Rollback immÃ©diat** - En cas de problÃ¨me, revenir Ã  l'ancienne config en <5 minutes
5. **Zero downtime possible** - DÃ©ployer sur nouveau droplet, tester, puis switcher DNS

---

## ğŸ“š RÃ‰FÃ‰RENCES

### Documentation

- **Ollama Baremetal Migration:** `Devplan/251105-OLLAMA-BAREMETAL-MIGRATION.md`
- **Docker Compose Dev:** `docker/docker-compose.dev.yml`
- **Docker Compose Prod:** `docker/docker-compose.prod.yml` (Ã  crÃ©er lors de la migration)
- **Note Technique Hybrid:** `resources/251104-note-technique-ollama-gpu-hybrid.md`

### Ressources Externes

- Docker Compose Overrides: https://docs.docker.com/compose/multiple-compose-files/
- Ollama API: https://github.com/ollama/ollama/blob/main/docs/api.md
- Docker GPU Support (NVIDIA): https://docs.docker.com/config/containers/resource_constraints/#gpu
- DigitalOcean GPU Droplets: https://docs.digitalocean.com/products/droplets/how-to/gpu/

---

## ğŸ“ CONTEXTE POUR AI ASSISTANT

### Situation Actuelle (November 2025)

- **Dev Local:** Ollama natif sur Mac M1 Max (Metal GPU)
- **Architecture:** Hybride (Ollama natif + services Docker)
- **Performance:** 7-14 tok/s (Metal GPU)
- **Raison:** Docker Desktop sur Mac ne supporte pas Metal GPU

### Migration Future (Quand PrÃªt pour Production)

- **Production Cloud:** Ollama dans Docker avec NVIDIA GPU sur DigitalOcean
- **Architecture:** Full Docker (Ollama + Backend + Frontend + Neo4j)
- **Performance attendue:** 40-60 tok/s (NVIDIA GPU)
- **Changements code:** ZÃ‰RO (uniquement `OLLAMA_BASE_URL` env var)

### Principe ClÃ©

**DÃ©pendre d'interfaces (API Ollama), pas d'implÃ©mentations (natif vs Docker).**

Ce pattern est standard dans l'industrie ML/GPU pour dÃ©veloppement cross-platform (Mac â†’ Linux cloud).

---

**ğŸ“ Note:** Ce guide doit Ãªtre conservÃ© et mis Ã  jour lors de la migration cloud effective. Il garantit une migration sans risque et sans modification de code.

**Status:** âœ… **READY - Documentation complÃ¨te pour migration future**

---

*Guide crÃ©Ã©: November 5, 2025*  
*BasÃ© sur: Devplan/251105-OLLAMA-BAREMETAL-MIGRATION.md + resources/251104-note-technique-ollama-gpu-hybrid.md*

