# RAG Knowledge Graph Boilerplate

**A production-ready foundation for building intelligent document Q&A applications**

[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE)
[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![React 18](https://img.shields.io/badge/react-18-blue.svg)](https://react.dev/)

---

## ğŸ¯ What is this?

A **complete, reusable boilerplate** for building RAG (Retrieval-Augmented Generation) applications with:
- **Document Processing:** Upload PDF/PPT â†’ Auto-process â†’ Knowledge Graph
- **Knowledge Graph:** Neo4j with Graphiti for intelligent entity/relationship extraction
- **LLM Agnostic:** Ollama (local), Claude, or OpenAI - your choice
- **Modern UI:** React + TailwindCSS + shadcn/ui with streaming responses
- **Production Ready:** Docker, Vercel, DigitalOcean deployment included

**Perfect for:** Private document Q&A, research assistants, legal/compliance tools, technical documentation systems

---

## âœ¨ Key Features

| Feature | Description |
|---------|-------------|
| ğŸ“„ **Document Upload** | Drag & drop PDF/PPT files with real-time progress |
| ğŸ§  **Knowledge Graph** | Neo4j + Graphiti for intelligent entity extraction |
| ğŸ¤– **LLM Agnostic** | Ollama (local), Claude, OpenAI - switch via env var |
| ğŸ’¬ **Streaming Chat** | Real-time token streaming for better UX |
| ğŸ¨ **Modern UI** | React + TailwindCSS + shadcn/ui components |
| ğŸ³ **Docker Ready** | One command to start (dev + prod configs) |
| ğŸš€ **Deploy Ready** | Vercel (frontend) + DigitalOcean (backend) |
| ğŸ“Š **Monitoring** | Sentry integration for error tracking |
| ğŸ” **Secure** | All secrets via environment variables |

---

## ğŸš€ Quick Start (5 minutes)

### Prerequisites
- Docker & Docker Compose installed
- Git
- (Optional) Vercel account for deployment
- (Optional) DigitalOcean account for backend hosting

### 1. Clone & Configure

```bash
# Clone repository
git clone https://github.com/your-username/rag-kg-boilerplate.git
cd rag-kg-boilerplate

# Copy environment template
cp .env.template .env

# Edit .env with your settings
nano .env
```

### 2. Start All Services

```bash
# Start backend + Neo4j + Ollama
docker-compose -f docker/docker-compose.dev.yml up -d

# Wait ~30 seconds for services to start
# Check status
docker-compose -f docker/docker-compose.dev.yml ps
```

### 3. Pull LLM Model (First Time Only)

```bash
# Pull Ollama model (llama3:8b recommended)
docker exec rag-ollama ollama pull llama3:8b

# Verify
docker exec rag-ollama ollama list
```

### 4. Access Application

| Service | URL | Purpose |
|---------|-----|---------|
| **Frontend** | http://localhost:5173 | React app |
| **Backend API** | http://localhost:8000 | FastAPI docs |
| **Neo4j Browser** | http://localhost:7474 | Graph visualization |
| **Ollama** | http://localhost:11434 | LLM server |

### 5. Test the System

1. **Upload a document:** Go to http://localhost:5173, drag & drop a PDF
2. **Wait for processing:** Status will update automatically
3. **Ask a question:** Type in chat interface, get streaming response

---

## ğŸ“‚ Project Structure

```
rag-kg-boilerplate/
â”œâ”€â”€ backend/                    # FastAPI backend
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ main.py            # FastAPI application
â”‚   â”‚   â”œâ”€â”€ api/               # API routes
â”‚   â”‚   â”‚   â”œâ”€â”€ upload.py      # File upload endpoint
â”‚   â”‚   â”‚   â”œâ”€â”€ query.py       # RAG query endpoint (streaming)
â”‚   â”‚   â”‚   â””â”€â”€ health.py      # Health checks
â”‚   â”‚   â”œâ”€â”€ core/              # Core business logic
â”‚   â”‚   â”‚   â”œâ”€â”€ llm.py         # LLM abstraction layer
â”‚   â”‚   â”‚   â”œâ”€â”€ rag.py         # RAG chain logic
â”‚   â”‚   â”‚   â””â”€â”€ processor.py   # Document processing
â”‚   â”‚   â”œâ”€â”€ integrations/      # External services
â”‚   â”‚   â”‚   â”œâ”€â”€ neo4j.py       # Neo4j client
â”‚   â”‚   â”‚   â”œâ”€â”€ graphiti.py    # Graphiti integration
â”‚   â”‚   â”‚   â”œâ”€â”€ dockling.py    # PDF/PPT processing
â”‚   â”‚   â”‚   â””â”€â”€ sentry.py      # Error tracking
â”‚   â”‚   â””â”€â”€ models/            # Pydantic models
â”‚   â”œâ”€â”€ requirements.txt       # Python dependencies
â”‚   â”œâ”€â”€ Dockerfile             # Backend container
â”‚   â””â”€â”€ .env.template          # Environment variables
â”‚
â”œâ”€â”€ frontend/                   # React frontend
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/        # React components
â”‚   â”‚   â”‚   â”œâ”€â”€ Chat.jsx       # Chat interface
â”‚   â”‚   â”‚   â”œâ”€â”€ FileUpload.jsx # Drag & drop upload
â”‚   â”‚   â”‚   â”œâ”€â”€ StreamingMessage.jsx
â”‚   â”‚   â”‚   â””â”€â”€ ui/            # shadcn/ui components
â”‚   â”‚   â”œâ”€â”€ hooks/
â”‚   â”‚   â”‚   â”œâ”€â”€ useStreamingQuery.js  # SSE hook
â”‚   â”‚   â”‚   â””â”€â”€ useFileUpload.js
â”‚   â”‚   â”œâ”€â”€ lib/
â”‚   â”‚   â”‚   â””â”€â”€ api.js         # API client
â”‚   â”‚   â””â”€â”€ App.jsx            # Main app
â”‚   â”œâ”€â”€ package.json
â”‚   â”œâ”€â”€ vite.config.js
â”‚   â”œâ”€â”€ tailwind.config.js
â”‚   â”œâ”€â”€ vercel.json            # Vercel deployment config
â”‚   â””â”€â”€ Dockerfile.dev         # Frontend container (dev)
â”‚
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ docker-compose.dev.yml      # Local development
â”‚   â”œâ”€â”€ docker-compose.prod.yml     # Production (DigitalOcean)
â”‚   â””â”€â”€ nginx/                      # Reverse proxy config
â”‚
â”œâ”€â”€ deploy/
â”‚   â”œâ”€â”€ digitalocean-setup.sh       # DO droplet setup
â”‚   â”œâ”€â”€ vercel-deploy.sh            # Vercel deployment
â”‚   â””â”€â”€ pull-ollama-models.sh       # LLM model setup
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ SETUP.md                    # Detailed setup guide
â”‚   â”œâ”€â”€ DEPLOYMENT.md               # Production deployment
â”‚   â”œâ”€â”€ ARCHITECTURE.md             # System design
â”‚   â”œâ”€â”€ API.md                      # API reference
â”‚   â””â”€â”€ TROUBLESHOOTING.md          # Common issues
â”‚
â”œâ”€â”€ examples/
â”‚   â””â”€â”€ sample-docs/                # Example PDFs for testing
â”‚
â”œâ”€â”€ .env.template                   # Environment variables template
â”œâ”€â”€ .gitignore
â”œâ”€â”€ LICENSE
â”œâ”€â”€ README.md                       # This file
â”œâ”€â”€ GOAL.md                         # Project objectives
â””â”€â”€ AI-AGENT-GUIDE.md              # Guide for AI assistants
```

---

## ğŸ”§ Configuration

### Environment Variables

All configuration is via `.env` file. See `.env.template` for all options.

**Key variables:**

```bash
# LLM Provider (choose one)
LLM_PROVIDER=ollama  # or 'claude' or 'openai'
OLLAMA_MODEL=llama3:8b

# Neo4j Database
NEO4J_PASSWORD=your_secure_password

# Monitoring (Optional)
SENTRY_DSN_BACKEND=https://...
SENTRY_DSN_FRONTEND=https://...

# Frontend (Vercel deployment)
VITE_API_URL=https://api.your-domain.com
```

### Switching LLM Providers

```bash
# Use Ollama (local, free)
LLM_PROVIDER=ollama
OLLAMA_MODEL=llama3:8b

# Use Claude (API, paid)
LLM_PROVIDER=claude
ANTHROPIC_API_KEY=sk-ant-...

# Use OpenAI (API, paid)
LLM_PROVIDER=openai
OPENAI_API_KEY=sk-...
```

---

## ğŸŒ Deployment

### Frontend: Vercel (Free Tier)

```bash
cd frontend

# Option 1: GitHub integration (recommended)
# Push to GitHub â†’ Vercel auto-deploys

# Option 2: CLI
npm install -g vercel
vercel --prod

# Set environment variables in Vercel dashboard:
# VITE_API_URL=https://api.your-domain.com
# VITE_SENTRY_DSN=...
```

### Backend: DigitalOcean ($12-40/month)

```bash
# 1. Create Droplet (Ubuntu 22.04)
# 2. SSH into droplet
ssh root@your-droplet-ip

# 3. Run setup script
curl -fsSL https://raw.githubusercontent.com/your-repo/deploy/digitalocean-setup.sh | bash

# 4. Configure .env
cd /opt/rag-app
nano .env

# 5. Start services
docker-compose -f docker-compose.prod.yml up -d

# 6. Pull Ollama model
docker exec rag-ollama ollama pull llama3:8b
```

**Detailed guides:** See `docs/DEPLOYMENT.md`

---

## ğŸ“Š Monitoring

### Sentry Integration

**Backend errors:** Automatically captured and sent to Sentry
**Frontend errors:** React error boundaries + Sentry SDK
**Performance:** Transaction tracing for API calls

**Setup:**
1. Create Sentry project (free tier available)
2. Copy DSN to `.env`
3. Errors automatically tracked

---

## ğŸ§ª Testing

```bash
# Backend tests
cd backend
python -m pytest tests/

# Frontend tests
cd frontend
npm test

# Integration tests
docker-compose -f docker/docker-compose.test.yml up --abort-on-container-exit
```

---

## ğŸ¤ Contributing

This is a boilerplate template - fork and customize for your needs!

**To contribute back:**
1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Submit a pull request

---

## ğŸ“š Documentation

| Document | Description |
|----------|-------------|
| [GOAL.md](GOAL.md) | Project objectives and features |
| [AI-AGENT-GUIDE.md](AI-AGENT-GUIDE.md) | Guide for AI assistants (Claude, etc.) |
| [docs/SETUP.md](docs/SETUP.md) | Detailed installation guide |
| [docs/DEPLOYMENT.md](docs/DEPLOYMENT.md) | Production deployment |
| [docs/ARCHITECTURE.md](docs/ARCHITECTURE.md) | System design deep-dive |
| [docs/API.md](docs/API.md) | Backend API reference |
| [docs/TROUBLESHOOTING.md](docs/TROUBLESHOOTING.md) | Common issues + solutions |

---

## ğŸ’¡ Use Cases

### Research Assistant
Upload academic papers â†’ Ask questions â†’ Get cited answers

### Legal Compliance Tool
Upload contracts/policies â†’ Query compliance requirements â†’ Audit-ready responses

### Technical Documentation
Upload developer docs â†’ Natural language queries â†’ Code examples with context

### Business Intelligence
Upload reports/presentations â†’ Executive summaries â†’ Data-driven insights

---

## ğŸ”® Roadmap

- [x] Core RAG pipeline (MVP)
- [x] Knowledge graph integration
- [x] LLM abstraction layer
- [x] Docker deployment
- [x] Vercel + DigitalOcean setup
- [x] Sentry monitoring
- [ ] Multi-user authentication
- [ ] Document collections
- [ ] Knowledge graph visualization
- [ ] Batch document processing
- [ ] Advanced search filters
- [ ] Export conversations

---

## ğŸ“ License

MIT License - See [LICENSE](LICENSE) file

---

## ğŸ™ Acknowledgments

Built with:
- [FastAPI](https://fastapi.tiangolo.com/) - Modern Python web framework
- [React](https://react.dev/) - UI library
- [TailwindCSS](https://tailwindcss.com/) - Utility-first CSS
- [shadcn/ui](https://ui.shadcn.com/) - Beautiful components
- [Neo4j](https://neo4j.com/) - Graph database
- [Graphiti](https://github.com/getzep/graphiti) - Knowledge graph extraction
- [Ollama](https://ollama.ai/) - Local LLM runtime
- [Dockling](https://github.com/DS4SD/dockling) - Document processing

---

## ğŸ†˜ Support

**Issues:** [GitHub Issues](https://github.com/your-username/rag-kg-boilerplate/issues)  
**Questions:** See [docs/TROUBLESHOOTING.md](docs/TROUBLESHOOTING.md)

---

## â­ Star History

If this boilerplate helps you, please consider giving it a star! â­

---

**Status:** ğŸš€ **Ready for production use**  
**Maintained by:** [Your Name](https://github.com/your-username)  
**Last Updated:** October 26, 2025

