# Note Technique - Solution GPU Ollama (Dev Local Mac â†’ Cloud GPU)

**Date:** 2025-11-04  
**Contexte:** DÃ©veloppement systÃ¨me RAG avancÃ© avec migration prÃ©vue vers DigitalOcean GPU Droplet  
**Assistant:** Claude Sonnet 4.5

---

## ğŸš¨ PROBLÃˆME

**Docker Desktop sur Mac ne supporte PAS le GPU Metal.**

```bash
docker exec rag-ollama ollama ps
# RÃ©sultat: PROCESSOR: 100% CPU âŒ
# Attendu:  PROCESSOR: 100% GPU âœ…
```

**ConsÃ©quence:** Tests locaux 10-20x plus lents, impossible de valider les perfs rÃ©elles.

---

## âœ… SOLUTION: Configuration Hybride

### Principe
| Environnement | Ollama | Services RAG | Raison |
|---------------|--------|--------------|---------|
| **Dev Local** | Natif (hors Docker) | Docker | Performance Metal GPU |
| **Production** | Docker avec GPU | Docker | CohÃ©rence infrastructure |

### Architecture de Communication

```
DEV LOCAL:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Ollama Natif    â”‚ â† InstallÃ© via brew, tourne sur Mac
â”‚ :11434 (Metal)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ HTTP API
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Docker Services                 â”‚
â”‚ â”œâ”€ Backend â†’ http://host.docker.internal:11434
â”‚ â”œâ”€ Frontend                     â”‚
â”‚ â””â”€ Neo4j/ChromaDB/etc           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

PRODUCTION (DigitalOcean):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Docker Stack                    â”‚
â”‚ â”œâ”€ Ollama â†’ http://ollama:11434 (NVIDIA GPU)
â”‚ â”œâ”€ Backend â†’ http://ollama:11434
â”‚ â”œâ”€ Frontend                     â”‚
â”‚ â””â”€ Neo4j/ChromaDB/etc           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”‘ POINTS CLÃ‰S - Pourquoi Cela Fonctionne

### 1. API Ollama Identique
L'API REST d'Ollama est **strictement identique** qu'elle tourne:
- En natif sur Mac
- Dans Docker CPU
- Dans Docker GPU

**MÃªme endpoints, mÃªme format JSON, mÃªme comportement.**

### 2. Abstraction par URL
Votre backend ne doit connaÃ®tre qu'**une seule variable d'environnement:**
```bash
OLLAMA_BASE_URL=<url_ollama>
```

Cette URL change selon l'environnement:
- Dev: `http://host.docker.internal:11434` (pointe vers Mac host)
- Prod: `http://ollama:11434` (pointe vers container Docker)

**ZÃ©ro modification de code nÃ©cessaire.**

### 3. Docker Compose Overrides
Utilisation du pattern standard Docker Compose avec fichiers multiples:
- `docker-compose.yml` â†’ Configuration commune (backend, frontend, DBs)
- `docker-compose.dev.yml` â†’ Overrides dev (pas de service Ollama)
- `docker-compose.prod.yml` â†’ Overrides prod (service Ollama avec GPU)

**Une seule commande diffÃ©rente:**
```bash
# Dev
docker-compose -f docker-compose.yml -f docker-compose.dev.yml up

# Prod
docker-compose -f docker-compose.yml -f docker-compose.prod.yml up
```

---

## ğŸ› ï¸ MISE EN Å’UVRE

### Ã‰tape 1: Installer Ollama Nativement (Mac)

```bash
brew install ollama
ollama serve  # Lance le serveur sur :11434
ollama pull qwen2.5:7b-instruct-q8_0  # Ou votre modÃ¨le
ollama ps  # Doit montrer "100% GPU" (Metal)
```

### Ã‰tape 2: Adapter Docker Compose

**Structure fichiers:**
```
projet/
â”œâ”€â”€ docker-compose.yml           # Config commune
â”œâ”€â”€ docker-compose.dev.yml       # Overrides local (pas d'Ollama)
â”œâ”€â”€ docker-compose.prod.yml      # Overrides cloud (Ollama + GPU)
â”œâ”€â”€ .env.dev                     # Vars local
â””â”€â”€ .env.prod                    # Vars cloud
```

**docker-compose.dev.yml** (extrait):
```yaml
services:
  backend:  # Ou le service qui appelle Ollama
    extra_hosts:
      - "host.docker.internal:host-gateway"
    environment:
      - OLLAMA_BASE_URL=http://host.docker.internal:11434
```

**docker-compose.prod.yml** (extrait):
```yaml
services:
  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
  
  backend:
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
    depends_on:
      - ollama
```

### Ã‰tape 3: Variables d'Environnement

**.env.dev:**
```bash
OLLAMA_BASE_URL=http://host.docker.internal:11434
OLLAMA_MODEL=qwen2.5:7b-instruct-q8_0
```

**.env.prod:**
```bash
OLLAMA_BASE_URL=http://ollama:11434
OLLAMA_MODEL=qwen2.5:7b-instruct-q8_0
```

**Important:** Votre code doit lire `OLLAMA_BASE_URL` depuis l'environnement.

---

## ğŸ“Š WORKFLOW QUOTIDIEN

### DÃ©veloppement Local
```bash
# Terminal 1: Ollama natif
ollama serve

# Terminal 2: Services Docker
docker-compose -f docker-compose.yml -f docker-compose.dev.yml --env-file .env.dev up
```

### Test Pre-Production (optionnel)
```bash
# Teste la config prod en local (CPU mais valide la config)
docker-compose -f docker-compose.yml -f docker-compose.prod.yml up
```

### DÃ©ploiement Production
```bash
# Sur votre droplet DigitalOcean
docker-compose -f docker-compose.yml -f docker-compose.prod.yml --env-file .env.prod up -d
```

---

## âš ï¸ POINTS D'ATTENTION

### Ce Qui Ne Change Pas
- âœ… Votre code backend/frontend (zÃ©ro ligne Ã  modifier)
- âœ… Les appels API Ã  Ollama (mÃªme format)
- âœ… Les modÃ¨les utilisÃ©s (mÃªme compatibilitÃ©)
- âœ… La logique RAG (identique)

### Ce Qui Change
- ğŸ”„ **Uniquement** la variable `OLLAMA_BASE_URL`
- ğŸ”„ **Uniquement** le fichier docker-compose utilisÃ©

### PrÃ©requis Cloud
Avant migration DigitalOcean, vÃ©rifier:
1. **NVIDIA drivers** installÃ©s: `nvidia-smi`
2. **NVIDIA Docker runtime** configurÃ©: `docker run --gpus all nvidia/cuda:11.8.0-base nvidia-smi`
3. **Suffisamment de VRAM** pour votre modÃ¨le (ex: 7B â†’ ~8GB VRAM)

Sur les **GPU Droplets DigitalOcean**, tout est prÃ©-configurÃ©.

---

## ğŸš€ CE QUI SE PASSE LORS DE LA MIGRATION CLOUD

### Changements Effectifs

**1. Ollama passe de Natif â†’ Docker avec GPU**
```bash
# Local (avant)
ollama serve  # Natif sur Mac, port :11434

# Cloud (aprÃ¨s)
docker-compose up  # Ollama dans container, port :11434
```

**2. Une seule variable change**
```bash
# .env.dev (local)
OLLAMA_BASE_URL=http://host.docker.internal:11434

# .env.prod (cloud)
OLLAMA_BASE_URL=http://ollama:11434
```

**3. Commande de lancement diffÃ©rente**
```bash
# Local
docker-compose -f docker-compose.yml -f docker-compose.dev.yml up

# Cloud
docker-compose -f docker-compose.yml -f docker-compose.prod.yml up -d
```

### Ce Qui NE Change PAS

| Composant | Status |
|-----------|--------|
| Code backend/frontend | âœ… Identique (0 ligne modifiÃ©e) |
| API calls Ã  Ollama | âœ… Identique (mÃªme format JSON) |
| ModÃ¨les utilisÃ©s | âœ… Identiques (mÃªme compatibilitÃ©) |
| Base de donnÃ©es | âœ… Identique (mÃªme config) |
| Logique RAG | âœ… Identique |
| Performance | âœ… GPU dans les 2 cas (Metal local, NVIDIA cloud) |

### DÃ©roulement Migration (Step-by-Step)

**Sur votre machine locale:**
```bash
# 1. Pousser le code
git push origin main
```

**Sur le Droplet DigitalOcean:**
```bash
# 2. Cloner le repo
git clone <votre-repo> /opt/rag-app
cd /opt/rag-app

# 3. Configurer l'environnement
cp .env.prod .env
# Ã‰diter .env avec les vrais secrets/passwords

# 4. VÃ©rifier GPU disponible
nvidia-smi  # Doit afficher votre GPU NVIDIA

# 5. Lancer en production
docker-compose -f docker-compose.yml -f docker-compose.prod.yml up -d

# 6. Attendre que Ollama dÃ©marre (~30 sec)
docker logs -f rag-ollama

# 7. Charger le modÃ¨le
docker exec rag-ollama ollama pull qwen2.5:7b-instruct-q8_0

# 8. VÃ‰RIFIER GPU
docker exec rag-ollama ollama ps
# âœ… Doit afficher: PROCESSOR: 100% GPU
```

**Si `ollama ps` affiche "100% GPU" â†’ Migration rÃ©ussie âœ…**

### Temps de Migration EstimÃ©

| Ã‰tape | DurÃ©e |
|-------|-------|
| Setup droplet (si nouveau) | ~5 min |
| Transfert code | ~1 min |
| Configuration .env | ~2 min |
| Premier `docker-compose up` | ~3-5 min (pull images) |
| Chargement modÃ¨le Ollama | ~2-10 min (selon taille) |
| **TOTAL** | **~15-25 minutes** |

Ensuite, les redÃ©ploiements suivants: **~2-3 minutes** (juste rebuild).

### Troubleshooting Migration

**âŒ Si `ollama ps` montre CPU au lieu de GPU:**
```bash
# VÃ©rifier NVIDIA runtime
docker run --rm --gpus all nvidia/cuda:11.8.0-base nvidia-smi

# Si erreur â†’ installer nvidia-docker2
sudo apt-get install -y nvidia-docker2
sudo systemctl restart docker
```

**âŒ Si containers ne dÃ©marrent pas:**
```bash
# Checker les logs
docker-compose logs

# ProblÃ¨me courant: ports dÃ©jÃ  utilisÃ©s
sudo netstat -tulpn | grep :11434
```

**âŒ Si backend ne peut pas joindre Ollama:**
```bash
# VÃ©rifier networking Docker
docker network ls
docker network inspect <network_name>

# Tester depuis le backend
docker exec rag-backend curl http://ollama:11434/api/tags
```

---

## ğŸ¯ VALIDATION MIGRATION

### Checklist PrÃ©-Migration
- [ ] Test local avec docker-compose.prod.yml (CPU, vÃ©rifie config)
- [ ] Variables d'environnement prod configurÃ©es (passwords, secrets)
- [ ] GPU Droplet provisionnÃ© et accessible SSH
- [ ] `nvidia-smi` fonctionne sur le droplet
- [ ] `docker --gpus` fonctionne sur le droplet

### Checklist Post-DÃ©ploiement
```bash
# Sur le droplet, aprÃ¨s dÃ©ploiement
docker ps  # Tous les containers running?
docker exec rag-ollama ollama ps  # Affiche "100% GPU"?
nvidia-smi  # GPU utilisÃ© par Ollama?
curl http://localhost:11434/api/tags  # API rÃ©pond?
```

Si `ollama ps` montre **"100% GPU"** â†’ âœ… Migration rÃ©ussie!

---

## ğŸ’¡ POURQUOI C'EST SANS RISQUE

1. **API identique** - Ollama API reste la mÃªme (natif ou Docker)
2. **Abstraction propre** - Code dÃ©pend de l'interface, pas de l'implÃ©mentation
3. **Testable avant prod** - `docker-compose -f docker-compose.prod.yml up` valide la config
4. **Rollback immÃ©diat** - En cas de problÃ¨me, juste changer l'env var

---

## ğŸ“š RESSOURCES COMPLÃ‰MENTAIRES

### Docker Compose Overrides
- https://docs.docker.com/compose/multiple-compose-files/

### Ollama API Documentation
- https://github.com/ollama/ollama/blob/main/docs/api.md

### Docker GPU Support (NVIDIA)
- https://docs.docker.com/config/containers/resource_constraints/#gpu

### DigitalOcean GPU Droplets
- https://docs.digitalocean.com/products/droplets/how-to/gpu/

---

## ğŸ“ CONTEXTE POUR SONNET 4.5

- **ProblÃ¨me:** Ollama en Docker sur Mac M1 Max = CPU only (limitation Docker Desktop)
- **Solution:** Ollama natif local (GPU Metal) + Docker prod (GPU NVIDIA)
- **Principe:** DÃ©pendre d'interfaces (API Ollama), pas d'implÃ©mentations (natif/Docker)
- **Migration:** Change uniquement `OLLAMA_BASE_URL` entre dev et prod
- **Risque:** Aucun - Pattern standard industrie ML/GPU cross-platform

---

**ğŸ“ Note:** Cette approche est recommandÃ©e et Ã©prouvÃ©e pour le dÃ©veloppement ML/GPU sur Mac avec dÃ©ploiement cloud Linux.