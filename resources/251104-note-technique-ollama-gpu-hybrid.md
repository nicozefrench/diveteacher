# Note Technique - RÃ©solution ProblÃ¨me GPU Ollama (Dev Local Mac â†’ Cloud)

**Date:** 2025-11-04  
**Contexte:** DÃ©veloppement systÃ¨me RAG avancÃ© avec migration prÃ©vue vers DigitalOcean GPU Droplet  
**Assistant:** Claude Sonnet 4.5

---

## ğŸš¨ PROBLÃˆME IDENTIFIÃ‰

### Situation Actuelle
- **Environnement:** Mac M1 Max avec GPU Metal intÃ©grÃ©
- **Configuration:** Ollama dans Docker via `diveteacher-ollama:latest`
- **RÃ©sultat:** `ollama ps` montre **"100% CPU"** au lieu de **"100% GPU"**

### Diagnostic
```bash
docker exec rag-ollama ollama ps
# OUTPUT: PROCESSOR: 100% CPU âŒ
# ATTENDU: PROCESSOR: 100% GPU âœ…
```

### Cause Racine
**Docker Desktop sur Mac ne supporte PAS le GPU passthrough pour Metal.**

C'est une **limitation connue et non-contournable** de Docker Desktop sur macOS:
- Les containers Docker ne peuvent pas accÃ©der au GPU Metal
- Aucune configuration Docker ne peut rÃ©soudre ce problÃ¨me
- MÃªme avec Rosetta, cela reste du CPU

**Impact:**
- Tests locaux **10-20x plus lents** qu'avec GPU
- ImpossibilitÃ© de valider les performances rÃ©elles avant dÃ©ploiement
- Cycles de dÃ©veloppement rallentis

---

## âœ… SOLUTION RETENUE: Approche Hybride

### Principe
| Environnement | Ollama | Services RAG | Raison |
|---------------|--------|--------------|---------|
| **Dev Local** | Natif (hors Docker) | Docker | Performance Metal GPU |
| **Production** | Docker avec GPU | Docker | CohÃ©rence infrastructure |

### Architecture de Communication

```
DEV LOCAL:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Ollama Natif    â”‚ â† InstallÃ© via brew, tourne sur Mac
â”‚ :11434 (Metal)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ HTTP API
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Docker Services                 â”‚
â”‚ â”œâ”€ Backend â†’ http://host.docker.internal:11434
â”‚ â”œâ”€ Frontend                     â”‚
â”‚ â””â”€ Neo4j/ChromaDB/etc           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

PRODUCTION (DigitalOcean):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Docker Stack                    â”‚
â”‚ â”œâ”€ Ollama â†’ http://ollama:11434 (NVIDIA GPU)
â”‚ â”œâ”€ Backend â†’ http://ollama:11434
â”‚ â”œâ”€ Frontend                     â”‚
â”‚ â””â”€ Neo4j/ChromaDB/etc           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”‘ POINTS CLÃ‰S - Pourquoi Cela Fonctionne

### 1. API Ollama Identique
L'API REST d'Ollama est **strictement identique** qu'elle tourne:
- En natif sur Mac
- Dans Docker CPU
- Dans Docker GPU

**MÃªme endpoints, mÃªme format JSON, mÃªme comportement.**

### 2. Abstraction par URL
Votre backend ne doit connaÃ®tre qu'**une seule variable d'environnement:**
```bash
OLLAMA_BASE_URL=<url_ollama>
```

Cette URL change selon l'environnement:
- Dev: `http://host.docker.internal:11434` (pointe vers Mac host)
- Prod: `http://ollama:11434` (pointe vers container Docker)

**ZÃ©ro modification de code nÃ©cessaire.**

### 3. Docker Compose Overrides
Utilisation du pattern standard Docker Compose avec fichiers multiples:
- `docker-compose.yml` â†’ Configuration commune (backend, frontend, DBs)
- `docker-compose.dev.yml` â†’ Overrides dev (pas de service Ollama)
- `docker-compose.prod.yml` â†’ Overrides prod (service Ollama avec GPU)

**Une seule commande diffÃ©rente:**
```bash
# Dev
docker-compose -f docker-compose.yml -f docker-compose.dev.yml up

# Prod
docker-compose -f docker-compose.yml -f docker-compose.prod.yml up
```

---

## ğŸ› ï¸ MISE EN Å’UVRE (Guidelines GÃ©nÃ©riques)

### Phase 1: Installation Locale

1. **Installer Ollama nativement**
   ```bash
   brew install ollama
   ollama serve  # Lance le serveur sur :11434
   ollama pull <votre_modele>
   ```

2. **VÃ©rifier GPU**
   ```bash
   ollama ps  # Doit montrer "100% GPU" avec Metal
   ```

### Phase 2: Adapter Docker Compose

1. **Extraire service Ollama dans un override**
   - CrÃ©er `docker-compose.dev.yml` SANS service Ollama
   - CrÃ©er `docker-compose.prod.yml` AVEC service Ollama + config GPU

2. **Configurer l'accÃ¨s host depuis Docker (dev)**
   ```yaml
   # docker-compose.dev.yml
   services:
     backend:  # ou votre service qui appelle Ollama
       extra_hosts:
         - "host.docker.internal:host-gateway"
       environment:
         - OLLAMA_BASE_URL=http://host.docker.internal:11434
   ```

3. **Configurer accÃ¨s container (prod)**
   ```yaml
   # docker-compose.prod.yml
   services:
     ollama:
       image: ollama/ollama:latest
       deploy:
         resources:
           reservations:
             devices:
               - driver: nvidia
                 count: all
                 capabilities: [gpu]
     
     backend:
       environment:
         - OLLAMA_BASE_URL=http://ollama:11434
       depends_on:
         - ollama
   ```

### Phase 3: Variables d'Environnement

CrÃ©er deux fichiers `.env`:

```bash
# .env.dev (local)
OLLAMA_BASE_URL=http://host.docker.internal:11434

# .env.prod (cloud)
OLLAMA_BASE_URL=http://ollama:11434
```

**Important:** Tous vos services doivent lire `OLLAMA_BASE_URL` depuis l'environnement, jamais en dur.

---

## ğŸ“Š WORKFLOW QUOTIDIEN

### DÃ©veloppement Local
```bash
# Terminal 1: Ollama natif
ollama serve

# Terminal 2: Services Docker
docker-compose -f docker-compose.yml -f docker-compose.dev.yml --env-file .env.dev up
```

### Test Pre-Production (optionnel)
```bash
# Teste la config prod en local (CPU mais valide la config)
docker-compose -f docker-compose.yml -f docker-compose.prod.yml up
```

### DÃ©ploiement Production
```bash
# Sur votre droplet DigitalOcean
docker-compose -f docker-compose.yml -f docker-compose.prod.yml --env-file .env.prod up -d
```

---

## âš ï¸ POINTS D'ATTENTION

### Ce Qui Ne Change Pas
- âœ… Votre code backend/frontend (zÃ©ro ligne Ã  modifier)
- âœ… Les appels API Ã  Ollama (mÃªme format)
- âœ… Les modÃ¨les utilisÃ©s (mÃªme compatibilitÃ©)
- âœ… La logique RAG (identique)

### Ce Qui Change
- ğŸ”„ **Uniquement** la variable `OLLAMA_BASE_URL`
- ğŸ”„ **Uniquement** le fichier docker-compose utilisÃ©

### PrÃ©requis Cloud
Avant migration DigitalOcean, vÃ©rifier:
1. **NVIDIA drivers** installÃ©s: `nvidia-smi`
2. **NVIDIA Docker runtime** configurÃ©: `docker run --gpus all nvidia/cuda:11.8.0-base nvidia-smi`
3. **Suffisamment de VRAM** pour votre modÃ¨le (ex: 7B â†’ ~8GB VRAM)

Sur les **GPU Droplets DigitalOcean**, tout est prÃ©-configurÃ©.

---

## ğŸ¯ VALIDATION MIGRATION

### Checklist PrÃ©-Migration
- [ ] Test local avec docker-compose.prod.yml (CPU, vÃ©rifie config)
- [ ] Variables d'environnement prod configurÃ©es (passwords, secrets)
- [ ] GPU Droplet provisionnÃ© et accessible SSH
- [ ] `nvidia-smi` fonctionne sur le droplet
- [ ] `docker --gpus` fonctionne sur le droplet

### Checklist Post-DÃ©ploiement
```bash
# Sur le droplet, aprÃ¨s dÃ©ploiement
docker ps  # Tous les containers running?
docker exec rag-ollama ollama ps  # Affiche "100% GPU"?
nvidia-smi  # GPU utilisÃ© par Ollama?
curl http://localhost:11434/api/tags  # API rÃ©pond?
```

Si `ollama ps` montre **"100% GPU"** â†’ âœ… Migration rÃ©ussie!

---

## ğŸ’¡ POURQUOI CETTE APPROCHE EST SÃ›RE

### 1. Pattern Standard
C'est une pratique **commune** en dÃ©veloppement cloud:
- Bases de donnÃ©es en Docker, cache Redis en local pour certains
- Services lourds (ML, GPU) souvent natifs en dev pour performance
- Abstractions par variables d'environnement = best practice

### 2. MÃªme Interface, ImplÃ©mentation DiffÃ©rente
Architecture **hexagonale** en pratique:
- Votre code dÃ©pend de l'**interface** Ollama API
- Pas de l'**implÃ©mentation** (natif vs Docker)

### 3. Testable
Vous pouvez tester la config prod **avant dÃ©ploiement**:
```bash
docker-compose -f docker-compose.prod.yml up
```
Tourne sur CPU mais valide networking, variables, dÃ©pendances.

### 4. Rollback Facile
Si problÃ¨me en prod, rollback = **zÃ©ro code change**, juste env vars.

---

## ğŸš€ ALTERNATIVES NON-RECOMMANDÃ‰ES

### âŒ Tout en Docker (Ollama CPU) en Dev
**ProblÃ¨me:** Tests 10-20x plus lents, impossible de valider perf rÃ©elles

### âŒ DÃ©velopper Directement sur GPU Cloud
**ProblÃ¨me:** CoÃ»ts Ã©levÃ©s, latence rÃ©seau, pas de dev offline

### âŒ Attendre Docker Desktop GPU Support
**ProblÃ¨me:** Pas de timeline Apple/Docker, peut ne jamais arriver

---

## ğŸ“š RESSOURCES COMPLÃ‰MENTAIRES

### Docker Compose Overrides
- https://docs.docker.com/compose/multiple-compose-files/

### Ollama API Documentation
- https://github.com/ollama/ollama/blob/main/docs/api.md

### Docker GPU Support (NVIDIA)
- https://docs.docker.com/config/containers/resource_constraints/#gpu

### DigitalOcean GPU Droplets
- https://docs.digitalocean.com/products/droplets/how-to/gpu/

---

## ğŸ“ APPRENTISSAGES POUR SONNET 4.5

### Contexte Utilisateur
- Dev systÃ¨me RAG avancÃ© sur Mac M1 Max
- Stack: Ollama (7B Q8), Neo4j, Backend/Frontend custom
- Cible: DigitalOcean GPU Droplet
- ProblÃ¨me: Ollama en Docker = CPU only sur Mac

### Solution AppliquÃ©e
- **Approche hybride** avec abstractions propres
- **ZÃ©ro impact** sur le code mÃ©tier
- **Pattern standard** Docker Compose overrides
- **Migration sans friction** via variables d'environnement

### Principe Architectural ClÃ©
> "DÃ©pendre d'interfaces (API Ollama), pas d'implÃ©mentations (natif/Docker)"

Cette note devrait servir de rÃ©fÃ©rence pour des problÃ©matiques similaires GPU/ML en dÃ©veloppement cross-platform.

---

**ğŸ“ Note:** Cette approche est **recommandÃ©e** et **standard** dans l'industrie pour le dÃ©veloppement ML/GPU sur Mac avec dÃ©ploiement cloud Linux. Elle n'introduit aucun risque technique lors de la migration.