# üîç DIVETEACHER: COMPLETE GEMINI 2.5 FLASH-LITE IMPLEMENTATION AUDIT

**Date:** 2025-11-03  
**From:** ARIA Team (apr√®s r√©solution de tous les bugs)  
**To:** DiveTeacher Developer  
**Priority:** üî¥ CRITICAL - V√©rification compl√®te requise avant production

---

## üéØ OBJECTIF

ARIA a finalis√© l'impl√©mentation de **Gemini 2.5 Flash-Lite + OpenAI Embeddings** apr√®s avoir r√©solu **7 bugs critiques** d√©couverts lors de tests exhaustifs. Vous devez maintenant auditer votre impl√©mentation DiveTeacher pour √©viter ces m√™mes bugs.

**R√©sultat attendu:** Syst√®me DiveTeacher 100% fonctionnel avec Gemini 2.5 Flash-Lite, sans erreurs.

---

## üìã CHECKLIST COMPL√àTE (√Ä SUIVRE DANS L'ORDRE)

### ‚úÖ PHASE 1: AUDIT DU CODE

#### 1.1 V√©rifier le fichier d'ingestion principal

**Fichier:** Votre √©quivalent de `ingest_to_graphiti.py`

**√Ä v√©rifier ligne par ligne:**

```python
# ‚úÖ 1. IMPORTS - V√©rifier que TOUS ces imports sont pr√©sents et corrects:

from graphiti_core import Graphiti
from graphiti_core.llm_client.gemini_client import GeminiClient, LLMConfig  # ‚ö†Ô∏è PAS OpenAIClient!
from graphiti_core.embedder.openai import OpenAIEmbedder, OpenAIEmbedderConfig  # ‚ö†Ô∏è OpenAI pour embeddings!
from graphiti_core.cross_encoder.openai_reranker_client import OpenAIRerankerClient
from graphiti_core.nodes import EpisodeType
```

**‚ùå Bug #1 d√©couvert:** Import incorrect ‚Üí `OpenAIClient` au lieu de `GeminiClient`  
**‚úÖ Fix:** Utiliser `GeminiClient` pour le LLM, `OpenAIEmbedder` pour les embeddings

---

#### 1.2 V√©rifier la configuration LLM

**Configuration LLM (Gemini):**

```python
# ‚úÖ CORRECT:
gemini_key = os.getenv('GEMINI_API_KEY')
if not gemini_key:
    raise ValueError("GEMINI_API_KEY not found in environment")

llm_config = LLMConfig(
    api_key=gemini_key,
    model='gemini-2.5-flash-lite'  # ‚ö†Ô∏è Exactement ce nom!
)

llm_client = GeminiClient(  # ‚ö†Ô∏è GeminiClient, PAS OpenAIClient!
    config=llm_config,
    cache=False
)
```

**‚ùå Bug #2 d√©couvert:** Mauvais nom de mod√®le ‚Üí `gemini-2.0-flash-exp` (overloaded)  
**‚úÖ Fix:** Utiliser `gemini-2.5-flash-lite` (stable, ultra-low cost, 4K RPM)

**‚ùå Bug #3 d√©couvert:** Utilisation de `OpenAIClient` avec Gemini  
**‚úÖ Fix:** `GeminiClient` est le client natif recommand√© par Graphiti

---

#### 1.3 V√©rifier la configuration Embeddings

**Configuration Embeddings (OpenAI):**

```python
# ‚úÖ CORRECT:
openai_key = os.getenv('OPENAI_API_KEY')
if not openai_key:
    raise ValueError("OPENAI_API_KEY not found in environment for embeddings")

embedder_config = OpenAIEmbedderConfig(
    api_key=openai_key,
    embedding_model="text-embedding-3-small",  # ‚ö†Ô∏è Exactement ce nom!
    embedding_dim=1536  # ‚ö†Ô∏è Critique: 1536 dimensions pour OpenAI!
)
embedder_client = OpenAIEmbedder(config=embedder_config)

# ‚úÖ CORRECT: Cross-encoder (OpenAI aussi)
cross_encoder_config = LLMConfig(
    api_key=openai_key,
    model="gpt-4o-mini"  # ‚ö†Ô∏è Mod√®le l√©ger pour reranking
)
cross_encoder_client = OpenAIRerankerClient(config=cross_encoder_config)
```

**‚ùå Bug #4 d√©couvert:** Embeddings Gemini (768 dims) incompatibles avec DB OpenAI (1536 dims)  
**‚úÖ Fix:** Toujours utiliser `OpenAIEmbedder` avec `text-embedding-3-small` (1536 dims)

**üö® CRITIQUE:** Si vous utilisez Gemini embeddings, TOUTE votre DB Neo4j doit √™tre vid√©e et r√©ing√©r√©e!

---

#### 1.4 V√©rifier l'initialisation Graphiti

**Initialisation compl√®te:**

```python
# ‚úÖ CORRECT:
self.graphiti = Graphiti(
    neo4j_uri,
    neo4j_user,
    neo4j_password,
    llm_client=llm_client,          # ‚ö†Ô∏è GeminiClient pour LLM
    embedder=embedder_client,        # ‚ö†Ô∏è EXPLICIT! OpenAIEmbedder pour embeddings
    cross_encoder=cross_encoder_client  # ‚ö†Ô∏è EXPLICIT! OpenAIRerankerClient
)
```

**‚ùå Bug #5 d√©couvert:** `embedder` et `cross_encoder` non pass√©s explicitement  
**‚úÖ Fix:** TOUJOURS passer `embedder` et `cross_encoder` explicitement pour √©viter les defaults

---

#### 1.5 V√©rifier SEMAPHORE_LIMIT

**Configuration rate limiting:**

```python
# ‚úÖ CORRECT pour Gemini 2.5 Flash-Lite Tier 1 (4K RPM):
if not os.getenv('SEMAPHORE_LIMIT'):
    os.environ['SEMAPHORE_LIMIT'] = '10'  # ‚ö†Ô∏è 10 pour 4K RPM (safe + fast)
```

**Dans votre `.env`:**
```bash
SEMAPHORE_LIMIT=10  # ‚ö†Ô∏è Pour Gemini 2.5 Flash-Lite Tier 1 (4K RPM)
```

**‚ùå Bug #6 d√©couvert:** `SEMAPHORE_LIMIT=15` trop √©lev√© ‚Üí 429 errors  
**‚úÖ Fix:** `SEMAPHORE_LIMIT=10` optimal pour 4K RPM (Tier 1)

**Rate limits Gemini 2.5 Flash-Lite:**
- **Free Tier:** 15 RPM ‚Üí `SEMAPHORE_LIMIT=2`
- **Tier 1 (payant):** 4K RPM ‚Üí `SEMAPHORE_LIMIT=10`

---

### ‚úÖ PHASE 2: AUDIT DES CL√âS API

#### 2.1 V√©rifier le fichier .env

**Fichier:** `.env` (√† la racine de votre projet)

```bash
# ‚úÖ V√©rifier ces deux cl√©s:
GEMINI_API_KEY=AIza...  # ‚ö†Ô∏è Cl√© Google AI Studio
OPENAI_API_KEY=sk-proj-...  # ‚ö†Ô∏è Cl√© OpenAI pour embeddings
SEMAPHORE_LIMIT=10  # ‚ö†Ô∏è Pour Tier 1

# ‚ùå NE PAS utiliser:
# OPENROUTER_API_KEY=...  # ‚ö†Ô∏è On n'utilise PLUS OpenRouter!
# ANTHROPIC_API_KEY=...   # ‚ö†Ô∏è On n'utilise PLUS Anthropic!
```

#### 2.2 Tester les cl√©s API

**Commande de test:**

```bash
# Test 1: V√©rifier que les cl√©s sont charg√©es
python3 -c "
import os
from dotenv import load_dotenv
load_dotenv('.env')

gemini = os.getenv('GEMINI_API_KEY')
openai = os.getenv('OPENAI_API_KEY')

print('GEMINI_API_KEY:', '‚úÖ Found' if gemini else '‚ùå Missing')
print('OPENAI_API_KEY:', '‚úÖ Found' if openai else '‚ùå Missing')
print('SEMAPHORE_LIMIT:', os.getenv('SEMAPHORE_LIMIT', 'NOT SET'))
"
```

**R√©sultat attendu:**
```
GEMINI_API_KEY: ‚úÖ Found
OPENAI_API_KEY: ‚úÖ Found
SEMAPHORE_LIMIT: 10
```

---

### ‚úÖ PHASE 3: AUDIT DE LA BASE DE DONN√âES NEO4J

#### 3.1 V√©rifier les dimensions des embeddings existants

**üö® CRITIQUE:** C'est le bug qui a cass√© ARIA pendant 2 jours!

**Commande de diagnostic:**

```bash
# Via Docker (si Neo4j dans Docker):
docker exec <your-neo4j-container> cypher-shell -u neo4j -p <password> \
  "MATCH (n:Entity) RETURN size(n.name_embedding) as dims LIMIT 1"

# Via cypher-shell direct:
cypher-shell -u neo4j -p <password> \
  "MATCH (n:Entity) RETURN size(n.name_embedding) as dims LIMIT 1"
```

**R√©sultats possibles:**

| Dimensions | Signification | Action requise |
|-----------|---------------|----------------|
| **Pas de r√©sultat** (DB vide) | ‚úÖ OK - DB neuve | Aucune action |
| **1536** | ‚úÖ OK - OpenAI embeddings | Aucune action |
| **768** | ‚ùå Gemini embeddings | üö® **VIDER LA DB!** |
| **1024** | ‚ùå Ancien mod√®le | üö® **VIDER LA DB!** |
| **Autre** | ‚ùå Mod√®le inconnu | üö® **VIDER LA DB!** |

**‚ùå Bug #7 d√©couvert:** DB avait 1024 dims ‚Üí Incompatible avec OpenAI (1536)  
**‚úÖ Fix:** Vider compl√®tement la DB avant de r√©ing√©rer avec les bons embeddings

---

#### 3.2 Vider la base de donn√©es (si n√©cessaire)

**‚ö†Ô∏è √Ä faire SEULEMENT si dimensions ‚â† 1536:**

```bash
# √âtape 1: Backup (optionnel, si donn√©es importantes)
docker exec <your-neo4j-container> neo4j-admin dump \
  --database=neo4j \
  --to=/backups/neo4j-pre-migration-$(date +%Y%m%d_%H%M%S).dump

# √âtape 2: Vider TOUTE la DB
docker exec <your-neo4j-container> cypher-shell -u neo4j -p <password> \
  "MATCH (n) DETACH DELETE n"

# √âtape 3: V√©rifier que la DB est vide
docker exec <your-neo4j-container> cypher-shell -u neo4j -p <password> \
  "MATCH (n) RETURN count(n) as total"
# R√©sultat attendu: total = 0
```

**üéØ Pourquoi vider la DB?**

Si votre DB contient des embeddings de **768 dimensions** (Gemini) et que vous essayez d'ins√©rer des embeddings de **1536 dimensions** (OpenAI), Neo4j va crasher avec:

```
Invalid input for 'vector.similarity.cosine()': 
The supplied vectors do not have the same number of dimensions.
```

**Solution:** Repartir avec une DB propre!

---

### ‚úÖ PHASE 4: TEST D'INT√âGRATION COMPLET

#### 4.1 Cr√©er un script de test

**Fichier:** `test_graphiti_integration.py`

```python
#!/usr/bin/env python3
"""
Test complet de l'int√©gration Gemini 2.5 Flash-Lite + OpenAI Embeddings
"""
import os
import sys
import asyncio
from datetime import datetime

# Adapter le path √† votre projet
sys.path.insert(0, '/path/to/your/project')

from dotenv import load_dotenv
load_dotenv('.env')

# Importer votre classe d'ingestion (adapter le nom)
from your_ingestion_module import YourGraphitiIngestionClass

print("\n" + "="*70)
print("üß™ TEST D'INT√âGRATION - GEMINI 2.5 FLASH-LITE + OPENAI EMBEDDINGS")
print("="*70)

async def test_full_integration():
    """Test complet de l'ingestion"""
    
    # 1. V√©rifier les variables d'environnement
    print("\n1Ô∏è‚É£  V√©rification des cl√©s API:")
    gemini_key = os.getenv('GEMINI_API_KEY')
    openai_key = os.getenv('OPENAI_API_KEY')
    semaphore = os.getenv('SEMAPHORE_LIMIT')
    
    print(f"   ‚îú‚îÄ GEMINI_API_KEY: {'‚úÖ Found' if gemini_key else '‚ùå Missing'}")
    print(f"   ‚îú‚îÄ OPENAI_API_KEY: {'‚úÖ Found' if openai_key else '‚ùå Missing'}")
    print(f"   ‚îî‚îÄ SEMAPHORE_LIMIT: {semaphore if semaphore else '‚ùå Not set'}")
    
    if not gemini_key or not openai_key:
        print("\n‚ùå API keys manquantes! V√©rifier votre .env")
        return False
    
    # 2. Initialiser le client
    print("\n2Ô∏è‚É£  Initialisation du client Graphiti:")
    try:
        client = YourGraphitiIngestionClass()  # Adapter le nom
        await client.initialize()
        print("   ‚úÖ Client initialis√© avec succ√®s")
    except Exception as e:
        print(f"   ‚ùå Erreur d'initialisation: {e}")
        return False
    
    # 3. Test d'ingestion
    print("\n3Ô∏è‚É£  Test d'ingestion d'un √©pisode:")
    
    test_episode = {
        "episode_id": f"test-integration-{datetime.now().strftime('%Y%m%d_%H%M%S')}",
        "content": """
        TEST D'INT√âGRATION DIVETEACHER
        
        Ce test valide:
        - LLM: Gemini 2.5 Flash-Lite (Google AI Direct)
        - Embeddings: OpenAI text-embedding-3-small (1536 dimensions)
        - Cross-encoder: OpenAI gpt-4o-mini
        - Neo4j: Connexion et stockage
        - Rate limiting: SEMAPHORE_LIMIT configur√©
        
        Si ce test passe, votre syst√®me est 100% op√©rationnel!
        """.strip(),
        "timestamp": datetime.now(),
        "agent": "TEST",
        "type": "integration_test",
        "date": datetime.now().strftime("%Y-%m-%d"),
        "context": "testing",
        "metadata": {"test": True, "version": "1.0.0"}
    }
    
    print(f"   ‚îú‚îÄ Episode ID: {test_episode['episode_id']}")
    print(f"   ‚îú‚îÄ Content: {len(test_episode['content'])} chars")
    print(f"   ‚îî‚îÄ Ingestion...")
    
    try:
        start = datetime.now()
        result = await client.add_episode(test_episode)
        elapsed = (datetime.now() - start).total_seconds()
        
        if result.get("success") or result.get("status") == "success":
            print(f"\n   ‚úÖ Ingestion r√©ussie en {elapsed:.1f}s!")
            print(f"   ‚îú‚îÄ Entities: {result.get('entities_count', 0)}")
            print(f"   ‚îú‚îÄ Relations: {result.get('relations_count', 0)}")
            print(f"   ‚îî‚îÄ Status: {result.get('status', 'success')}")
            success = True
        else:
            print(f"\n   ‚ùå Ingestion √©chou√©e:")
            print(f"   ‚îî‚îÄ Error: {result.get('error', 'Unknown')}")
            success = False
            
    except Exception as e:
        print(f"\n   ‚ùå Exception lors de l'ingestion:")
        print(f"   ‚îî‚îÄ {str(e)[:200]}")
        import traceback
        traceback.print_exc()
        success = False
    
    finally:
        print("\n4Ô∏è‚É£  Cleanup:")
        await client.close()
        print("   ‚úÖ Client ferm√©")
    
    return success

# Ex√©cuter le test
try:
    success = asyncio.run(test_full_integration())
    
    print("\n" + "="*70)
    if success:
        print("‚úÖ‚úÖ‚úÖ TEST D'INT√âGRATION: R√âUSSI ‚úÖ‚úÖ‚úÖ")
        print("="*70)
        print("\nüéâ Votre syst√®me DiveTeacher est PRODUCTION READY!")
        print("üí∞ Configuration:")
        print("   ‚îú‚îÄ LLM: Gemini 2.5 Flash-Lite ($0.10/M input + $0.40/M output)")
        print("   ‚îú‚îÄ Embeddings: OpenAI text-embedding-3-small ($0.02/M)")
        print("   ‚îú‚îÄ Rate Limit: 4K RPM (Tier 1)")
        print("   ‚îî‚îÄ SEMAPHORE_LIMIT: 10")
        print("\n‚úÖ Tous les syst√®mes GO! üöÄ")
        sys.exit(0)
    else:
        print("‚ùå‚ùå‚ùå TEST D'INT√âGRATION: √âCHOU√â ‚ùå‚ùå‚ùå")
        print("="*70)
        print("\nüö® Syst√®me NON PR√äT!")
        print("üìã V√©rifier les logs ci-dessus pour les erreurs")
        print("üìû Consulter la section TROUBLESHOOTING ci-dessous")
        sys.exit(1)
        
except Exception as e:
    print("\n" + "="*70)
    print("‚ùå‚ùå‚ùå TEST CRASH√â ‚ùå‚ùå‚ùå")
    print("="*70)
    print(f"\nErreur fatale: {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)
```

#### 4.2 Ex√©cuter le test

```bash
# Ex√©cuter le test d'int√©gration
python3 test_graphiti_integration.py
```

**R√©sultat attendu:**
```
======================================================================
üß™ TEST D'INT√âGRATION - GEMINI 2.5 FLASH-LITE + OPENAI EMBEDDINGS
======================================================================

1Ô∏è‚É£  V√©rification des cl√©s API:
   ‚îú‚îÄ GEMINI_API_KEY: ‚úÖ Found
   ‚îú‚îÄ OPENAI_API_KEY: ‚úÖ Found
   ‚îî‚îÄ SEMAPHORE_LIMIT: 10

2Ô∏è‚É£  Initialisation du client Graphiti:
   ‚úÖ Client initialis√© avec succ√®s

3Ô∏è‚É£  Test d'ingestion d'un √©pisode:
   ‚îú‚îÄ Episode ID: test-integration-20251103_184500
   ‚îú‚îÄ Content: 287 chars
   ‚îî‚îÄ Ingestion...

   ‚úÖ Ingestion r√©ussie en 8.5s!
   ‚îú‚îÄ Entities: 3
   ‚îú‚îÄ Relations: 2
   ‚îî‚îÄ Status: success

4Ô∏è‚É£  Cleanup:
   ‚úÖ Client ferm√©

======================================================================
‚úÖ‚úÖ‚úÖ TEST D'INT√âGRATION: R√âUSSI ‚úÖ‚úÖ‚úÖ
======================================================================

üéâ Votre syst√®me DiveTeacher est PRODUCTION READY!
```

---

## üö® TROUBLESHOOTING - ERREURS COURANTES

### Erreur #1: `GEMINI_API_KEY not found`

**Sympt√¥me:**
```
ValueError: GEMINI_API_KEY not found in environment
```

**Cause:** Fichier `.env` non charg√© ou cl√© manquante

**Solution:**
```python
# Ajouter au d√©but de votre script:
from dotenv import load_dotenv
load_dotenv('.env')  # ‚ö†Ô∏è Charger AVANT tous les imports de votre code!

# V√©rifier:
import os
print(os.getenv('GEMINI_API_KEY'))  # Doit afficher votre cl√©
```

---

### Erreur #2: `Invalid input for 'vector.similarity.cosine()': vectors do not have same dimensions`

**Sympt√¥me:**
```
neo4j.exceptions.ClientError: Invalid input for 'vector.similarity.cosine()': 
The supplied vectors do not have the same number of dimensions.
```

**Cause:** DB contient des embeddings d'un autre mod√®le (768 ou 1024 dims) incompatibles avec OpenAI (1536 dims)

**Solution:**
```bash
# VIDER COMPL√àTEMENT LA DB:
docker exec <your-neo4j-container> cypher-shell -u neo4j -p <password> \
  "MATCH (n) DETACH DELETE n"

# V√©rifier:
docker exec <your-neo4j-container> cypher-shell -u neo4j -p <password> \
  "MATCH (n) RETURN count(n) as total"
# Doit retourner: total = 0
```

---

### Erreur #3: `429 Resource Exhausted` (Rate limit)

**Sympt√¥me:**
```
429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'Resource exhausted'}}
```

**Cause:** `SEMAPHORE_LIMIT` trop √©lev√© pour votre tier Gemini

**Solution:**

```bash
# Dans .env, ajuster selon votre tier:
SEMAPHORE_LIMIT=2   # Si FREE tier (15 RPM)
SEMAPHORE_LIMIT=10  # Si TIER 1 payant (4K RPM)
```

**V√©rifier votre tier:**
- Aller sur: https://aistudio.google.com/app/apikey
- Cliquer sur votre cl√©
- Voir "Rate limits" dans les d√©tails

---

### Erreur #4: `ImportError: cannot import name 'GeminiClient'`

**Sympt√¥me:**
```
ImportError: cannot import name 'GeminiClient' from 'graphiti_core.llm_client.gemini_client'
```

**Cause:** Version de Graphiti trop ancienne ou mauvais import

**Solution:**
```bash
# V√©rifier version Graphiti:
pip show graphiti-core

# Si < 0.20.0, mettre √† jour:
pip install --upgrade graphiti-core

# V√©rifier import:
python3 -c "from graphiti_core.llm_client.gemini_client import GeminiClient; print('‚úÖ OK')"
```

---

### Erreur #5: `503 UNAVAILABLE` (Mod√®le Gemini)

**Sympt√¥me:**
```
503 UNAVAILABLE. {'error': {'code': 503, 'message': 'The model is overloaded'}}
```

**Cause:** Utilisation du mod√®le `gemini-2.0-flash-exp` (experimental, surcharg√©)

**Solution:**
```python
# ‚ùå NE PAS utiliser:
model='gemini-2.0-flash-exp'  # Experimental, instable!

# ‚úÖ UTILISER:
model='gemini-2.5-flash-lite'  # Stable, ultra-low cost, 4K RPM!
```

---

### Erreur #6: `Unsupported parameter: 'reasoning.effort'`

**Sympt√¥me:**
```
Unsupported parameter: 'reasoning.effort' is not supported with this model
```

**Cause:** Utilisation de `OpenAIClient` avec Gemini (mauvais client!)

**Solution:**
```python
# ‚ùå NE PAS utiliser OpenAIClient pour Gemini:
from graphiti_core.llm_client import OpenAIClient  # ‚ùå FAUX!
llm_client = OpenAIClient(...)

# ‚úÖ UTILISER GeminiClient pour Gemini:
from graphiti_core.llm_client.gemini_client import GeminiClient  # ‚úÖ CORRECT!
llm_client = GeminiClient(...)
```

---

### Erreur #7: Neo4j non d√©marr√©

**Sympt√¥me:**
```
ServiceUnavailable: Unable to connect to bolt://localhost:7687
```

**Cause:** Container Neo4j arr√™t√©

**Solution:**
```bash
# V√©rifier status:
docker ps | grep neo4j

# Si absent, d√©marrer:
docker start <your-neo4j-container>

# Attendre 10 secondes
sleep 10

# V√©rifier connexion:
curl -I http://localhost:7474
# Doit retourner: HTTP/1.1 200 OK
```

---

## üìä CONFIGURATION FINALE VALID√âE (ARIA)

**Voici la configuration exacte qui fonctionne en production chez ARIA:**

### LLM (Gemini 2.5 Flash-Lite)
```python
from graphiti_core.llm_client.gemini_client import GeminiClient, LLMConfig

llm_config = LLMConfig(
    api_key=os.getenv('GEMINI_API_KEY'),
    model='gemini-2.5-flash-lite'
)

llm_client = GeminiClient(
    config=llm_config,
    cache=False
)
```

### Embeddings (OpenAI)
```python
from graphiti_core.embedder.openai import OpenAIEmbedder, OpenAIEmbedderConfig

embedder_config = OpenAIEmbedderConfig(
    api_key=os.getenv('OPENAI_API_KEY'),
    embedding_model="text-embedding-3-small",
    embedding_dim=1536
)

embedder_client = OpenAIEmbedder(config=embedder_config)
```

### Cross-encoder (OpenAI)
```python
from graphiti_core.cross_encoder.openai_reranker_client import OpenAIRerankerClient

cross_encoder_config = LLMConfig(
    api_key=os.getenv('OPENAI_API_KEY'),
    model="gpt-4o-mini"
)

cross_encoder_client = OpenAIRerankerClient(config=cross_encoder_config)
```

### Graphiti Initialization
```python
self.graphiti = Graphiti(
    neo4j_uri,
    neo4j_user,
    neo4j_password,
    llm_client=llm_client,
    embedder=embedder_client,
    cross_encoder=cross_encoder_client
)
```

### Environment Variables (.env)
```bash
GEMINI_API_KEY=AIzaSy...  # Google AI Studio
OPENAI_API_KEY=sk-proj-...  # OpenAI
SEMAPHORE_LIMIT=10  # Tier 1 (4K RPM)
GRAPHITI_TELEMETRY_ENABLED=false  # Optionnel
```

---

## üí∞ CO√õTS ATTENDUS

### Gemini 2.5 Flash-Lite
- **Input:** $0.10 / million tokens
- **Output:** $0.40 / million tokens
- **Estimation:** ~$0.005 par ingestion de 3 documents

### OpenAI Embeddings
- **text-embedding-3-small:** $0.02 / million tokens
- **Estimation:** ~$0.001 par ingestion de 3 documents

### Total
- **Par run:** ~$0.006
- **Par mois (30 runs):** ~$0.18
- **Par an:** ~$2.16

**vs Haiku/GPT-4o:** √âconomie de 99%+ üéâ

---

## ‚úÖ CHECKLIST FINALE

Avant de consid√©rer l'impl√©mentation comme termin√©e:

- [ ] Tous les imports sont corrects (GeminiClient, OpenAIEmbedder, etc.)
- [ ] Mod√®le LLM: `gemini-2.5-flash-lite`
- [ ] Embeddings: `text-embedding-3-small` (1536 dims)
- [ ] Cross-encoder: `gpt-4o-mini`
- [ ] Les 3 clients sont pass√©s explicitement √† `Graphiti()`
- [ ] Fichier `.env` contient `GEMINI_API_KEY` et `OPENAI_API_KEY`
- [ ] `SEMAPHORE_LIMIT=10` dans `.env`
- [ ] Base Neo4j a des embeddings 1536 dims (ou est vide)
- [ ] Test d'int√©gration passe avec succ√®s
- [ ] Logs montrent "Gemini 2.5 Flash-Lite" et "OpenAI embeddings"
- [ ] Aucune erreur de dimension de vecteurs
- [ ] Aucune erreur 429 (rate limit)
- [ ] Aucune erreur 503 (model overloaded)

---

## üìû SUPPORT

Si apr√®s avoir suivi ce guide vous rencontrez toujours des probl√®mes:

1. **Logs d√©taill√©s:** Capturer toute la stacktrace de l'erreur
2. **Configuration:** Partager votre code de configuration (sans les cl√©s!)
3. **Neo4j:** V√©rifier les dimensions des embeddings existants
4. **Test isolation:** Essayer avec une DB Neo4j vide

**Ressources:**
- Graphiti Docs: https://help.getzep.com/graphiti/configuration/llm-configuration
- Gemini API: https://ai.google.dev/gemini-api/docs
- OpenAI Embeddings: https://platform.openai.com/docs/guides/embeddings

---

## üéØ R√âSUM√â EX√âCUTIF

**Ce que vous DEVEZ faire:**

1. ‚úÖ V√©rifier imports: `GeminiClient` + `OpenAIEmbedder` + `OpenAIRerankerClient`
2. ‚úÖ Mod√®le LLM: `gemini-2.5-flash-lite`
3. ‚úÖ Embeddings: `text-embedding-3-small` (1536 dims)
4. ‚úÖ Passer les 3 clients explicitement √† `Graphiti()`
5. ‚úÖ Configurer `SEMAPHORE_LIMIT=10` dans `.env`
6. ‚úÖ V√©rifier dimensions Neo4j (1536 ou vide)
7. ‚úÖ Ex√©cuter test d'int√©gration complet

**Si test passe:** ‚úÖ Production ready!  
**Si test √©choue:** üö® Consulter TROUBLESHOOTING

---

**üéâ BONNE CHANCE AVEC VOTRE AUDIT!**

**Temps estim√©:** 30-60 minutes  
**Difficult√©:** Moyenne  
**Impact:** CRITIQUE pour la production

**Questions?** Consulter les sections TROUBLESHOOTING et SUPPORT ci-dessus.

---

**Document cr√©√© par:** ARIA Team  
**Date:** 2025-11-03  
**Statut:** ‚úÖ Valid√© en production ARIA (0 erreurs apr√®s 5 jours de debug)

