# Qwen2.5-7B-Instruct-Q8_0 with Ollama on DigitalOcean
## Complete Implementation & Debugging Guide for AI Agents

**Target Audience**: Claude Sonnet 4.5 AI Agent  
**Purpose**: Implementation, debugging, and operational reference for Qwen2.5-7B-Instruct-Q8_0 running on Ollama in a DigitalOcean droplet  
**Last Updated**: October 28, 2025

---

## Table of Contents

1. [Model Overview](#model-overview)
2. [Quantization Format (Q8_0)](#quantization-format-q8_0)
3. [DigitalOcean Infrastructure Setup](#digitalocean-infrastructure-setup)
4. [Ollama Installation & Configuration](#ollama-installation--configuration)
5. [Model Deployment](#model-deployment)
6. [API Reference](#api-reference)
7. [Prompt Format & Best Practices](#prompt-format--best-practices)
8. [Debugging & Troubleshooting](#debugging--troubleshooting)
9. [Performance Optimization](#performance-optimization)
10. [Monitoring & Maintenance](#monitoring--maintenance)
11. [Resource Links](#resource-links)

---

## Model Overview

### Qwen2.5-7B-Instruct

**Official Model Page**: https://huggingface.co/Qwen/Qwen2.5-7B-Instruct  
**GGUF Quantizations**: https://huggingface.co/bartowski/Qwen2.5-7B-Instruct-GGUF  
**GitHub Repository**: https://github.com/mx4ai/qwen2.5  
**Documentation**: https://qwen.readthedocs.io/  
**Ollama Model Page**: https://ollama.com/library/qwen2.5

#### Key Specifications

- **Parameters**: 7 billion
- **Architecture**: Dense decoder-only transformer
- **Context Length**: Up to 128K tokens
- **Max Generation**: Up to 8K tokens
- **Training Data**: 18 trillion tokens
- **Multilingual Support**: 29+ languages including English, French, Spanish, German, Japanese, Chinese, Arabic, etc.

#### Capabilities & Improvements

1. **Instruction Following**: Significantly improved over Qwen2
2. **Long-Form Generation**: Optimized for generating 8K+ tokens
3. **Structured Data Understanding**: Enhanced table and structured data processing
4. **JSON Output**: Superior structured output generation, especially JSON
5. **Code Generation**: Strong coding capabilities across 40+ programming languages
6. **Mathematics**: Enhanced mathematical reasoning
7. **System Prompts**: More resilient to diverse system prompt variations
8. **Role-Playing**: Improved chatbot condition-setting and role implementation

#### Use Cases

- Conversational AI and chatbots
- Code generation and debugging
- Content creation (reports, articles, documentation)
- Data analysis and structured output generation
- Mathematical problem solving
- Multilingual translation and understanding
- Question answering systems

---

## Quantization Format (Q8_0)

### What is Q8_0?

Q8_0 is an 8-bit quantization format that offers an optimal balance between model quality and resource efficiency.

#### Technical Details

- **Bit Precision**: 8-bit integer quantization
- **File Size**: ~7.87 GB for Qwen2.5-7B-Instruct-Q8_0.gguf
- **Quality**: Near-original model quality (minimal degradation)
- **Speed**: Faster inference than higher precision formats
- **Memory**: Requires approximately 8-10 GB RAM/VRAM

#### Download Link

**Direct GGUF File**: https://huggingface.co/bartowski/Qwen2.5-7B-Instruct-GGUF/blob/main/Qwen2.5-7B-Instruct-Q8_0.gguf

#### Why Q8_0?

- **Highest Quality Quantization**: Minimal quality loss compared to FP16
- **Production-Ready**: Suitable for production deployments
- **Balance**: Optimal trade-off between performance and resource usage
- **Compatibility**: Works well on both CPU and GPU systems
- **Stability**: More stable than lower bit quantizations (Q4, Q3, Q2)

#### Alternative Quantizations Available

If Q8_0 is too large or slow, consider:

- **Q6_K**: 6-bit, ~6.1 GB (good quality, faster)
- **Q5_K_M**: 5-bit medium, ~5.0 GB (decent quality, good speed)
- **Q4_K_M**: 4-bit medium, ~4.1 GB (acceptable quality, fast)
- **Q3_K_M**: 3-bit medium, ~3.3 GB (lower quality, very fast)

---

## DigitalOcean Infrastructure Setup

### Recommended Droplet Specifications

#### CPU-Only Option (Budget-Friendly)

**Minimum Requirements**:
- **vCPUs**: 4 AMD/Intel cores
- **RAM**: 16 GB (minimum 12 GB)
- **Storage**: 50 GB SSD
- **OS**: Ubuntu 24.04 LTS (AI/ML Ready preferred)
- **Estimated Cost**: $48-96/month

**Premium AMD Option**:
- **vCPUs**: 4 AMD cores
- **RAM**: 8 GB
- **Storage**: 160 GB SSD
- **Note**: Slower inference, suitable for development/testing

#### GPU Option (Production/High-Performance)

**Recommended for Production**:
- **GPU**: NVIDIA H100 (1x)
- **vCPUs**: 8-16 cores
- **RAM**: 40-80 GB
- **Storage**: 100-200 GB SSD
- **OS**: Ubuntu 24.04 LTS with AI/ML Ready image
- **Estimated Cost**: Variable, check DigitalOcean GPU Droplets pricing

**GPU Droplet Documentation**: https://www.digitalocean.com/blog/announcing-gpu-droplets  
**GPU Tutorial**: https://www.digitalocean.com/community/tutorials/run-llms-with-ollama-on-h100-gpus-for-maximum-efficiency

### Creating Your Droplet

#### Step-by-Step Setup

1. **Log into DigitalOcean Dashboard**
   - Navigate to: https://cloud.digitalocean.com/

2. **Create New Droplet**
   ```bash
   # Click "Create" → "Droplets"
   ```

3. **Select Region**
   - Choose region closest to your users
   - Consider data residency requirements

4. **Choose Image**
   - **Recommended**: Marketplace → "AI/ML Ready" Ubuntu 24.04
   - **Alternative**: OS → Ubuntu 24.04 LTS
   - AI/ML Ready includes pre-installed: Python, PyTorch, CUDA, cuDNN

5. **Select Droplet Size**
   - See specifications above
   - Can resize later if needed

6. **Authentication**
   - **Recommended**: SSH Key (more secure)
   - **Alternative**: Password (convenient for quick testing)

7. **Additional Options**
   - **Monitoring**: Enable (recommended)
   - **IPv6**: Enable if needed
   - **Backups**: Optional (adds 20% cost)

8. **Hostname & Tags**
   - Example hostname: `qwen-ollama-prod`
   - Tags: `ai`, `llm`, `qwen`, `ollama`

9. **Create Droplet**
   - Wait 1-2 minutes for provisioning

### Initial Droplet Access

```bash
# SSH into your droplet
ssh root@YOUR_DROPLET_IP

# If using password, you'll be prompted
# If using SSH key, direct access
```

### Security Best Practices

```bash
# Update system packages
sudo apt update && sudo apt upgrade -y

# Create non-root user (optional but recommended)
adduser aiagent
usermod -aG sudo aiagent

# Configure firewall
sudo ufw allow 22/tcp        # SSH
sudo ufw allow 11434/tcp     # Ollama API
sudo ufw allow 3000/tcp      # Optional: Open WebUI
sudo ufw enable

# Check firewall status
sudo ufw status
```

### Network Configuration Notes

- **Default Ollama Port**: 11434
- **Default Ollama API**: `http://localhost:11434`
- **External Access**: Configure firewall and environment variables
- **HTTPS**: Consider reverse proxy (Nginx) for production

---

## Ollama Installation & Configuration

### Installation Methods

#### Method 1: Official Installation Script (Recommended)

```bash
# Download and install Ollama
curl -fsSL https://ollama.com/install.sh | sh

# Verify installation
ollama --version

# Expected output: ollama version 0.5.x or higher
```

#### Method 2: Manual Installation

```bash
# Download Ollama binary
curl -L https://ollama.com/download/ollama-linux-amd64 -o /usr/local/bin/ollama
chmod +x /usr/local/bin/ollama

# Create Ollama service
sudo useradd -r -s /bin/false -m -d /usr/share/ollama ollama

# Create systemd service file
sudo nano /etc/systemd/system/ollama.service
```

**ollama.service content**:
```ini
[Unit]
Description=Ollama Service
After=network-online.target

[Service]
ExecStart=/usr/local/bin/ollama serve
User=ollama
Group=ollama
Restart=always
RestartSec=3
Environment="HOME=/usr/share/ollama"
Environment="OLLAMA_HOST=0.0.0.0:11434"

[Install]
WantedBy=default.target
```

```bash
# Reload systemd and start service
sudo systemctl daemon-reload
sudo systemctl enable ollama
sudo systemctl start ollama

# Check status
sudo systemctl status ollama
```

### Ollama Configuration

#### Environment Variables

Configure Ollama behavior through environment variables:

```bash
# Edit Ollama service to add environment variables
sudo systemctl edit ollama

# Add in the [Service] section:
```

**Common Environment Variables**:

```bash
# Listen on all interfaces (required for external access)
OLLAMA_HOST=0.0.0.0:11434

# GPU configuration (if using GPU)
CUDA_VISIBLE_DEVICES=0

# Memory limits (optional)
OLLAMA_MAX_LOADED_MODELS=1  # Keep only 1 model in memory
OLLAMA_MAX_QUEUE=512         # Max queue size

# Model keep-alive duration (default: 5m)
OLLAMA_KEEP_ALIVE=10m

# Number of parallel requests
OLLAMA_NUM_PARALLEL=1

# Context window size
OLLAMA_NUM_CTX=4096
```

#### Restart Service After Configuration

```bash
sudo systemctl daemon-reload
sudo systemctl restart ollama
sudo systemctl status ollama
```

### Verify Ollama is Running

```bash
# Check if Ollama API is accessible
curl http://localhost:11434/api/version

# Expected response:
# {"version":"0.5.x"}

# List running models
ollama ps

# List all downloaded models
ollama list
```

### Ollama Command Reference

```bash
# Start Ollama server (if not using systemd)
ollama serve

# Pull a model
ollama pull qwen2.5:7b

# Run model interactively
ollama run qwen2.5:7b

# Show model information
ollama show qwen2.5:7b

# List models
ollama list

# Remove a model
ollama rm qwen2.5:7b

# Copy a model
ollama cp qwen2.5:7b qwen2.5-custom

# Update Ollama
curl -fsSL https://ollama.com/install.sh | sh
```

---

## Model Deployment

### Pulling Qwen2.5-7B-Instruct-Q8_0

#### Using Ollama (Recommended)

```bash
# Pull the official Qwen2.5 7B model
# Ollama automatically selects an appropriate quantization
ollama pull qwen2.5:7b

# To specifically request the 7b variant
ollama pull qwen2.5:7b-instruct

# List models to verify
ollama list
```

**Expected Output**:
```
NAME                    ID              SIZE      MODIFIED
qwen2.5:7b              a3dg72h5        4.7 GB    5 minutes ago
```

#### Using Custom GGUF File

If you want to use the exact Q8_0 quantization from Hugging Face:

```bash
# Download the Q8_0 GGUF file
mkdir -p ~/models
cd ~/models
wget https://huggingface.co/bartowski/Qwen2.5-7B-Instruct-GGUF/resolve/main/Qwen2.5-7B-Instruct-Q8_0.gguf

# Or use huggingface-cli
pip install -U "huggingface_hub[cli]"
huggingface-cli download bartowski/Qwen2.5-7B-Instruct-GGUF \
  --include "Qwen2.5-7B-Instruct-Q8_0.gguf" \
  --local-dir ~/models/

# Create a Modelfile
nano ~/models/Qwen2.5-Q8-Modelfile
```

**Modelfile Content**:
```dockerfile
FROM ~/models/Qwen2.5-7B-Instruct-Q8_0.gguf

# Set temperature (higher = more creative, lower = more coherent)
PARAMETER temperature 0.7
PARAMETER top_p 0.8
PARAMETER repeat_penalty 1.05
PARAMETER top_k 20

# Set context window
PARAMETER num_ctx 4096

# Set stop sequences
PARAMETER stop "<|im_start|>"
PARAMETER stop "<|im_end|>"

# System prompt (optional)
SYSTEM You are Qwen, created by Alibaba Cloud. You are a helpful assistant.

# Chat template
TEMPLATE """{{ if .Messages }}
{{- if or .System .Tools }}<|im_start|>system
{{ .System }}
{{- if .Tools }}

# Tools

You are provided with function signatures within <tools></tools> XML tags:
<tools>
{{- range .Tools }}
{"type": "function", "function": {{ .Function }}}
{{- end }}
</tools>

For each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:
<tool_call>
{"name": <function-name>, "arguments": <args-json-object>}
</tool_call>
{{- end }}<|im_end|>
{{ end }}
{{- range $i, $_ := .Messages }}
{{- $last := eq (len (slice $.Messages $i)) 1 -}}
{{- if eq .Role "user" }}<|im_start|>user
{{ .Content }}<|im_end|>
{{ else if eq .Role "assistant" }}<|im_start|>assistant
{{ if .Content }}{{ .Content }}
{{- else if .ToolCalls }}<tool_call>
{{ range .ToolCalls }}{"name": "{{ .Function.Name }}", "arguments": {{ .Function.Arguments }}}
{{ end }}</tool_call>
{{- end }}{{ if not $last }}<|im_end|>
{{ end }}
{{- else if eq .Role "tool" }}<|im_start|>tool
{{ .Content }}<|im_end|>
{{ end }}
{{- end }}
{{- if not (eq (len .Messages) 0) }}{{ if ne (index .Messages (sub (len .Messages) 1)).Role "assistant" }}<|im_start|>assistant
{{ end }}{{ end }}{{ end }}"""
```

**Create the model in Ollama**:
```bash
ollama create qwen2.5-q8-custom -f ~/models/Qwen2.5-Q8-Modelfile

# Verify creation
ollama list

# Test the model
ollama run qwen2.5-q8-custom
```

### Model Verification

```bash
# Interactive test
ollama run qwen2.5:7b
>>> Hello, can you introduce yourself?
>>> /bye

# API test
curl http://localhost:11434/api/generate -d '{
  "model": "qwen2.5:7b",
  "prompt": "Explain quantum computing in simple terms.",
  "stream": false
}'

# Check model info
ollama show qwen2.5:7b --modelfile
```

### Loading Model into Memory

```bash
# Preload model into memory (optional, improves first response time)
curl http://localhost:11434/api/generate -d '{
  "model": "qwen2.5:7b",
  "prompt": "",
  "keep_alive": 600
}'

# Check loaded models
ollama ps

# Expected output:
# NAME            ID         SIZE    PROCESSOR    UNTIL
# qwen2.5:7b      abc123     7.8GB   100% GPU     10 minutes from now
```

---

## API Reference

### Ollama API Base URL

```
http://YOUR_DROPLET_IP:11434
```

**Local**: `http://localhost:11434`  
**Remote**: `http://YOUR_DROPLET_IP:11434`

### Key API Endpoints

#### 1. Generate Completion (`/api/generate`)

Generate a response for a given prompt.

**POST** `/api/generate`

```bash
curl http://localhost:11434/api/generate -d '{
  "model": "qwen2.5:7b",
  "prompt": "Write a Python function to calculate factorial",
  "stream": false,
  "options": {
    "temperature": 0.7,
    "top_p": 0.8,
    "top_k": 40
  }
}'
```

**Parameters**:
- `model` (required): Model name
- `prompt` (required): Input text
- `stream` (optional): Boolean, default `true`
- `system` (optional): System message override
- `template` (optional): Prompt template override
- `context` (optional): Context from previous request
- `raw` (optional): Boolean, skip formatting
- `format` (optional): Response format (`json`)
- `options` (optional): Model parameters
  - `temperature`: 0.0-2.0 (default: 0.8)
  - `top_p`: 0.0-1.0 (default: 0.9)
  - `top_k`: Integer (default: 40)
  - `repeat_penalty`: 1.0+ (default: 1.1)
  - `num_predict`: Max tokens to generate
  - `num_ctx`: Context window size
- `keep_alive` (optional): Duration to keep model loaded (e.g., "5m", "1h")

**Response** (stream: false):
```json
{
  "model": "qwen2.5:7b",
  "created_at": "2025-10-28T10:30:00.123Z",
  "response": "Here's a Python function to calculate factorial:\n\n```python\ndef factorial(n):\n    if n == 0 or n == 1:\n        return 1\n    return n * factorial(n - 1)\n```",
  "done": true,
  "context": [/* context array */],
  "total_duration": 2500000000,
  "load_duration": 500000000,
  "prompt_eval_count": 12,
  "prompt_eval_duration": 300000000,
  "eval_count": 45,
  "eval_duration": 1700000000
}
```

#### 2. Chat Completion (`/api/chat`)

Multi-turn conversational interface.

**POST** `/api/chat`

```bash
curl http://localhost:11434/api/chat -d '{
  "model": "qwen2.5:7b",
  "messages": [
    {
      "role": "system",
      "content": "You are a helpful coding assistant."
    },
    {
      "role": "user",
      "content": "How do I reverse a string in Python?"
    }
  ],
  "stream": false
}'
```

**Message Roles**:
- `system`: System instructions
- `user`: User messages
- `assistant`: Assistant responses
- `tool`: Tool/function results (for function calling)

**Parameters**:
- `model` (required): Model name
- `messages` (required): Array of message objects
- `stream` (optional): Boolean, default `true`
- `format` (optional): Response format (`json`)
- `options` (optional): Model parameters (same as /api/generate)
- `keep_alive` (optional): Duration string
- `tools` (optional): Array of function definitions (for function calling)

**Response**:
```json
{
  "model": "qwen2.5:7b",
  "created_at": "2025-10-28T10:30:00.123Z",
  "message": {
    "role": "assistant",
    "content": "To reverse a string in Python, you can use slicing:\n\n```python\ntext = 'hello'\nreversed_text = text[::-1]\nprint(reversed_text)  # Output: 'olleh'\n```"
  },
  "done": true,
  "total_duration": 1800000000,
  "load_duration": 200000000,
  "prompt_eval_count": 25,
  "prompt_eval_duration": 400000000,
  "eval_count": 38,
  "eval_duration": 1200000000
}
```

#### 3. List Models (`/api/tags`)

List all downloaded models.

**GET** `/api/tags`

```bash
curl http://localhost:11434/api/tags
```

**Response**:
```json
{
  "models": [
    {
      "name": "qwen2.5:7b",
      "modified_at": "2025-10-28T10:00:00.123Z",
      "size": 4700000000,
      "digest": "sha256:abc123...",
      "details": {
        "format": "gguf",
        "family": "qwen2.5",
        "families": ["qwen2.5"],
        "parameter_size": "7B",
        "quantization_level": "Q4_0"
      }
    }
  ]
}
```

#### 4. Show Model Info (`/api/show`)

Get detailed information about a model.

**POST** `/api/show`

```bash
curl http://localhost:11434/api/show -d '{
  "model": "qwen2.5:7b"
}'
```

#### 5. Pull Model (`/api/pull`)

Download a model from Ollama library.

**POST** `/api/pull`

```bash
curl http://localhost:11434/api/pull -d '{
  "model": "qwen2.5:7b",
  "stream": true
}'
```

#### 6. Delete Model (`/api/delete`)

Remove a model.

**DELETE** `/api/delete`

```bash
curl -X DELETE http://localhost:11434/api/delete -d '{
  "model": "qwen2.5:7b"
}'
```

#### 7. Copy Model (`/api/copy`)

Create a copy of a model with a different name.

**POST** `/api/copy`

```bash
curl http://localhost:11434/api/copy -d '{
  "source": "qwen2.5:7b",
  "destination": "qwen2.5-production"
}'
```

#### 8. List Running Models (`/api/ps`)

List currently loaded models.

**GET** `/api/ps`

```bash
curl http://localhost:11434/api/ps
```

#### 9. Create Model (`/api/create`)

Create a model from a Modelfile.

**POST** `/api/create`

```bash
curl http://localhost:11434/api/create -d '{
  "model": "qwen2.5-custom",
  "modelfile": "FROM qwen2.5:7b\nPARAMETER temperature 0.5",
  "stream": false
}'
```

#### 10. Generate Embeddings (`/api/embeddings`)

Generate embeddings for text.

**POST** `/api/embeddings`

```bash
curl http://localhost:11434/api/embeddings -d '{
  "model": "qwen2.5:7b",
  "prompt": "The quick brown fox"
}'
```

### OpenAI-Compatible API

Ollama provides OpenAI-compatible endpoints:

```python
from openai import OpenAI

client = OpenAI(
    base_url='http://YOUR_DROPLET_IP:11434/v1/',
    api_key='ollama'  # required but ignored
)

response = client.chat.completions.create(
    model='qwen2.5:7b',
    messages=[
        {'role': 'system', 'content': 'You are a helpful assistant.'},
        {'role': 'user', 'content': 'Explain machine learning'}
    ]
)

print(response.choices[0].message.content)
```

### Streaming Responses

For real-time streaming:

```python
import requests
import json

response = requests.post(
    'http://localhost:11434/api/chat',
    json={
        'model': 'qwen2.5:7b',
        'messages': [
            {'role': 'user', 'content': 'Tell me a story'}
        ],
        'stream': True
    },
    stream=True
)

for line in response.iter_lines():
    if line:
        chunk = json.loads(line)
        if 'message' in chunk:
            print(chunk['message']['content'], end='', flush=True)
```

---

## Prompt Format & Best Practices

### Official Qwen2.5 Chat Template

Qwen2.5 uses the ChatML format with specific tokens:

```
<|im_start|>system
{system_message}<|im_end|>
<|im_start|>user
{user_message}<|im_end|>
<|im_start|>assistant
{assistant_response}<|im_end|>
```

### Example Prompts

#### Basic Conversation

```python
messages = [
    {
        "role": "system",
        "content": "You are Qwen, created by Alibaba Cloud. You are a helpful assistant."
    },
    {
        "role": "user",
        "content": "What is the capital of France?"
    }
]
```

#### Code Generation

```python
messages = [
    {
        "role": "system",
        "content": "You are an expert Python programmer. Provide clean, well-documented code."
    },
    {
        "role": "user",
        "content": "Write a function to find the longest common subsequence between two strings."
    }
]
```

#### JSON Structured Output

```bash
curl http://localhost:11434/api/chat -d '{
  "model": "qwen2.5:7b",
  "messages": [
    {
      "role": "system",
      "content": "You are a data extraction assistant. Always respond with valid JSON."
    },
    {
      "role": "user",
      "content": "Extract the key information from this text: John Smith works at Acme Corp as a Software Engineer since 2020. His email is john@acme.com"
    }
  ],
  "format": "json",
  "stream": false
}'
```

**Expected Response**:
```json
{
  "name": "John Smith",
  "company": "Acme Corp",
  "position": "Software Engineer",
  "start_year": 2020,
  "email": "john@acme.com"
}
```

#### Multi-Turn Conversation

```python
conversation = [
    {
        "role": "system",
        "content": "You are a helpful tutor teaching Python programming."
    },
    {
        "role": "user",
        "content": "What is a list comprehension?"
    },
    {
        "role": "assistant",
        "content": "A list comprehension is a concise way to create lists in Python. It follows the pattern: [expression for item in iterable if condition]"
    },
    {
        "role": "user",
        "content": "Can you show me an example?"
    }
]
```

### Best Practices for Prompting

1. **Be Specific and Clear**
   - Provide clear instructions
   - Specify desired format and length
   - Include examples when helpful

2. **Use System Messages Effectively**
   - Define the assistant's role and behavior
   - Set constraints and guidelines
   - Specify output format requirements

3. **Leverage Context Window**
   - Qwen2.5 supports up to 128K tokens
   - Include relevant context for complex tasks
   - Maintain conversation history for multi-turn chats

4. **Optimize for Structured Output**
   - Use `format: "json"` parameter for JSON responses
   - Provide schema examples in system message
   - Validate outputs programmatically

5. **Temperature Guidelines**
   - **0.0-0.3**: Deterministic, factual responses
   - **0.4-0.7**: Balanced creativity and coherence (default: 0.7)
   - **0.8-1.0**: More creative, varied outputs
   - **1.1-2.0**: Highly creative, less predictable

6. **Function Calling / Tool Use**
   - Supported in Ollama 0.5.0+
   - Define tools in the `tools` parameter
   - Model will return structured function calls

### Common Pitfalls to Avoid

1. **Inconsistent Formatting**: Don't mix chat templates manually if using API
2. **Ignoring Context Limits**: Monitor context window usage
3. **Over-constraining**: Allow model flexibility within guidelines
4. **Neglecting Stop Tokens**: Properly configure stop sequences
5. **Inconsistent Temperature**: Keep temperature consistent for similar tasks

---

## Debugging & Troubleshooting

### Common Issues & Solutions

#### 1. Model Download Fails

**Symptoms**:
- `Error: failed to pull model`
- Network timeout errors
- Incomplete downloads

**Solutions**:
```bash
# Check internet connectivity
ping -c 4 8.8.8.8

# Check disk space
df -h

# Verify Ollama is running
systemctl status ollama

# Try pulling with verbose output
ollama pull qwen2.5:7b --verbose

# Manual download using huggingface-cli
pip install -U "huggingface_hub[cli]"
huggingface-cli download bartowski/Qwen2.5-7B-Instruct-GGUF \
  --include "Qwen2.5-7B-Instruct-Q8_0.gguf" \
  --local-dir ~/models/

# Then create model from GGUF file
```

#### 2. Out of Memory (OOM) Errors

**Symptoms**:
- `Error: out of memory`
- Model fails to load
- System becomes unresponsive

**Solutions**:
```bash
# Check available memory
free -h

# Check loaded models
ollama ps

# Unload models
ollama stop qwen2.5:7b

# Reduce context window in Modelfile
PARAMETER num_ctx 2048  # Instead of 4096 or higher

# Use a smaller quantization
ollama pull qwen2.5:7b  # May use Q4 instead of Q8

# Limit concurrent requests
PARAMETER num_parallel 1
```

#### 3. Slow Inference / Low Performance

**Symptoms**:
- Very slow token generation (<1 token/sec)
- High CPU usage
- Long wait times

**Solutions**:
```bash
# Check CPU/GPU usage
top
nvidia-smi  # If using GPU

# Verify GPU is being used (if available)
ollama ps  # Check PROCESSOR column

# Check for CPU throttling
sudo cpupower frequency-info

# Optimize Ollama settings
# Edit /etc/systemd/system/ollama.service
Environment="OLLAMA_NUM_PARALLEL=1"
Environment="OLLAMA_MAX_LOADED_MODELS=1"

# Use smaller context window
PARAMETER num_ctx 2048

# Consider smaller model quantization
ollama pull qwen2.5:7b  # Let Ollama choose optimal quant
```

#### 4. API Connection Refused

**Symptoms**:
- `Connection refused` errors
- Unable to connect to port 11434
- Timeout errors

**Solutions**:
```bash
# Check if Ollama is running
systemctl status ollama

# Start Ollama if stopped
sudo systemctl start ollama

# Check port binding
sudo netstat -tulpn | grep 11434

# Ensure firewall allows port 11434
sudo ufw allow 11434/tcp
sudo ufw reload

# Check OLLAMA_HOST environment variable
# Should be 0.0.0.0:11434 for external access
sudo systemctl cat ollama | grep OLLAMA_HOST

# Test local connection
curl http://localhost:11434/api/version

# Test external connection
curl http://YOUR_DROPLET_IP:11434/api/version
```

#### 5. Model Not Found

**Symptoms**:
- `Error: model not found`
- `404` API responses

**Solutions**:
```bash
# List available models
ollama list

# Check exact model name
ollama show qwen2.5:7b

# Pull model if missing
ollama pull qwen2.5:7b

# Verify model name in API calls
curl http://localhost:11434/api/generate -d '{
  "model": "qwen2.5:7b",
  "prompt": "test"
}'
```

#### 6. Invalid or Garbled Outputs

**Symptoms**:
- Repeated tokens
- Incoherent responses
- Unexpected formatting

**Solutions**:
```bash
# Check temperature setting
# Lower temperature for more coherent output
{
  "options": {
    "temperature": 0.5,  # Reduced from default
    "repeat_penalty": 1.2  # Increase if repetition occurs
  }
}

# Verify prompt template
ollama show qwen2.5:7b --modelfile

# Clear context and restart
# Remove context parameter from requests

# Check model integrity
ollama pull qwen2.5:7b --force  # Re-download
```

#### 7. JSON Format Issues

**Symptoms**:
- Invalid JSON in responses
- Extra text before/after JSON
- Parsing errors

**Solutions**:
```bash
# Use format parameter
curl http://localhost:11434/api/chat -d '{
  "model": "qwen2.5:7b",
  "format": "json",
  "messages": [...]
}'

# Emphasize JSON in system prompt
{
  "role": "system",
  "content": "You are a data assistant. ALWAYS respond with valid JSON only. Do not include any explanatory text before or after the JSON."
}

# Strip markdown code fences in post-processing
import json
import re

response_text = api_response['message']['content']
# Remove ```json and ``` markers
cleaned = re.sub(r'```json\n?|\n?```', '', response_text)
data = json.loads(cleaned.strip())
```

#### 8. Context Window Exceeded

**Symptoms**:
- `Error: context length exceeded`
- Truncated conversations
- Missing early messages

**Solutions**:
```bash
# Increase context window (if memory allows)
PARAMETER num_ctx 8192  # Or 16384, 32768

# Implement context management
# Summarize older messages
# Keep only recent N messages
# Use sliding window approach

# Check current context usage in responses
# Look at prompt_eval_count in API response

# Consider context compression techniques
```

#### 9. Installation Issues

**Symptoms**:
- `ollama: command not found`
- Permission denied errors
- Service fails to start

**Solutions**:
```bash
# Verify installation
which ollama
ollama --version

# Check PATH
echo $PATH

# Reinstall Ollama
curl -fsSL https://ollama.com/install.sh | sh

# Fix permissions
sudo chown -R $USER:$USER ~/.ollama
chmod -R 755 ~/.ollama

# Check service logs
sudo journalctl -u ollama -f

# Verify binary location
ls -la /usr/local/bin/ollama
```

#### 10. Networking Issues

**Symptoms**:
- Cannot access API from external IP
- CORS errors in web applications
- Network timeouts

**Solutions**:
```bash
# Ensure OLLAMA_HOST is set correctly
sudo systemctl edit ollama
# Add:
Environment="OLLAMA_HOST=0.0.0.0:11434"

# Restart service
sudo systemctl daemon-reload
sudo systemctl restart ollama

# Configure firewall
sudo ufw allow 11434/tcp

# Check DigitalOcean firewall settings
# Ensure port 11434 is open in Cloud Firewall

# For CORS issues, use reverse proxy
# nginx configuration example:
sudo apt install nginx
sudo nano /etc/nginx/sites-available/ollama

# Add location block:
location /api/ {
    proxy_pass http://localhost:11434/api/;
    proxy_http_version 1.1;
    proxy_set_header Upgrade $http_upgrade;
    proxy_set_header Connection 'upgrade';
    proxy_set_header Host $host;
    proxy_cache_bypass $http_upgrade;
    
    # CORS headers
    add_header 'Access-Control-Allow-Origin' '*';
    add_header 'Access-Control-Allow-Methods' 'GET, POST, OPTIONS';
    add_header 'Access-Control-Allow-Headers' 'Content-Type';
}
```

### Diagnostic Commands

```bash
# System diagnostics
free -h                    # Check memory
df -h                      # Check disk space
top                        # Monitor CPU/Memory usage
nvidia-smi                 # GPU status (if available)

# Ollama diagnostics
ollama --version           # Version check
ollama list                # List models
ollama ps                  # Running models
ollama show qwen2.5:7b     # Model details
systemctl status ollama    # Service status
journalctl -u ollama -n 50 # Service logs

# Network diagnostics
curl http://localhost:11434/api/version  # Local API test
netstat -tulpn | grep 11434              # Port check
sudo ufw status                          # Firewall status

# Test API endpoints
curl http://localhost:11434/api/tags     # List models API
curl http://localhost:11434/api/ps       # Running models API
```

### Log Analysis

```bash
# View Ollama service logs
sudo journalctl -u ollama -f

# View recent errors
sudo journalctl -u ollama --since "1 hour ago" | grep -i error

# Full logs
sudo journalctl -u ollama --no-pager

# Filter by priority
sudo journalctl -u ollama -p err  # Errors only
```

---

## Performance Optimization

### Hardware Optimization

#### CPU-Only Systems

```bash
# Optimize for multi-core usage
export OMP_NUM_THREADS=$(nproc)
export MKL_NUM_THREADS=$(nproc)

# Enable CPU affinity
taskset -c 0-7 ollama serve  # Use cores 0-7
```

#### GPU Systems

```bash
# Set GPU device
export CUDA_VISIBLE_DEVICES=0

# Check GPU utilization
watch -n 1 nvidia-smi

# Enable GPU memory growth (if using TensorFlow/PyTorch)
export TF_FORCE_GPU_ALLOW_GROWTH=true
```

### Ollama Configuration Optimization

```bash
# Edit service file
sudo systemctl edit ollama
```

**Optimized Configuration**:
```ini
[Service]
Environment="OLLAMA_HOST=0.0.0.0:11434"
Environment="OLLAMA_MAX_LOADED_MODELS=1"
Environment="OLLAMA_MAX_QUEUE=512"
Environment="OLLAMA_NUM_PARALLEL=1"
Environment="OLLAMA_KEEP_ALIVE=10m"
Environment="OLLAMA_FLASH_ATTENTION=1"
```

### Model Parameters Optimization

```dockerfile
# In Modelfile
FROM qwen2.5:7b

# Context window - balance between capability and performance
PARAMETER num_ctx 4096  # 2048 for faster, 8192 for more context

# Batch size - higher = faster but more memory
PARAMETER num_batch 512

# Thread count - match CPU cores
PARAMETER num_thread 8

# GPU layers - maximize GPU usage
PARAMETER num_gpu 99  # Set to high number to use all available GPU

# Prediction parameters
PARAMETER top_k 40
PARAMETER top_p 0.9
PARAMETER temperature 0.7
PARAMETER repeat_penalty 1.1

# Generation limits
PARAMETER num_predict 2048  # Max tokens to generate
```

### API Usage Optimization

#### 1. Batch Processing

```python
# Process multiple prompts efficiently
prompts = [
    "Summarize: [text1]",
    "Summarize: [text2]",
    "Summarize: [text3]"
]

for prompt in prompts:
    # Keep model loaded between requests
    response = generate(prompt, keep_alive=600)
```

#### 2. Streaming for Long Responses

```python
# Use streaming for better UX
response = requests.post(
    'http://localhost:11434/api/chat',
    json={'model': 'qwen2.5:7b', 'messages': messages, 'stream': True},
    stream=True
)

for line in response.iter_lines():
    if line:
        process_chunk(json.loads(line))
```

#### 3. Context Reuse

```python
# Save and reuse context for follow-up questions
initial_response = generate(prompt)
context = initial_response['context']

# Use context in follow-up
follow_up_response = generate(
    follow_up_prompt,
    context=context
)
```

#### 4. Parallel Requests (with caution)

```python
import concurrent.futures

def process_prompt(prompt):
    return generate(prompt)

with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:
    results = executor.map(process_prompt, prompts)
```

### Caching Strategies

```python
import hashlib
import json
from functools import lru_cache

# Cache identical prompts
@lru_cache(maxsize=128)
def cached_generate(prompt_hash, model):
    return generate(json.loads(prompt_hash))

# Use cache
prompt_json = json.dumps({"model": model, "prompt": prompt})
prompt_hash = hashlib.md5(prompt_json.encode()).hexdigest()
result = cached_generate(prompt_hash, model)
```

### Monitoring Performance

```python
import time

def benchmark_inference(model, prompt, iterations=5):
    times = []
    for _ in range(iterations):
        start = time.time()
        response = generate(prompt, stream=False)
        end = time.time()
        times.append(end - start)
        
        tokens = response['eval_count']
        duration = response['eval_duration'] / 1e9  # Convert to seconds
        tokens_per_sec = tokens / duration
        
        print(f"Tokens: {tokens}, Duration: {duration:.2f}s, Speed: {tokens_per_sec:.2f} tokens/sec")
    
    avg_time = sum(times) / len(times)
    print(f"\nAverage time: {avg_time:.2f}s")
```

### Resource Monitoring

```bash
# Create monitoring script
nano ~/monitor_ollama.sh
```

```bash
#!/bin/bash

echo "=== System Resources ==="
echo "CPU Usage:"
top -bn1 | grep "Cpu(s)"

echo -e "\nMemory Usage:"
free -h

echo -e "\nDisk Usage:"
df -h /

if command -v nvidia-smi &> /dev/null; then
    echo -e "\n=== GPU Status ==="
    nvidia-smi --query-gpu=utilization.gpu,utilization.memory,memory.used,memory.total,temperature.gpu --format=csv,noheader,nounits
fi

echo -e "\n=== Ollama Status ==="
ollama ps

echo -e "\n=== Network Connections ==="
netstat -an | grep 11434 | wc -l
```

```bash
chmod +x ~/monitor_ollama.sh
watch -n 5 ~/monitor_ollama.sh
```

---

## Monitoring & Maintenance

### Health Checks

```bash
# Create health check script
nano ~/health_check.sh
```

```bash
#!/bin/bash

# Check Ollama service
if systemctl is-active --quiet ollama; then
    echo "✓ Ollama service is running"
else
    echo "✗ Ollama service is NOT running"
    sudo systemctl start ollama
fi

# Check API endpoint
if curl -s http://localhost:11434/api/version > /dev/null; then
    echo "✓ Ollama API is accessible"
else
    echo "✗ Ollama API is NOT accessible"
fi

# Check model availability
if ollama list | grep -q "qwen2.5:7b"; then
    echo "✓ Qwen2.5:7b model is available"
else
    echo "✗ Qwen2.5:7b model is NOT available"
fi

# Check disk space
DISK_USAGE=$(df -h / | awk 'NR==2 {print $5}' | sed 's/%//')
if [ $DISK_USAGE -lt 80 ]; then
    echo "✓ Disk space is sufficient ($DISK_USAGE%)"
else
    echo "⚠ Disk space is running low ($DISK_USAGE%)"
fi

# Check memory
MEM_AVAILABLE=$(free -m | awk 'NR==2 {print $7}')
if [ $MEM_AVAILABLE -gt 2000 ]; then
    echo "✓ Memory is sufficient (${MEM_AVAILABLE}MB available)"
else
    echo "⚠ Memory is running low (${MEM_AVAILABLE}MB available)"
fi
```

```bash
chmod +x ~/health_check.sh
# Run manually or set up cron job
crontab -e
# Add: */5 * * * * /root/health_check.sh >> /var/log/ollama_health.log 2>&1
```

### Automated Backups

```bash
# Backup models and configurations
nano ~/backup_ollama.sh
```

```bash
#!/bin/bash

BACKUP_DIR="/root/ollama_backups"
TIMESTAMP=$(date +%Y%m%d_%H%M%S)

mkdir -p $BACKUP_DIR

# Backup Ollama models
echo "Backing up Ollama models..."
tar -czf "$BACKUP_DIR/ollama_models_$TIMESTAMP.tar.gz" ~/.ollama/models/

# Backup custom Modelfiles
if [ -d ~/models ]; then
    echo "Backing up custom Modelfiles..."
    tar -czf "$BACKUP_DIR/custom_models_$TIMESTAMP.tar.gz" ~/models/
fi

# Backup Ollama service configuration
echo "Backing up service configuration..."
sudo cp /etc/systemd/system/ollama.service "$BACKUP_DIR/ollama.service_$TIMESTAMP"

# Keep only last 7 days of backups
find $BACKUP_DIR -name "*.tar.gz" -mtime +7 -delete
find $BACKUP_DIR -name "ollama.service_*" -mtime +7 -delete

echo "Backup completed: $TIMESTAMP"
```

```bash
chmod +x ~/backup_ollama.sh
# Schedule daily backups
crontab -e
# Add: 0 2 * * * /root/backup_ollama.sh >> /var/log/ollama_backup.log 2>&1
```

### Update Management

```bash
# Update Ollama
curl -fsSL https://ollama.com/install.sh | sh

# Update models (pulls latest version)
ollama pull qwen2.5:7b

# Update system packages
sudo apt update && sudo apt upgrade -y
```

### Log Rotation

```bash
# Configure log rotation for custom logs
sudo nano /etc/logrotate.d/ollama
```

```
/var/log/ollama_health.log
/var/log/ollama_backup.log {
    daily
    missingok
    rotate 14
    compress
    delaycompress
    notifempty
    create 0644 root root
}
```

### Performance Tracking

```python
# Track inference performance over time
import json
import time
from datetime import datetime

def log_performance(model, prompt, response):
    metrics = {
        'timestamp': datetime.now().isoformat(),
        'model': model,
        'prompt_length': len(prompt),
        'response_length': len(response.get('response', '')),
        'total_duration': response.get('total_duration', 0) / 1e9,
        'eval_count': response.get('eval_count', 0),
        'eval_duration': response.get('eval_duration', 0) / 1e9,
        'tokens_per_second': response.get('eval_count', 0) / (response.get('eval_duration', 1) / 1e9)
    }
    
    with open('/var/log/ollama_performance.jsonl', 'a') as f:
        f.write(json.dumps(metrics) + '\n')
```

### Alerting

```bash
# Simple alerting script
nano ~/alert_check.sh
```

```bash
#!/bin/bash

ALERT_EMAIL="your-email@example.com"
THRESHOLD_CPU=80
THRESHOLD_MEM=80
THRESHOLD_DISK=85

# Check CPU
CPU_USAGE=$(top -bn1 | grep "Cpu(s)" | awk '{print $2}' | cut -d'%' -f1 | cut -d'.' -f1)
if [ $CPU_USAGE -gt $THRESHOLD_CPU ]; then
    echo "High CPU usage: $CPU_USAGE%" | mail -s "Alert: High CPU" $ALERT_EMAIL
fi

# Check Memory
MEM_USAGE=$(free | grep Mem | awk '{print ($3/$2) * 100.0}' | cut -d'.' -f1)
if [ $MEM_USAGE -gt $THRESHOLD_MEM ]; then
    echo "High memory usage: $MEM_USAGE%" | mail -s "Alert: High Memory" $ALERT_EMAIL
fi

# Check Disk
DISK_USAGE=$(df -h / | awk 'NR==2 {print $5}' | sed 's/%//')
if [ $DISK_USAGE -gt $THRESHOLD_DISK ]; then
    echo "High disk usage: $DISK_USAGE%" | mail -s "Alert: High Disk Usage" $ALERT_EMAIL
fi

# Check Ollama service
if ! systemctl is-active --quiet ollama; then
    echo "Ollama service is down" | mail -s "Alert: Ollama Down" $ALERT_EMAIL
    sudo systemctl start ollama
fi
```

---

## Resource Links

### Official Documentation

- **Qwen Team**
  - Official Blog: https://qwenlm.github.io/blog/qwen2.5/
  - Documentation: https://qwen.readthedocs.io/
  - Hugging Face Organization: https://huggingface.co/Qwen
  - GitHub Repository: https://github.com/mx4ai/qwen2.5
  - Research Paper: https://arxiv.org/abs/2412.15115

- **Ollama**
  - Official Website: https://ollama.com
  - Documentation: https://ollama.readthedocs.io/
  - API Reference: https://ollama.readthedocs.io/en/api/
  - GitHub Repository: https://github.com/ollama/ollama
  - Model Library: https://ollama.com/library
  - Qwen2.5 on Ollama: https://ollama.com/library/qwen2.5

### Model Files & Quantizations

- **Hugging Face**
  - Base Model: https://huggingface.co/Qwen/Qwen2.5-7B-Instruct
  - GGUF Quantizations: https://huggingface.co/bartowski/Qwen2.5-7B-Instruct-GGUF
  - Q8_0 Direct Download: https://huggingface.co/bartowski/Qwen2.5-7B-Instruct-GGUF/blob/main/Qwen2.5-7B-Instruct-Q8_0.gguf

### DigitalOcean Resources

- **Official Guides**
  - GPU Droplets: https://www.digitalocean.com/blog/announcing-gpu-droplets
  - Ollama on H100: https://www.digitalocean.com/community/tutorials/run-llms-with-ollama-on-h100-gpus-for-maximum-efficiency
  - Phi-3 Deployment: https://www.digitalocean.com/community/tutorials/deploy-phi3-with-ollama-webui
  - DeepSeek R1 Tutorial: https://www.digitalocean.com/community/tutorials/deepseek-r1-gpu-droplets

### Community Tutorials

- **Ollama Setup**
  - Ollama on Linux Droplets: https://medium.com/@kamleshgehaniii/how-to-run-llms-like-ollama-on-linux-droplets-digitalocean-e1f09072e9d8
  - Self-Hosting Guide: https://tarunvelli.site/post/self-hosted-ollama/
  - Docker Setup: https://www.oneclickitsolution.com/centerofexcellence/aiml/how-to-set-up-qwen-2-5-in-docker-with-ollama-on-azure-vm

- **Qwen2.5 Guides**
  - Running Locally: https://www.analyticsvidhya.com/blog/2025/01/run-qwen2-5-models-locally/
  - Qwen2.5 Coder: https://atalupadhyay.wordpress.com/2025/01/13/running-qwen-2-5-coder-locally-a-complete-guide/

### API & Integration

- **API Documentation**
  - Ollama API Reference: https://ollama.readthedocs.io/en/api/
  - Ollama REST Endpoints: https://notes.kodekloud.com/docs/Running-Local-LLMs-With-Ollama/Building-AI-Applications/Ollama-REST-API-Endpoints
  - OpenAI Compatibility: https://docs.litellm.ai/docs/providers/ollama
  - Ollama Cheatsheet: https://apidog.com/blog/ollama-cheatsheet/

### Tools & Interfaces

- **Open WebUI**
  - Official Site: https://openwebui.com
  - Documentation: https://docs.openwebui.com/
  - API Endpoints: https://docs.openwebui.com/getting-started/api-endpoints/
  - GitHub: https://github.com/open-webui/open-webui

### GitHub Repositories

- **Ollama**
  - Main Repository: https://github.com/ollama/ollama
  - Issues: https://github.com/ollama/ollama/issues

- **Qwen**
  - QwenLM Organization: https://github.com/QwenLM
  - Qwen2.5: https://github.com/mx4ai/qwen2.5
  - Qwen Agent: https://github.com/QwenLM/Qwen-Agent

- **Utilities**
  - DigitalOcean Automation: https://github.com/f-lombardo/ollama-digital-ocean

### Additional Resources

- **llama.cpp**
  - GitHub: https://github.com/ggerganov/llama.cpp
  - Documentation: https://github.com/ggerganov/llama.cpp/blob/master/README.md

- **GGUF Format**
  - Specification: https://github.com/ggerganov/ggml/blob/master/docs/gguf.md
  - Quantization Guide: https://gist.github.com/Artefact2/b5f810600771265fc1e39442288e8ec9

### Community & Support

- **Discord**
  - Qwen Discord: https://discord.gg/CV4E9rpNSD
  - Ollama Discord: (Check GitHub repository)

- **Forums & Discussions**
  - Hugging Face Discussions: https://huggingface.co/Qwen/Qwen2.5-7B-Instruct/discussions
  - DigitalOcean Community: https://www.digitalocean.com/community/

---

## Quick Reference Card

### Essential Commands

```bash
# Installation
curl -fsSL https://ollama.com/install.sh | sh

# Pull model
ollama pull qwen2.5:7b

# Run interactively
ollama run qwen2.5:7b

# List models
ollama list

# Check running models
ollama ps

# Service management
sudo systemctl status ollama
sudo systemctl restart ollama
sudo systemctl stop ollama

# API health check
curl http://localhost:11434/api/version
```

### Common API Calls

```bash
# Generate (simple)
curl http://localhost:11434/api/generate -d '{
  "model": "qwen2.5:7b",
  "prompt": "Your prompt here",
  "stream": false
}'

# Chat (conversation)
curl http://localhost:11434/api/chat -d '{
  "model": "qwen2.5:7b",
  "messages": [
    {"role": "system", "content": "You are helpful."},
    {"role": "user", "content": "Hello!"}
  ],
  "stream": false
}'

# JSON output
curl http://localhost:11434/api/chat -d '{
  "model": "qwen2.5:7b",
  "messages": [{"role": "user", "content": "Extract data..."}],
  "format": "json",
  "stream": false
}'
```

### Troubleshooting Checklist

- [ ] Ollama service is running: `systemctl status ollama`
- [ ] Model is downloaded: `ollama list`
- [ ] Port 11434 is open: `sudo ufw status`
- [ ] API responds: `curl http://localhost:11434/api/version`
- [ ] Sufficient memory: `free -h`
- [ ] Disk space available: `df -h`
- [ ] Logs are clean: `journalctl -u ollama -n 50`

---

## Conclusion

This guide provides comprehensive coverage of deploying and managing Qwen2.5-7B-Instruct-Q8_0 with Ollama on DigitalOcean. As an AI agent, you should:

1. **Reference this document** when implementing or debugging the system
2. **Verify all links** are accessible and up-to-date
3. **Monitor performance** using the provided tools and scripts
4. **Adapt configurations** based on specific use case requirements
5. **Maintain regular backups** of models and configurations
6. **Stay updated** with Ollama and Qwen releases

For any issues not covered here, consult the official documentation links provided in the Resource Links section.

---

**Document Version**: 1.0  
**Last Updated**: October 28, 2025  
**Prepared For**: Claude Sonnet 4.5 AI Agent  
**Maintainer**: AI Operations Team
